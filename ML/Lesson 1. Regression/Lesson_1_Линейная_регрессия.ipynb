{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMTULSKM3MAooGQOitoY5hr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CodeHunterOfficial/AI_DataMining/blob/main/ML/Lesson%201.%20Regression/Lesson_1_%D0%9B%D0%B8%D0%BD%D0%B5%D0%B9%D0%BD%D0%B0%D1%8F_%D1%80%D0%B5%D0%B3%D1%80%D0%B5%D1%81%D1%81%D0%B8%D1%8F.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Линейная регрессия\n",
        "\n",
        "**Регрессия** — одна из трех основных задач машинного обучения наряду с классификацией и кластеризацией, предназначенная для предсказания числовых значений целевой переменной на основе одной или нескольких независимых переменных. Цель линейной регрессии — установить линейную зависимость между признаками и целевой переменной, чтобы сделать прогноз для новых данных.\n",
        "\n",
        "### Типы линейной регрессии\n",
        "\n",
        "1. **Простая линейная регрессия**: Модель содержит одну независимую переменную.\n",
        "2. **Множественная линейная регрессия**: Модель содержит несколько независимых переменных.\n",
        "\n",
        "Линейная регрессия является одним из самых простых и популярных методов регрессии, благодаря понятности и простоте интерпретации. Модель линейной регрессии подбирает коэффициенты (веса) для независимых переменных, минимизируя среднеквадратичную ошибку между предсказанными и фактическими значениями.\n",
        "\n",
        "### Определение задачи регрессии\n",
        "\n",
        "Задача предсказания значения целевой переменной $t$ на основе вектора признаков $x$ называется задачей регрессии. Для этого входные признаки $x$ (например, характеристики дома) используются для предсказания целевого значения $t$ (например, цена дома). Линейная регрессия сводится к построению \"линии наилучшего соответствия\", проходящей через набор точек данных.\n",
        "\n",
        "## Данные\n",
        "\n",
        "Модель линейной регрессии обучается на **датасете** — наборе данных, содержащем информацию о независимых переменных и целевой переменной. Датасет состоит из наблюдений $(x, t)$, где:\n",
        "- $x$ — вектор предикторов (признаков),\n",
        "- $t$ — целевая переменная.\n",
        "\n",
        "### Разделение данных\n",
        "\n",
        "Часто данные делят на три подмножества:\n",
        "1. **Обучающая выборка (Training set)**: используется для подбора параметров модели.\n",
        "2. **Валидационная выборка (Validation set)**: служит для настройки гиперпараметров и предотвращения переобучения.\n",
        "3. **Тестовая выборка (Test set)**: предназначена для окончательной оценки модели после завершения обучения.\n",
        "\n",
        "Рекомендуемые соотношения деления:\n",
        "- Обучающая выборка — 60-80%\n",
        "- Валидационная выборка — 10-20%\n",
        "- Тестовая выборка — 10-20%\n",
        "\n",
        "## Постановка задачи регрессии\n",
        "\n",
        "Пусть значение целевой переменной $t$ для вектора $x$ описывается детерминированной функцией $y(x, w)$ с добавлением гауссовского шума $\\varepsilon$:\n",
        "$$\n",
        "t = y(x, w) + \\varepsilon, \\quad \\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)\n",
        "$$\n",
        "Здесь:\n",
        "- $y(x, w)$ — детерминированная функция, зависящая от параметров $w$,\n",
        "- $\\varepsilon$ — нормально распределенный шум с дисперсией $\\sigma^2$.\n",
        "\n",
        "**Вероятностная интерпретация модели** заключается в том, что вероятность наблюдения конкретного значения $t$ при известных $x$, $w$ и $\\sigma^2$ описывается нормальным распределением:\n",
        "$$\n",
        "p(t|x, w, \\sigma^2) = \\mathcal{N}(y(x, w), \\sigma^2)\n",
        "$$\n",
        "В большинстве случаев предполагается, что дисперсия ошибки $\\sigma^2$ постоянна (гомоскедастичность). Если дисперсия зависит от значений $x$ — это **гетероскедастичность**. В случае гетероскедастичности применяют методы, учитывающие изменяющуюся дисперсию, например, взвешенную регрессию.\n",
        "\n",
        "### Проверка гомоскедастичности и гетероскедастичности\n",
        "\n",
        "**Визуальные методы**:  \n",
        "- **График остатков** — если остатки распределены случайно и равномерно вокруг оси, то присутствует гомоскедастичность. Отклонения указывают на гетероскедастичность.\n",
        "- **Q-Q график** — помогает оценить нормальность распределения остатков.\n",
        "\n",
        "**Статистические тесты**:  \n",
        "- **Тест Бреуша-Пагана** и **тест Голдфелда-Квандта** помогают проверить гомоскедастичность на статистически значимом уровне.\n",
        "  \n",
        "  \n",
        "## Построение функции $y(x, w)$\n",
        "\n",
        "Задача линейной регрессии заключается в построении функции вида:\n",
        "$$\n",
        "y(x, w) \\to \\mathbb{R}\n",
        "$$\n",
        "где $w$ — вектор параметров функции, а $x$ — вектор характеристик (признаков) объекта.\n",
        "\n",
        "Наиболее часто используемая форма линейной функции:\n",
        "$$\n",
        "y(x, w) = w_0 + w_1 x_1 + \\ldots + w_D x_D = w_0 + \\sum_{i=1}^D w_i \\cdot x_i\n",
        "$$\n",
        "где:\n",
        "- $y(x, w)$ — предсказанное значение целевой переменной (таргета),\n",
        "- $x = (x_1, \\ldots, x_D)$ — вектор признаков,\n",
        "- $w_1, \\ldots, w_D, w_0$ — параметры модели.\n",
        "\n",
        "Признаки также называют **фичами** (от англ. features). Вектор $w = (w_1, \\ldots, w_D)$ часто называют вектором **весов**, так как модель можно интерпретировать как взвешенную сумму признаков объекта. Число $w_0$ называется **свободным членом** или **сдвигом** (bias).\n",
        "\n",
        "**Примечание.** Свободный член $w_0$ иногда опускают, добавляя ко всем $x_i$ признак, тождественно равный единице. В таком случае роль $w_0$ будет выполнять соответствующий вес:\n",
        "$$\n",
        "\\begin{pmatrix}x_{1} & \\ldots & x_{D} \\end{pmatrix}\\cdot\\begin{pmatrix}w_1\\\\ \\vdots \\\\ w_D\\end{pmatrix} + w_0 =\n",
        "\\begin{pmatrix}1 & x_{1} & \\ldots & x_{D} \\end{pmatrix}\\cdot\\begin{pmatrix}w_0 \\\\ w_1 \\\\ \\vdots \\\\ w_D \\end{pmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "## Базисные функции (Basis Functions)\n",
        "\n",
        "Базисные функции в линейной регрессии представляют собой преобразования, которые применяются к исходным признакам (входным переменным), чтобы перенести их в новое пространство признаков. Это позволяет модели лучше улавливать сложные зависимости между входными данными и целевой переменной, обеспечивая более точные предсказания. Введение базисных функций в линейную модель особенно полезно, когда между исходными признаками и целевой переменной нет простой линейной зависимости. С их помощью можно расширить пространство признаков и создать более точную и гибкую модель.\n",
        "\n",
        "### Модель линейной регрессии с базисными функциями\n",
        "\n",
        "Для включения базисных функций модель линейной регрессии может быть записана следующим образом:\n",
        "$$\n",
        "y(x, w) = w_0 + \\sum_{j=1}^{M-1} w_j \\cdot \\phi_j(x) = w^T \\phi(x),\n",
        "$$\n",
        "где вектор $\\phi(x)$ определяется как:\n",
        "$$\n",
        "\\phi(x) = (\\phi_0(x), \\phi_1(x), \\ldots, \\phi_{M-1}(x))^T,\n",
        "$$\n",
        "при этом $\\phi_0(x) = 1$, что позволяет учесть свободный член модели $w_0$.\n",
        "\n",
        "Параметры модели $w$ также представлены вектором:\n",
        "$$\n",
        "w = (w_0, w_1, \\ldots, w_{M-1})^T.\n",
        "$$\n",
        "\n",
        "### Зачем нужны базисные функции?\n",
        "\n",
        "Использование базисных функций позволяет решать несколько важных задач:\n",
        "\n",
        "1. **Моделирование сложных зависимостей.** Базисные функции помогают выявить сложные нелинейные зависимости между входными признаками и целевой переменной, делая модель более гибкой.\n",
        "   \n",
        "2. **Повышение точности предсказаний.** Применение различных базисных функций увеличивает способность модели подстраиваться под данные, улучшая точность прогнозов.\n",
        "   \n",
        "3. **Снижение ошибки модели.** Введение базисных функций способствует уменьшению ошибки, позволяя более точно описывать закономерности в данных.\n",
        "   \n",
        "4. **Улучшение обобщающей способности.** Базисные функции позволяют модели лучше обобщать на новые данные, особенно если исходные признаки не подчиняются простой линейной зависимости.\n",
        "\n",
        "### Примеры базисных функций\n",
        "\n",
        "Существует несколько распространённых типов базисных функций, полезных для разных задач линейной регрессии:\n",
        "\n",
        "1. **Полиномиальные базисные функции.** Полиномы позволяют моделировать нелинейные зависимости, добавляя степени признаков в качестве новых базисных функций:\n",
        "$$\n",
        "   \\phi(x) = (1, x, x^2, x^3, \\ldots).\n",
        "$$\n",
        "\n",
        "2. **Радиально-базисные функции (RBF).** Эти функции полезны для аппроксимации сложных нелинейных зависимостей, так как каждая RBF сосредоточена вокруг определённого значения признака:\n",
        "$$\n",
        "   \\phi(x) = \\left(e^{-\\frac{(x - \\mu_1)^2}{2\\sigma^2}}, e^{-\\frac{(x - \\mu_2)^2}{2\\sigma^2}}, \\ldots\\right),\n",
        "$$\n",
        "   где $\\mu_i$ — центр функции, а $\\sigma$ — параметр ширины функции.\n",
        "\n",
        "3. **Сплайн-функции.** Сплайны делят пространство признаков на интервалы и строят отдельные полиномы для каждого из них, плавно соединяя их между собой. Они полезны для моделирования зависимостей, изменяющихся в разных диапазонах данных:\n",
        "$$\n",
        "   \\phi(x) = \\begin{cases}\n",
        "      p_1(x), & x < k_1 \\\\\n",
        "      p_2(x), & k_1 \\leq x < k_2 \\\\\n",
        "      \\dots \\\\\n",
        "      p_n(x), & x \\geq k_{n-1}\n",
        "   \\end{cases}\n",
        "$$\n",
        "   где $k_i$ — узлы, а $p_i(x)$ — полиномы на каждом интервале.\n",
        "\n",
        "4. **Синусоидальные базисные функции.** Для анализа периодических данных используются синусоидальные функции, особенно полезные для моделирования сезонных или циклических процессов:\n",
        "$$\n",
        "   \\phi(x) = (\\sin(\\omega_1 x), \\cos(\\omega_1 x), \\sin(\\omega_2 x), \\cos(\\omega_2 x), \\ldots),\n",
        "$$\n",
        "   где $\\omega_i$ — частоты, задающие периодичность.\n",
        "\n",
        "5. **Логистические базисные функции.** Эти функции подходят для задач, где зависимость резко изменяется при переходе через определённые значения, например, для биологических или социальных процессов. Функция имеет вид:\n",
        "$$\n",
        "   \\phi(x) = \\frac{1}{1 + e^{-(x - \\mu)/\\sigma}},\n",
        "$$\n",
        "   где $\\mu$ — центр, а $\\sigma$ — параметр, регулирующий крутизну перехода.\n",
        "\n",
        "Эти базисные функции дают модели возможность учитывать разнообразные типы зависимостей, от полиномиальных до периодических, локальных и пороговых.\n",
        "\n",
        "### Варианты моделей с базисными функциями\n",
        "\n",
        "Рассмотрим примеры применения базисных функций в линейной регрессии.\n",
        "\n",
        "#### 1. Линейная регрессия с исходными признаками\n",
        "\n",
        "В стандартной линейной регрессии в качестве базисных функций можно использовать сами исходные признаки. Для признаков $x_1, x_2, \\ldots, x_D$ базисные функции определяются так:\n",
        "$$\n",
        "\\phi_1(x) = x_1, \\quad \\phi_2(x) = x_2, \\quad \\ldots, \\quad \\phi_D(x) = x_D.\n",
        "$$\n",
        "Здесь базисные функции представляют исходные переменные, а модель остаётся линейной по параметрам $w$.\n",
        "\n",
        "#### 2. Полиномиальная регрессия с одной переменной\n",
        "\n",
        "Полиномиальная регрессия расширяет линейную модель, добавляя степени исходной переменной в качестве базисных функций. Это позволяет учитывать нелинейные зависимости от переменной:\n",
        "$$\n",
        "\\phi_1(x) = x, \\quad \\phi_2(x) = x^2, \\quad \\ldots, \\quad \\phi_K(x) = x^K.\n",
        "$$\n",
        "Каждая степень переменной $x$ становится новой базисной функцией, увеличивая степень нелинейности модели.\n",
        "\n",
        "#### 3. Регрессия с радиально-базисными функциями (RBF-регрессия)\n",
        "\n",
        "Радиально-базисные функции обеспечивают моделирование локальных зависимостей. Каждая RBF-функция определяется центром $\\mu_i$ и шириной $\\sigma$, создавая «пик» вблизи $\\mu_i$. Это полезно для задач с локальными изменениями зависимостей:\n",
        "$$\n",
        "\\phi(x) = \\left(e^{-\\frac{(x - \\mu_1)^2}{2\\sigma^2}}, e^{-\\frac{(x - \\mu_2)^2}{2\\sigma^2}}, \\ldots, e^{-\\frac{(x - \\mu_K)^2}{2\\sigma^2}}\\right).\n",
        "$$\n",
        "RBF-регрессия особенно эффективна для аппроксимации данных, где зависимость меняется локально, как в задачах географического моделирования.\n",
        "\n",
        "#### 4. Сплайн-регрессия\n",
        "\n",
        "Сплайн-регрессия использует полиномы, которые адаптируются в каждом из заданных интервалов (узлов). Это помогает избежать чрезмерного роста значений на краях, что часто наблюдается в полиномиальных моделях. В сплайн-регрессии на каждом интервале применяется свой полином, плавно соединённый с соседними:\n",
        "$$\n",
        "\\phi(x) = \\begin{cases}\n",
        "      p_1(x), & x < k_1 \\\\\n",
        "      p_2(x), & k_1 \\leq x < k_2 \\\\\n",
        "      \\dots \\\\\n",
        "      p_n(x), & x \\geq k_{n-1}\n",
        "   \\end{cases}.\n",
        "$$\n",
        "Сплайн-регрессия подходит для данных, где зависимость может значительно изменяться в разных диапазонах.\n",
        "\n",
        "#### 5. Синусоидальные базисные функции\n",
        "\n",
        "Для задач анализа временных рядов или периодических данных полезны синусоидальные функции. Эти базисные функции захватывают повторяющиеся тренды, такие как сезонные изменения:\n",
        "$$\n",
        "\\phi(x) = (\\sin(\\omega_1 x), \\cos(\\omega_1 x), \\sin(\\omega_2 x), \\cos(\\omega_2 x), \\ldots),\n",
        "$$\n",
        "где $\\omega_i$ — частоты, задающие периодичность.\n",
        "\n",
        "#### 6. Регрессия с логистическими базисными функциями\n",
        "\n",
        "Логистические базисные функции применяются для задач, в которых зависимость резко изменяется при переходе через пороговые значения. Они полезны для моделирования плав\n",
        "\n",
        "ных переходов, характерных для социальных или биологических процессов:\n",
        "$$\n",
        "\\phi(x) = \\frac{1}{1 + e^{-(x - \\mu)/\\sigma}}.\n",
        "$$\n",
        "Логистические функции позволяют модели плавно описывать скачки или пороговые эффекты, делая её гибкой для работы с нелинейными данными.\n",
        "Таким образом, выбор подходящих базисных функций позволяет адаптировать линейную регрессию под задачи с различной степенью сложности. Полиномиальные, радиально-базисные, сплайновые, синусоидальные и логистические функции создают разнообразные способы моделирования зависимости, от простой до сильно нелинейной.\n",
        "\n",
        "\n",
        "## Матрица плана (Design Matrix)\n",
        "\n",
        "Матрица плана, или матрица признаков, обозначаемая как $X$, представляет собой таблицу значений всех независимых переменных (факторов или признаков) для каждого наблюдения в модели линейной регрессии. Каждая строка этой матрицы соответствует одному наблюдению (например, объекту или записи в выборке), а каждый столбец — отдельному фактору, который может быть представлен в исходном виде или преобразован базисной функцией.\n",
        "\n",
        "### Структура матрицы плана\n",
        "\n",
        "Для классической линейной регрессии матрица плана $X$ имеет размерность $n \\times (p+1)$, где $n$ — количество наблюдений, а $p$ — количество факторов (включая свободный член). Структура стандартной матрицы плана выглядит так:\n",
        "\n",
        "$$\n",
        "X = \\begin{bmatrix}\n",
        "1 & x_{11} & x_{12} & \\dots & x_{1p} \\\\\n",
        "1 & x_{21} & x_{22} & \\dots & x_{2p} \\\\\n",
        "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "1 & x_{n1} & x_{n2} & \\dots & x_{np}\n",
        "\\end{bmatrix},\n",
        "$$\n",
        "\n",
        "где первый столбец состоит из единиц (это bias term, или смещение), представляющий свободный член $w_0$ в модели, а остальные столбцы содержат значения факторов для каждого наблюдения.\n",
        "\n",
        "### Функции и задачи матрицы плана\n",
        "\n",
        "Матрица плана играет ключевую роль в линейной регрессии, облегчая расчётные операции и позволяя использовать методы линейной алгебры для построения модели. Основные её функции:\n",
        "\n",
        "1. **Оценка параметров модели.** С помощью матрицы плана можно вычислить параметры $w$ линейной регрессии с использованием метода наименьших квадратов, минимизируя отклонения между реальными значениями целевой переменной и предсказанными значениями модели. Оценка параметров $w$ производится по формуле:\n",
        "$$\n",
        "   \\hat{w} = (X^T X)^{-1} X^T y,\n",
        "$$\n",
        "   где $y$ — вектор значений целевой переменной, $X^T$ — транспонированная матрица плана. Позже мы подробно рассмотрим, как получить эту оценку.\n",
        "\n",
        "2. **Учет множества факторов.** Матрица плана позволяет модели регрессии учитывать несколько факторов одновременно, что повышает её гибкость и даёт возможность рассматривать влияние различных переменных на целевую переменную.\n",
        "\n",
        "3. **Проведение статистических тестов.** Через матрицу плана можно проводить статистические тесты, чтобы оценить значимость коэффициентов регрессионной модели, определяя, какие из факторов оказывают статистически значимое влияние на целевую переменную.\n",
        "\n",
        "### Матрица плана с базисными функциями\n",
        "\n",
        "В случае использования базисных функций структура матрицы плана изменяется, поскольку каждый фактор может быть представлен нелинейными преобразованиями, что позволяет модели захватывать сложные зависимости между признаками и целевой переменной. В этом случае матрица плана, также называемая матрицей базисных функций и обозначаемая как $\\Phi(x)$, имеет следующий вид:\n",
        "\n",
        "$$\n",
        "\\Phi(x) = \\begin{bmatrix}\n",
        "\\phi_0(x_1) & \\phi_1(x_1) & \\phi_2(x_1) & \\dots & \\phi_p(x_1) \\\\\n",
        "\\phi_0(x_2) & \\phi_1(x_2) & \\phi_2(x_2) & \\dots & \\phi_p(x_2) \\\\\n",
        "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "\\phi_0(x_n) & \\phi_1(x_n) & \\phi_2(x_n) & \\dots & \\phi_p(x_n)\n",
        "\\end{bmatrix},\n",
        "$$\n",
        "\n",
        "где:\n",
        "- каждая строка соответствует одному наблюдению $x_i$,\n",
        "- каждый столбец представляет значение базисной функции $\\phi_j(x_i)$, применённой к признаку $x_i$.\n",
        "\n",
        "### Примеры матрицы плана с базисными функциями\n",
        "\n",
        "#### Полиномиальные базисные функции\n",
        "\n",
        "Для полиномиальной регрессии, где базисные функции представляют собой степени переменной $x$, матрица плана будет выглядеть так:\n",
        "\n",
        "$$\n",
        "\\Phi(x) = \\begin{bmatrix}\n",
        "1 & x_1 & x_1^2 & \\dots & x_1^p \\\\\n",
        "1 & x_2 & x_2^2 & \\dots & x_2^p \\\\\n",
        "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "1 & x_n & x_n^2 & \\dots & x_n^p\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "#### Радиально-базисные функции (RBF)\n",
        "\n",
        "В случае радиально-базисных функций, которые обычно используют для захвата локальных зависимостей в данных, каждая базисная функция фокусируется на определённой точке $\\mu_j$, и матрица плана принимает вид:\n",
        "\n",
        "$$\n",
        "\\Phi(x) = \\begin{bmatrix}\n",
        "e^{-\\frac{(x_1 - \\mu_1)^2}{2\\sigma^2}} & e^{-\\frac{(x_1 - \\mu_2)^2}{2\\sigma^2}} & \\dots & e^{-\\frac{(x_1 - \\mu_p)^2}{2\\sigma^2}} \\\\\n",
        "e^{-\\frac{(x_2 - \\mu_1)^2}{2\\sigma^2}} & e^{-\\frac{(x_2 - \\mu_2)^2}{2\\sigma^2}} & \\dots & e^{-\\frac{(x_2 - \\mu_p)^2}{2\\sigma^2}} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "e^{-\\frac{(x_n - \\mu_1)^2}{2\\sigma^2}} & e^{-\\frac{(x_n - \\mu_2)^2}{2\\sigma^2}} & \\dots & e^{-\\frac{(x_n - \\mu_p)^2}{2\\sigma^2}}\n",
        "\\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "Здесь $\\mu_j$ — центры радиально-базисных функций, а $\\sigma$ регулирует их ширину.\n",
        "\n",
        "### Преимущества матрицы плана с базисными функциями\n",
        "\n",
        "Использование матрицы плана с базисными функциями даёт несколько преимуществ:\n",
        "\n",
        "1. **Моделирование нелинейных зависимостей.** Преобразованные признаки позволяют регрессионной модели захватывать нелинейные зависимости между факторами и целевой переменной, делая её более гибкой и подходящей для моделирования сложных данных.\n",
        "\n",
        "2. **Расширение возможностей анализа.** Базисные функции позволяют строить более сложные и точные модели, которые можно адаптировать для задач с различными типами данных и зависимостей.\n",
        "\n",
        "3. **Повышение точности модели.** С добавлением базисных функций модель способна лучше подстраиваться под закономерности в данных, улучшая точность прогнозирования и обобщающую способность.\n",
        "\n",
        "Таким образом, матрица плана с базисными функциями является мощным инструментом при построении моделей линейной регрессии, позволяя учитывать различные нелинейные преобразования исходных данных и повышать качество модели за счёт лучшего описания сложных зависимостей.\n",
        "\n",
        "\n",
        "\n",
        "## Функция правдоподобия (Likelihood)\n",
        "\n",
        "Функция правдоподобия — это мера того, насколько хорошо параметры модели $w$ объясняют наблюдаемые данные $t$ при известных значениях входных переменных $X$. В линейной регрессии обычно предполагается, что ошибки модели распределены нормально с математическим ожиданием 0 и дисперсией $\\sigma^2$.\n",
        "\n",
        "### Правдоподобие для одного наблюдения\n",
        "\n",
        "Предположим, что каждое наблюдение $t_i$ имеет нормальное распределение с математическим ожиданием, заданным линейной моделью $y(x_i, w) = w^T \\phi(x_i)$, и дисперсией $\\sigma^2$. Тогда правдоподобие для одного наблюдения $t_i$, при заданных значениях $x_i$ и $w$, описывается функцией плотности вероятности нормального распределения:\n",
        "\n",
        "$$\n",
        "L(t_i | x_i, w) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\cdot e^{-\\frac{(t_i - y(x_i, w))^2}{2\\sigma^2}}.\n",
        "$$\n",
        "\n",
        "### Полная функция правдоподобия\n",
        "\n",
        "Функция правдоподобия для всей выборки из $n$ наблюдений — это произведение правдоподобий для каждого наблюдения:\n",
        "\n",
        "$$\n",
        "L(t | X, w) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\cdot e^{-\\frac{(t_i - y(x_i, w))^2}{2\\sigma^2}}.\n",
        "$$\n",
        "\n",
        "Подставив $y(x_i, w) = w^T \\phi(x_i)$, мы получаем:\n",
        "\n",
        "$$\n",
        "L(t | X, w, \\sigma^2) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\cdot e^{-\\frac{(t_i - w^T \\phi(x_i))^2}{2\\sigma^2}}.\n",
        "$$\n",
        "\n",
        "### Логарифмирование функции правдоподобия\n",
        "\n",
        "Вместо максимизации функции правдоподобия $L(t | X, w, \\sigma^2)$ мы можем максимизировать её логарифм, что значительно упрощает вычисления. Логарифмирование преобразует произведение в сумму, что делает анализ и оптимизацию более управляемыми.\n",
        "\n",
        "Рассчитаем логарифм функции правдоподобия:\n",
        "\n",
        "$$\n",
        "\\ln L(t | X, w, \\sigma^2) = \\ln \\left( \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\cdot e^{-\\frac{(t_i - w^T \\phi(x_i))^2}{2\\sigma^2}} \\right).\n",
        "$$\n",
        "\n",
        "Применяя свойство логарифма $\\ln (a \\cdot b) = \\ln a + \\ln b$, получаем:\n",
        "\n",
        "$$\n",
        "\\ln L(t | X, w, \\sigma^2) = \\sum_{i=1}^{n} \\ln \\left( \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\cdot e^{-\\frac{(t_i - w^T \\phi(x_i))^2}{2\\sigma^2}} \\right).\n",
        "$$\n",
        "\n",
        "Теперь можем разложить логарифм произведения на сумму логарифмов:\n",
        "\n",
        "$$\n",
        "\\ln L(t | X, w, \\sigma^2) = \\sum_{i=1}^{n} \\left( \\ln \\frac{1}{\\sqrt{2\\pi\\sigma^2}} + \\ln e^{-\\frac{(t_i - w^T \\phi(x_i))^2}{2\\sigma^2}} \\right).\n",
        "$$\n",
        "\n",
        "### Применение свойств логарифма\n",
        "\n",
        "Воспользуемся двумя свойствами логарифма:\n",
        "\n",
        "1. $\\ln \\frac{1}{a} = -\\ln a$,\n",
        "2. $\\ln e^x = x$.\n",
        "\n",
        "Используя эти свойства, получаем:\n",
        "\n",
        "$$\n",
        "\\ln L(t | X, w, \\sigma^2) = \\sum_{i=1}^{n} \\left( -\\ln \\sqrt{2\\pi\\sigma^2} - \\frac{(t_i - w^T \\phi(x_i))^2}{2\\sigma^2} \\right).\n",
        "$$\n",
        "\n",
        "Теперь разложим первый член более подробно:\n",
        "\n",
        "$$\n",
        "\\ln L(t | X, w, \\sigma^2) = \\sum_{i=1}^{n} \\left( -\\frac{1}{2} \\ln (2\\pi\\sigma^2) - \\frac{(t_i - w^T \\phi(x_i))^2}{2\\sigma^2} \\right).\n",
        "$$\n",
        "\n",
        "Раскрывая сумму, можно представить результат в виде двух отдельных слагаемых:\n",
        "\n",
        "$$\n",
        "\\ln L(t | X, w, \\sigma^2) = -\\frac{n}{2} \\ln (2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (t_i - w^T \\phi(x_i))^2.\n",
        "$$\n",
        "\n",
        "### Окончательная логарифмическая функция правдоподобия\n",
        "\n",
        "Итак, логарифм функции правдоподобия для линейной регрессии принимает следующий вид:\n",
        "\n",
        "$$\n",
        "\\ln L(t | X, w, \\sigma^2) = -\\frac{n}{2} \\ln (2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (t_i - w^T \\phi(x_i))^2.\n",
        "$$\n",
        "\n",
        "Эта функция называется **логарифмической функцией правдоподобия**, и её удобно использовать для оптимизации параметров $w$ и $\\sigma^2$ модели линейной регрессии, так как задача максимизации этой функции эквивалентна минимизации функции потерь:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(w) = \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (t_i - w^T \\phi(x_i))^2.\n",
        "$$\n",
        "\n",
        "Такое логарифмическое представление упрощает работу с функцией правдоподобия, особенно при использовании методов оптимизации, таких как градиентный спуск или аналитические методы для нахождения оптимальных параметров модели.\n",
        "\n",
        "### Оптимизация параметров модели\n",
        "\n",
        "Для нахождения оценок параметров $w$ и дисперсии ошибок $\\sigma^2$ максимизируют логарифмическую функцию правдоподобия. Так как логарифмическая функция правдоподобия является убывающей функцией ошибки модели, это эквивалентно минимизации суммы квадратов отклонений, или функции потерь, для линейной регрессии.\n",
        "\n",
        "В частности, целевая функция для минимизации, полученная из логарифмической функции правдоподобия, имеет вид:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(w) = \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (t_i - w^T \\phi(x_i))^2.\n",
        "$$\n",
        "\n",
        "### Преимущества логарифмирования функции правдоподобия\n",
        "\n",
        "1. **Превращение произведения в сумму**: Логарифм позволяет преобразовать произведение плотностей вероятности для каждого наблюдения в сумму логарифмов. Это упрощает вычисления и делает задачу более устойчивой.\n",
        "\n",
        "2. **Удобство для градиентного спуска и аналитического решения**: В большинстве задач, включая линейную регрессию, логарифмированная функция правдоподобия легче оптимизируется, так как её градиенты имеют более простую форму. Это упрощает поиск оптимальных параметров $w$.\n",
        "\n",
        "3. **Стабильность при вычислениях**: Логарифмирование снижает риск ошибки округления при работе с очень малыми вероятностями, что может быть важно при большом объёме данных.\n",
        "\n",
        "Таким образом, логарифмическая функция правдоподобия является важным инструментом для максимизации правдоподобия в линейной регрессии, позволяя находить такие значения параметров $w$, которые обеспечивают наилучшее объяснение имеющихся данных в рамках предположений модели.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Минимизация целевой функции\n",
        "\n",
        "Целевая функция (или функционал) в задачах оптимизации — это функция, которую необходимо минимизировать или максимизировать в зависимости от поставленной задачи. В контексте линейной регрессии целевая функция служит критерием для оценки того, насколько хорошо модель соответствует данным.\n",
        "\n",
        "### Целевая функция линейной регрессии\n",
        "\n",
        "В линейной регрессии целевая функция, отражающая ошибку предсказания, обычно представляется следующим образом:\n",
        "\n",
        "$$\n",
        "E(w) = \\frac{1}{2} \\sum_{i=1}^{N} \\left( t_i - w^T \\phi(x_i) \\right)^2,\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $t_i$ — фактическое значение зависимой переменной для $i$-го наблюдения,\n",
        "- $w$ — вектор параметров модели (коэффициентов),\n",
        "- $\\phi(x_i)$ — вектор базисных функций для $i$-го наблюдения.\n",
        "\n",
        "Минимизация этой целевой функции позволяет найти оптимальные коэффициенты модели, которые наилучшим образом соответствуют данным, тем самым создавая наилучшую прямую (или плоскость в случае многомерной регрессии), описывающую зависимость между независимыми и зависимой переменными.\n",
        "\n",
        "### Важность минимизации целевой функции\n",
        "\n",
        "Минимизация целевой функции в линейной регрессии критически важна для нахождения значений коэффициентов, которые оптимально объясняют зависимую переменную на основе предикторов. Это позволяет строить модели, способные эффективно прогнозировать или анализировать зависимую переменную.\n",
        "\n",
        "### Отличие целевой функции от функции потерь\n",
        "\n",
        "Хотя целевая функция и функция потерь могут совпадать в некоторых случаях, они имеют разные роли в контексте оптимизации:\n",
        "\n",
        "- **Целевая функция** — это общее понятие, описывающее функцию, которую необходимо минимизировать или максимизировать. В случае линейной регрессии целевая функция направлена на нахождение наилучших параметров модели для объяснения зависимости между переменными.\n",
        "  \n",
        "- **Функция потерь** — это более специфический термин, который обычно используется для оценки ошибок предсказания модели. Она показывает, насколько сильно предсказанные значения отличаются от фактических. В линейной регрессии функция потерь часто представляется в виде средней квадратичной ошибки (MSE), которая фактически является целевой функцией.\n",
        "\n",
        "Таким образом, целевая функция может быть функцией потерь, но не всегда. В других задачах оптимизации целевая функция может иметь другие формы, в зависимости от контекста и требований.\n",
        "\n",
        "### Методы оптимизации\n",
        "\n",
        "Для минимизации целевой функции в линейной регрессии можно использовать различные методы оптимизации:\n",
        "\n",
        "- **Метод наименьших квадратов (МНК)** — находит коэффициенты, минимизируя сумму квадратов ошибок.\n",
        "- **Градиентный спуск** — итеративный метод, который обновляет параметры модели в направлении антиградиента функции потерь.\n",
        "- **Нормальное уравнение** — аналитический метод, позволяющий находить оптимальные параметры без необходимости итераций.\n",
        "\n",
        "После минимизации целевой функции и нахождения оптимальных значений коэффициентов модель становится более точной и эффективной в предсказании зависимой переменной на основе предикторов.\n",
        "\n",
        "## Функция потерь\n",
        "\n",
        "**Функция потерь** — это мера ошибок предсказания модели. В задаче регрессии такой мерой может служить расстояние между предсказанным значением $f(x)$ и его фактическим значением $y$.\n",
        "\n",
        "Одной из распространенных функций потерь является **средняя квадратичная ошибка** (MSE), которая совпадает с нашей целевой функцией:\n",
        "\n",
        "$$\n",
        "\\text{loss} = E(w) = \\frac{1}{2} \\sum_{i=1}^{N} \\left( y_i - f(x_i) \\right)^2,\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $y_i$ — фактическое значение зависимой переменной для $i$-го наблюдения,\n",
        "- $f(x_i) = w^T \\phi(x_i)$ — предсказанное значение зависимой переменной, полученное с использованием модели с параметрами $w$ и базисными функциями $\\phi$.\n",
        "\n",
        "### Оптимизация функции потерь\n",
        "\n",
        "Поиск оптимальных параметров модели сводится к задаче нахождения минимума функции потерь. Это можно сделать с использованием различных алгоритмов оптимизации, в зависимости от требований к вычислительной эффективности и точности. Например, при использовании градиентного спуска параметры обновляются следующим образом:\n",
        "\n",
        "$$\n",
        "w^{(t+1)} = w^{(t)} - \\eta \\nabla E(w^{(t)}),\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $w^{(t)}$ — текущее значение параметров,\n",
        "- $\\eta$ — скорость обучения,\n",
        "- $\\nabla E(w^{(t)})$ — градиент функции потерь по параметрам.\n",
        "\n",
        "Подробно рассмотрим метод градиентного спуска в следующих разделах.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Поиск локального минимума\n",
        "\n",
        "Рассмотрим нахождение локального минимума для простой линейной регрессии. Необходимым (но недостаточным) условием для нахождения локального минимума дифференцируемой функции является равенство нулю частных производных:\n",
        "\n",
        "$$\n",
        "\\begin{equation*}\n",
        "\\begin{cases}\n",
        "\\displaystyle\\frac{\\partial E(w)}{\\partial w_0} = 0, \\$$5pt]\n",
        "\\displaystyle\\frac{\\partial E(w)}{\\partial w_1} = 0.\n",
        "\\end{cases}\n",
        "\\end{equation*}\n",
        "$$\n",
        "\n",
        "Эти уравнения указывают на то, что для нахождения локального минимума производные функции потерь по всем параметрам должны равняться нулю. В случае среднеквадратичной ошибки (MSE) для линейной регрессии мы можем вычислить частные производные и определить точки, в которых функция достигает экстремумов.\n",
        "\n",
        "Так как MSE для линейной регрессии является полиномом второй степени относительно параметров $w_0$ и $w_1$, можно утверждать, что локальный минимум будет одновременно и глобальным минимумом.\n",
        "\n",
        "## Метод наименьших квадратов\n",
        "\n",
        "Для простой линейной регрессии функция потерь обычно определяется как среднеквадратичная ошибка (Mean Squared Error, MSE) и имеет следующий вид:\n",
        "\n",
        "$$\n",
        "E(w) = \\frac{1}{2} \\sum_{i=1}^{n} \\left( y_i - (w_0 + w_1 x_i) \\right)^2,\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $E(w)$ — функция потерь, которую мы стремимся минимизировать при настройке параметров модели.\n",
        "- $w$ — вектор параметров модели. В данном случае $w_0$ и $w_1$ — это коэффициенты регрессии, которые мы хотим определить.\n",
        "- $n$ — количество наблюдений в нашем наборе данных.\n",
        "- $y_i$ — фактическое значение зависимой переменной для $i$-го наблюдения.\n",
        "- $x_i$ — значение независимой переменной для $i$-го наблюдения.\n",
        "- Формула $y_i - (w_0 + w_1 x_i)$ представляет собой разницу между фактическим и предсказанным значением зависимой переменной для $i$-го наблюдения.\n",
        "- Квадрат этой разницы используется для учета как положительных, так и отрицательных отклонений от фактического значения.\n",
        "- Сумма квадратов ошибок (среднеквадратичная ошибка, MSE) дает общую меру того, насколько хорошо модель соответствует данным.\n",
        "\n",
        "Наша цель заключается в подборе значений параметров $w_0$ и $w_1$ так, чтобы минимизировать эту функцию потерь, обеспечивая наилучшее соответствие модели данным.\n",
        "\n",
        "### Оптимизация методом наименьших квадратов\n",
        "\n",
        "Чтобы найти значения параметров $w_0$ и $w_1$, минимизирующие функцию потерь, мы можем использовать метод наименьших квадратов, который включает в себя следующие шаги:\n",
        "\n",
        "1. **Запись функции потерь:** Определяем функцию потерь $E(w)$ как среднеквадратичную ошибку.\n",
        "\n",
        "2. **Вычисление частных производных:** Вычисляем частные производные функции потерь по параметрам $w_0$ и $w_1$:\n",
        "\n",
        "#### 1. Вычисление производных функции потерь по параметрам $w_0$ и $w_1$\n",
        "\n",
        "Для нахождения оптимальных значений параметров $w_0$ и $w_1$ необходимо вычислить частные производные функции потерь $E(w)$:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial E}{\\partial w_0} = -\\sum_{i=1}^{n} (y_i - (w_0 + w_1 x_i)),\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial E}{\\partial w_1} = -\\sum_{i=1}^{n} x_i (y_i - (w_0 + w_1 x_i)).\n",
        "$$\n",
        "\n",
        "#### 2. Приравнивание производных к нулю и решение системы уравнений\n",
        "\n",
        "После вычисления производных, приравниваем их к нулю и решаем систему уравнений:\n",
        "\n",
        "$$\n",
        "-\\sum_{i=1}^{n} (y_i - (w_0 + w_1 x_i)) = 0,\n",
        "$$\n",
        "\n",
        "$$\n",
        "-\\sum_{i=1}^{n} x_i (y_i - (w_0 + w_1 x_i)) = 0.\n",
        "$$\n",
        "\n",
        "#### 3. Нахождение оптимальных значений $w_0$ и $w_1$\n",
        "\n",
        "Решив полученную систему уравнений, можно найти оптимальные значения параметров $w_0$ и $w_1$, которые минимизируют функцию потерь MSE:\n",
        "\n",
        "$$\n",
        "w_1 = \\frac{n \\sum_{i=1}^{n} x_i y_i - \\left( \\sum_{i=1}^{n} x_i \\right) \\left( \\sum_{i=1}^{n} y_i \\right)}{n \\sum_{i=1}^{n} x_i^2 - \\left( \\sum_{i=1}^{n} x_i \\right)^2},\n",
        "$$\n",
        "\n",
        "$$\n",
        "w_0 = \\frac{\\sum_{i=1}^{n} y_i - w_1 \\left( \\sum_{i=1}^{n} x_i \\right)}{n}.\n",
        "$$\n",
        "\n",
        "Для повышения вычислительной эффективности часто используют матричный подход для определения модели линейной регрессии и выполнения последующего анализа.\n",
        "\n",
        "### Матричный подход в линейной регрессии\n",
        "\n",
        "Уравнения простой линейной регрессии можно записать в матричной форме следующим образом:\n",
        "\n",
        "$$\n",
        "Y = Xw + \\varepsilon,\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $Y = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}$ — вектор зависимой переменной.\n",
        "- $X = \\begin{bmatrix} 1 & x_1 \\\\ 1 & x_2 \\\\ \\vdots & \\vdots \\\\ 1 & x_n \\end{bmatrix}$ — матрица независимых переменных, где первый столбец состоит из единиц для учета свободного члена.\n",
        "- $w = \\begin{bmatrix} w_0 \\\\ w_1 \\end{bmatrix}$ — вектор коэффициентов модели.\n",
        "- $\\varepsilon = \\begin{bmatrix} \\varepsilon_1 \\\\ \\varepsilon_2 \\\\ \\vdots \\\\ \\varepsilon_n \\end{bmatrix}$ — вектор ошибок модели.\n",
        "\n",
        "### Связи между матрицами\n",
        "\n",
        "Ранее мы нашли соотношения:\n",
        "\n",
        "1. $\\sum y_i = n w_0 + w_1 \\sum x_i$\n",
        "\n",
        "2. $\\sum x_i y_i = w_0 \\sum x_i + w_1 \\sum x_i^2$\n",
        "\n",
        "Известно, что:\n",
        "\n",
        "$$\n",
        "X^T \\cdot X =\n",
        "\\begin{bmatrix}\n",
        "1 & 1 & \\cdots & 1 \\\\\n",
        "x_1 & x_2 & \\cdots & x_n\n",
        "\\end{bmatrix}\n",
        "\\cdot\n",
        "\\begin{bmatrix}\n",
        "1 & x_1 \\\\\n",
        "1 & x_2 \\\\\n",
        "\\vdots & \\vdots \\\\\n",
        "1 & x_n\n",
        "\\end{bmatrix} =\n",
        "\\begin{bmatrix}\n",
        "n & \\sum x_i \\\\\n",
        "\\sum x_i & \\sum x_i^2\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "X^T \\cdot Y =\n",
        "\\begin{bmatrix}\n",
        "1 & 1 & \\cdots & 1 \\\\\n",
        "x_1 & x_2 & \\cdots & x_n\n",
        "\\end{bmatrix}\n",
        "\\cdot\n",
        "\\begin{bmatrix}\n",
        "y_1 \\\\\n",
        "y_2 \\\\\n",
        "\\vdots \\\\\n",
        "y_n\n",
        "\\end{bmatrix} =\n",
        "\\begin{bmatrix}\n",
        "\\sum y_i \\\\\n",
        "\\sum x_i y_i\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Таким образом, уравнения (1) и (2) эквивалентны следующей матричной формуле:\n",
        "\n",
        "$$\n",
        "X^T Y = X^T X w\n",
        "$$\n",
        "\n",
        "Отсюда мы можем получить:\n",
        "\n",
        "$$\n",
        "w = (X^T \\cdot X)^{-1} \\cdot X^T Y\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $w = \\begin{bmatrix} w_0 \\\\ w_1 \\end{bmatrix}$ — вектор коэффициентов модели (размерность $2 \\times 1$, где первый элемент — свободный член, а второй — коэффициент при независимой переменной),\n",
        "- $X$ — матрица независимых переменных (размерность $n \\times 2$),\n",
        "- $Y$ — вектор зависимой переменной (размерность $n \\times 1$),\n",
        "- $X^T$ — транспонированная матрица $X$,- $(X^T \\cdot X)^{-1}$ — обратная матрица от произведения транспонированной матрицы $X$ на матрицу $X$.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Функции базиса\n",
        "\n",
        "Напомним, что ранее мы вводили функции базиса, но не использовали их на практике:\n",
        "\n",
        "$$\n",
        "\\Phi(x) =\n",
        "\\begin{bmatrix}\n",
        "\\phi_0(x_1) & \\phi_1(x_1) & \\phi_2(x_1) & \\ldots & \\phi_p(x_1) \\\\\n",
        "\\phi_0(x_2) & \\phi_1(x_2) & \\phi_2(x_2) & \\ldots & \\phi_p(x_2) \\\\\n",
        "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "\\phi_0(x_n) & \\phi_1(x_n) & \\phi_2(x_n) & \\ldots & \\phi_p(x_n)\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Общая формула целевой функции для многомерной линейной регрессии записывается следующим образом:\n",
        "\n",
        "$$\n",
        "E(w) = \\frac{1}{2} \\sum_{i=1}^{N} \\left( t_i - w^T \\Phi(x_i) \\right)^2\n",
        "$$\n",
        "\n",
        "где $t_i$ — истинные значения зависимой переменной, а $w^T \\Phi(x_i)$ — предсказанные значения, полученные с использованием функций базиса.\n",
        "\n",
        "Таким образом, мы представили детализированный подход к минимизации функции потерь в линейной регрессии, используя как традиционные методы, так и матричное представление. Это позволяет более эффективно работать с многомерными данными и функциями базиса.\n",
        "\n",
        "Теперь, используя матрицу и общую целевую функцию, мы можем записать формулы для нахождения коэффициентов модели регрессии в общем виде с помощью метода наименьших квадратов.\n",
        "\n",
        "Для нахождения производной функции $E(w)$ по вектору параметров $w$ применим правило дифференцирования сложной функции и правило дифференцирования квадрата:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial E(w)}{\\partial w_k} = -\\frac{1}{2} \\cdot \\sum_{i=1}^{N} 2(t_i - w^T \\Phi(x_i)) \\cdot \\phi_k(x_i) = \\sum_{i=1}^{N} (w^T \\Phi(x_i) - t_i) \\cdot \\phi_k(x_i)\n",
        "$$\n",
        "\n",
        "В векторной форме производная будет записана следующим образом:\n",
        "\n",
        "$$\n",
        "\\nabla_w E = \\sum_{i=1}^{N} (w^T \\Phi(x_i) - t_i) \\cdot \\Phi(x_i)^T\n",
        "$$\n",
        "\n",
        "Это выражение позволяет находить градиент функции потерь $E(w)$ по вектору параметров $w$.\n",
        "\n",
        "Чтобы найти коэффициенты $w$, необходимо решить уравнение:\n",
        "\n",
        "$$\n",
        "\\nabla_w E = 0\n",
        "$$\n",
        "\n",
        "Если транспонировать обе стороны данного уравнения, результат останется тем же. Транспонирование не изменяет смысла уравнения:\n",
        "\n",
        "$$\n",
        "(\\nabla_w E)^T = 0^T\n",
        "$$\n",
        "\n",
        "Таким образом, мы получаем:\n",
        "\n",
        "$$\n",
        "\\nabla_w E^T = \\sum_{i=1}^{N} (w^T \\Phi(x_i) - t_i) \\cdot \\Phi(x_i)^T = \\sum_{i=1}^{N} \\left( w^T \\Phi(x_i) \\cdot \\Phi(x_i)^T - t_i \\cdot \\Phi(x_i)^T \\right)\n",
        "$$\n",
        "\n",
        "Теперь можем записать это в более развернутом виде:\n",
        "\n",
        "$$\n",
        "\\nabla_w E^T = w^T \\cdot \\sum_{i=1}^{N} \\Phi(x_i) \\cdot \\Phi(x_i)^T - \\sum_{i=1}^{N} t_i \\cdot \\Phi(x_i)^T\n",
        "$$\n",
        "\n",
        "### 1. Рассмотрим матрицу $\\sum_{i=1}^{N} \\Phi(x_i) \\cdot \\Phi(x_i)^T$\n",
        "\n",
        "Здесь мы умножаем вектор-столбец на вектор-строку. Если их размерности совпадают, получится матрица.\n",
        "\n",
        "В данном случае, если $\\Phi(x_i)$ — это вектор-столбец размерности $m \\times 1$, а $\\Phi(x_i)^T$ — это вектор-строка размерности $1 \\times m$, то их произведение даст матрицу размерности $m \\times m$:\n",
        "\n",
        "$$\n",
        "\\Phi(x_i) \\cdot \\Phi(x_i)^T =\n",
        "\\begin{bmatrix}\n",
        "\\phi_0(x_i) \\\\\n",
        "\\phi_1(x_i) \\\\\n",
        "\\vdots \\\\\n",
        "\\phi_p(x_i)\n",
        "\\end{bmatrix} \\cdot\n",
        "\\begin{bmatrix}\n",
        "\\phi_0(x_i) & \\phi_1(x_i) & \\ldots & \\phi_p(x_i)\n",
        "\\end{bmatrix} =\n",
        "\\begin{bmatrix}\n",
        "\\phi_0(x_i)^2 & \\phi_0(x_i) \\phi_1(x_i) & \\ldots & \\phi_0(x_i) \\phi_p(x_i) \\\\\n",
        "\\phi_1(x_i) \\phi_0(x_i) & \\phi_1(x_i)^2 & \\ldots & \\phi_1(x_i) \\phi_p(x_i) \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "\\phi_p(x_i) \\phi_0(x_i) & \\phi_p(x_i) \\phi_1(x_i) & \\ldots & \\phi_p(x_i)^2\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Таким образом, результат произведения вектора-столбца на вектор-строку дает квадратную матрицу:\n",
        "\n",
        "$$\n",
        "\\Phi(x_i) \\cdot \\Phi(x_i)^T = \\Phi \\cdot \\Phi^T\n",
        "$$\n",
        "\n",
        "### 2. Рассмотрим $\\sum_{i=1}^{N} t_i \\cdot \\Phi(x_i)^T$\n",
        "\n",
        "С одной стороны, можем записать:\n",
        "\n",
        "$$\n",
        "\\sum_{i=1}^{N} t_i \\cdot \\Phi(x_i)^T = \\left(\\sum_{i=1}^{N} t_i \\cdot \\phi_0(x_i), \\sum_{i=1}^{N} t_i \\cdot \\phi_1(x_i), \\ldots\\right)\n",
        "$$\n",
        "\n",
        "С другой стороны, это выражение можно представить в матричной форме:\n",
        "\n",
        "$$\n",
        "(t_1, t_2, \\ldots, t_N) \\cdot\n",
        "\\begin{bmatrix}\n",
        "\\phi_0(x_1) & \\phi_1(x_1) & \\phi_2(x_1) & \\ldots & \\phi_p(x_1) \\\\\n",
        "\\phi_0(x_2) & \\phi_1(x_2) & \\phi_2(x_2) & \\ldots & \\phi_p(x_2) \\\\\n",
        "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "\\phi_0(x_n) & \\phi_1(x_n) & \\phi_2(x_n) & \\ldots & \\phi_p(x_n)\n",
        "\\end{bmatrix} = \\left(\\sum_{i=1}^{N} t_i \\cdot \\phi_0(x_i), \\sum_{i=1}^{N} t_i \\cdot \\phi_1(x_i), \\ldots\\right)\n",
        "$$\n",
        "\n",
        "Следовательно:\n",
        "\n",
        "$$\n",
        "\\sum_{i=1}^{N} t_i \\cdot \\Phi(x_i)^T = t^T \\cdot \\Phi\n",
        "$$\n",
        "\n",
        "### 3. Подставляем выражения в градиент\n",
        "\n",
        "Таким образом, подставляя полученные результаты, мы имеем:\n",
        "\n",
        "$$\n",
        "\\nabla_w E^T = w^T \\Phi^T \\Phi - t^T \\Phi^T\n",
        "$$\n",
        "\n",
        "Теперь можем записать уравнение, приравняв его к нулю:\n",
        "\n",
        "$$\n",
        "w^T \\Phi^T \\Phi - t^T \\Phi^T = 0\n",
        "$$\n",
        "\n",
        "Отсюда следует:\n",
        "\n",
        "$$\n",
        "w^T \\Phi^T \\Phi = t^T \\Phi^T\n",
        "$$\n",
        "\n",
        "Умножим обе части последнего равенства на $(\\Phi^T \\Phi)^{-1}$:\n",
        "\n",
        "$$\n",
        "w^T \\Phi^T \\Phi \\cdot (\\Phi^T \\Phi)^{-1} = t^T \\Phi^T \\cdot (\\Phi^T \\Phi)^{-1}\n",
        "$$\n",
        "\n",
        "После сокращения и некоторых преобразований получим:\n",
        "\n",
        "$$\n",
        "w^T = t^T \\Phi^T \\cdot (\\Phi^T \\Phi)^{-1}\n",
        "$$\n",
        "\n",
        "### 4. Применение свойств транспонирования\n",
        "\n",
        "Чтобы решить уравнение $x^T = A$, где $x$ и $A$ — матрицы, необходимо транспонировать обе стороны уравнения. После транспонирования получаем:\n",
        "\n",
        "$$\n",
        "(x^T)^T = A^T\n",
        "$$\n",
        "\n",
        "Так как транспонирование дважды возвращает исходную матрицу, то:\n",
        "\n",
        "\n",
        "\n",
        "$$\n",
        "x = A^T\n",
        "$$\n",
        "\n",
        "### 5. Финальное выражение для коэффициентов\n",
        "\n",
        "Таким образом, получаем:\n",
        "\n",
        "$$\n",
        "w = (t^T \\Phi^T \\cdot (\\Phi^T \\Phi)^{-1})^T\n",
        "$$\n",
        "\n",
        "Из курса линейной алгебры известно, что:\n",
        "\n",
        "$$\n",
        "(A \\cdot B)^T = A^T \\cdot B^T \\quad \\text{и} \\quad (A^T)^{-1} = (A^{-1})^T\n",
        "$$\n",
        "\n",
        "Тогда у нас будет:\n",
        "\n",
        "$$\n",
        "w = (\\Phi^T \\Phi)^{-1} \\Phi^T t^T\n",
        "$$\n",
        "\n",
        "\n",
        "Таким образом, мы вывели формулы для коэффициентов линейной регрессии в общем виде, используя метод наименьших квадратов и матричный подход. Это позволяет эффективно рассчитывать оптимальные параметры модели, минимизируя ошибку между предсказанными значениями и реальными наблюдениями.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Вопрос на подумать\n",
        "\n",
        "*Для вычисления $w$ нам приходится обращать (квадратную) матрицу $\\Phi^T \\Phi$, что возможно только если она невырожденна. Что это значит с точки зрения анализа данных? Почему мы верим, что это выполняется во всех разумных ситуациях?*\n",
        "\n",
        "### Ответ\n",
        "\n",
        "С точки зрения линейной алгебры, для вещественной матрицы $\\Phi$ ранги матриц $\\Phi$ и $\\Phi^T \\Phi$ совпадают. Матрица $\\Phi^T \\Phi$ будет невырожденной, то есть обратимой, тогда и только тогда, когда её ранг равен числу её столбцов, что также равно числу столбцов матрицы $\\Phi$. Иными словами, формула регрессии поломается, только если столбцы матрицы $\\Phi$ линейно зависимы.\n",
        "\n",
        "В данной ситуации столбцы матрицы $\\Phi$ представляют собой признаки (фичи) нашего набора данных. Если признаки линейно зависимы, это значит, что один или несколько признаков могут быть выражены как линейные комбинации других признаков. Это приводит к проблеме избыточности информации. Когда признаки линейно зависимы, необходимо оставить только линейно независимые признаки, так как зависимые признаки не добавляют новой информации и лишь усложняют вычисления. Таким образом, чтобы избежать вырождения, мы обычно следим за тем, чтобы признаки были линейно независимыми.\n",
        "\n",
        "Однако на практике часто встречается ситуация, когда признаки приближённо линейно зависимы, особенно если их много. В таких случаях матрица $\\Phi^T \\Phi$ будет близка к вырожденной, что приводит к ряду вычислительных проблем. Как мы увидим далее, такие случаи плохо сказываются на численной устойчивости решения и могут потребовать дополнительных методов для его стабилизации.\n",
        "\n",
        "## Вычислительная сложность аналитического решения\n",
        "\n",
        "Аналитическое решение задачи линейной регрессии имеет вычислительную сложность $O(D^2 N + D^3)$, где:\n",
        "- $N$ — это количество объектов в выборке,\n",
        "- $D$ — это количество признаков у каждого объекта.\n",
        "\n",
        "### Разбор компонентов вычислительной сложности\n",
        "\n",
        "1. **Слагаемое $O(D^2 N)$** отвечает за сложность перемножения матриц $\\Phi^T$ и $\\Phi$. Для нахождения матрицы $\\Phi^T \\Phi$ требуется выполнить $N$ умножений для каждого из $D \\times D$ элементов.\n",
        "\n",
        "2. **Слагаемое $O(D^3)$** обусловлено сложностью обращения матрицы $\\Phi^T \\Phi$. Поскольку эта матрица размерности $D \\times D$, стандартные методы обращения, такие как метод Гаусса или метод разложения Холецкого, требуют $O(D^3)$ операций.\n",
        "\n",
        "Чтобы минимизировать вычислительные затраты, не рекомендуется перемножать матрицы в выражении $(\\Phi^T \\Phi)^{-1} \\cdot \\Phi^T$ целиком. Вместо этого целесообразно сначала умножить вектор $t$ на $\\Phi^T$, а затем результат умножить на $(\\Phi^T \\Phi)^{-1}$. Такой подход позволяет значительно снизить объём вычислений и уменьшает необходимость хранения матрицы $(\\Phi^T \\Phi)^{-1} \\cdot \\Phi^T$ в памяти, что особенно важно при больших размерах данных.\n",
        "\n",
        "### Ускорение вычислений\n",
        "\n",
        "Вычисление можно ускорить, используя продвинутые алгоритмы перемножения матриц, такие как метод Штрассена или метод Винограда, которые снижают степень вычислительной сложности. Кроме того, для больших матриц эффективны итерационные методы обращения, такие как метод Якоби или метод сопряжённых градиентов, которые позволяют обойтись без явного обращения матрицы, что также повышает стабильность численных решений.\n",
        "\n",
        "## Проблемы «точного» решения\n",
        "\n",
        "Для нахождения коэффициентов регрессии требуется обращение матрицы $\\Phi^T \\Phi$, что порождает несколько серьёзных проблем:\n",
        "\n",
        "1. **Сложность обращения больших матриц.**  \n",
        "   Обращение больших матриц вычислительно затратно, а в задачах машинного обучения часто необходимо работать с датасетами, содержащими миллионы точек. В таких случаях обращение может потребовать значительных вычислительных ресурсов, и для оптимизации приходится применять приближённые или итерационные методы.\n",
        "\n",
        "2. **Плохая обусловленность матрицы $\\Phi^T \\Phi$.**  \n",
        "   Даже если матрица $\\Phi^T \\Phi$ является обратимой, она может быть плохо обусловлена. Это происходит особенно часто, когда количество признаков велико. В такой ситуации некоторые признаки могут быть почти линейно зависимы от других, что приводит к тому, что $\\Phi^T \\Phi$ становится близкой к вырожденной. Плохая обусловленность означает, что малые изменения в данных могут привести к большим изменениям в итоговом решении $w$, что делает его численно неустойчивым.\n",
        "\n",
        "   Например, малое изменение в целевом векторе $t$ может привести к значительным изменениям в коэффициентах $w$. Это явление возникает из-за того, что погрешность результата будет зависеть от квадрата обусловленности матрицы $\\Phi$. В итоге решение становится ненадёжным: незначительные погрешности данных могут вызвать серьёзные ошибки в предсказаниях модели.\n",
        "\n",
        "3. **Численные ошибки при вычислении.**  \n",
        "   Плохая обусловленность ведет к накоплению численных ошибок. Даже при использовании высокоточных вычислений, ошибки округления при обращении матрицы $\\Phi^T \\Phi$ могут существенно исказить решение. Эта проблема особенно критична для систем с большой размерностью, где погрешности округления накапливаются в каждой операции и могут искажать конечный результат.\n",
        "\n",
        "\n",
        "\n",
        "Таким образом, обращение матрицы $\\Phi^T \\Phi$ при решении задачи линейной регрессии связано с рядом вычислительных и численных трудностей, особенно в случае больших данных и многомерных признаков. Для их преодоления разработаны методы регуляризации, такие как метод ридж-регрессии (регуляризация Тихонова), позволяющие стабилизировать решение путем добавления регуляризующего члена к матрице $\\Phi^T \\Phi$. Этот подход помогает улучшить обусловленность матрицы и снизить зависимость от линейных зависимостей между признаками, что в свою очередь делает модель более устойчивой к численным ошибкам и повышает её обобщающую способность.\n",
        "\n",
        "\n",
        "\n",
        "### Пара слов про число обусловленности\n",
        "\n",
        "С математической точки зрения, число обусловленности матрицы $\\Phi$ — это показатель, отражающий, насколько различаются масштабы её собственных значений. Упрощая, число обусловленности матрицы $\\Phi$ можно рассматривать как корень из отношения наибольшего и наименьшего собственных значений матрицы $\\Phi^T \\Phi$. Иными словами, оно показывает, насколько разного масштаба бывают собственные значения этой матрицы.\n",
        "\n",
        "Если рассмотреть $L^2$-норму ошибки предсказания как функцию от коэффициентов, то линии уровня этой функции представляют собой эллипсоиды. Форма этих эллипсоидов определяется квадратичной формой, заданной матрицей $\\Phi^T \\Phi$. Вытянутость этих эллипсоидов говорит о том, насколько сильно отличаются величины собственных значений $\\Phi^T \\Phi$, что и выражает число обусловленности. Высокое число обусловленности может сигнализировать о том, что матрица плохо обусловлена, что, в свою очередь, может привести к численным проблемам при вычислении.\n",
        "\n",
        "### Подробнее\n",
        "\n",
        "Проблемы с численной устойчивостью и высокой сложностью вычислений не означают, что «точное» решение необходимо отбросить. Существуют несколько методов для улучшения численных свойств решения. Однако для их полного понимания необходимо знание сингулярного разложения. Если оно вам не знакомо, рекомендуется сначала изучить этот материал и затем вернуться к данной теме.\n",
        "\n",
        "#### Метод 1: Использование QR-разложения\n",
        "\n",
        "Первый способ улучшения решения заключается в применении QR-разложения матрицы $X$. QR-разложение — это представление матрицы $X$ в виде произведения $X = QR$, где:\n",
        "- $Q$ — матрица с ортогональными столбцами (то есть $Q^T Q = E$),\n",
        "- $R$ — квадратная верхнетреугольная матрица.\n",
        "\n",
        "Подставив это разложение в исходное уравнение для $w$, получим:\n",
        "\n",
        "$$\n",
        "w = ((QR)^T QR)^{-1} (QR)^T y\n",
        "$$\n",
        "\n",
        "Раскроем выражение, используя свойства ортогональной матрицы $Q$:\n",
        "\n",
        "$$\n",
        "w = (R^T Q^T Q R)^{-1} R^T Q^T y = (R^T R)^{-1} R^T Q^T y = R^{-1} R^{-T} R^T Q^T y = R^{-1} Q^T y\n",
        "$$\n",
        "\n",
        "Здесь мы использовали тот факт, что $(R^T R)^{-1} = R^{-1} R^{-T}$. Полученное выражение значительно проще, поскольку обращение верхнетреугольной матрицы $R$ сводится к решению системы уравнений с верхнетреугольной левой частью, что позволяет выполнять вычисления быстрее и с меньшей численной погрешностью. Погрешность вычисления $w$ при этом будет зависеть от числа обусловленности матрицы $\\Phi$, а QR-разложение, как правило, обладает хорошей численной устойчивостью, что делает его применение предпочтительным для улучшения численных свойств решения.\n",
        "\n",
        "#### Метод 2: Использование псевдообратной матрицы через сингулярное разложение\n",
        "\n",
        "Другим подходом является применение псевдообратной матрицы, построенной с помощью сингулярного разложения. Пусть\n",
        "\n",
        "$$\n",
        "A = U \\underbrace{\\mathrm{diag}(\\sigma_1, \\ldots, \\sigma_r)}_{=\\Sigma} V^T\n",
        "$$\n",
        "\n",
        "— это усечённое сингулярное разложение матрицы $A$, где $r$ — ранг $A$. Здесь:\n",
        "- $\\Sigma$ — диагональная матрица, содержащая ненулевые сингулярные значения $\\sigma_i$,\n",
        "- $U$ и $V$ — ортогональные матрицы, такие что $U^T U = E$ и $V^T V = E$.\n",
        "\n",
        "В таком случае $w$ можно выразить следующим образом:\n",
        "\n",
        "$$\n",
        "w = (V \\Sigma U^T U \\Sigma V^T)^{-1} V \\Sigma U^T y\n",
        "$$\n",
        "\n",
        "Используем свойства ортогональных матриц и диагональной матрицы $\\Sigma$. Поскольку $V \\Sigma^{-2} V^T \\cdot V \\Sigma^2 V^T = E$, верно, что\n",
        "\n",
        "$$\n",
        "(V \\Sigma^2 V^T)^{-1} = V \\Sigma^{-2} V^T,\n",
        "$$\n",
        "\n",
        "что позволяет упростить выражение:\n",
        "\n",
        "$$\n",
        "w = V \\Sigma^{-2} V^T \\Sigma U^T y = V \\Sigma^{-1} U^T y\n",
        "$$\n",
        "\n",
        "Применение сингулярного разложения гарантирует численную устойчивость решения, так как сингулярное разложение хорошо себя ведёт при вычислениях, даже если матрица плохо обусловлена. Это улучшает численные свойства решения и делает его более надёжным.\n",
        "\n",
        "Тем не менее, несмотря на преимущества сингулярного разложения, вычислительная сложность остаётся значительной, особенно для больших матриц. Плохая обусловленность матрицы $X$ всё равно окажет влияние, хоть и в меньшей степени, на точность вычислений.\n",
        "\n",
        "Хотя эти методы помогают улучшить численную устойчивость и стабилизировать решение, они не могут полностью устранить проблемы, связанные с вырожденностью или плохой обусловленностью матрицы $X$. Поэтому нет необходимости останавливаться на «точных» решениях, которые всё равно никогда не будут полностью точными в условиях реальных данных. В следующих разделах мы рассмотрим альтернативные подходы к решению задачи, которые не полагаются на обращение матрицы и могут дать более надёжные результаты в условиях плохой обусловленности.\n",
        "\n",
        "\n",
        "\n",
        "### Почему градиент указывает направление наибольшего возрастания функции?\n",
        "\n",
        "**Что такое градиент функции?**\n",
        "\n",
        "Градиент функции многих переменных — это вектор, составленный из частных производных этой функции по каждой переменной. Он показывает направление наибольшего возрастания функции в данной точке и его величина указывает на скорость изменения функции вдоль этого направления. Градиент используется в задачах оптимизации для нахождения экстремумов, так как именно он указывает направление наискорейшего увеличения функции.\n",
        "\n",
        "Для функции $f(x_1, x_2, \\dots, x_n)$ градиент обозначается символом $\\nabla f$ и формируется следующим образом:\n",
        "\n",
        "$$\n",
        "\\nabla f = \\left( \\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\dots, \\frac{\\partial f}{\\partial x_n} \\right)\n",
        "$$\n",
        "\n",
        "Этот вектор указывает направление наибольшего возрастания функции в данной точке.\n",
        "\n",
        "**Что такое производная по направлению?**\n",
        "\n",
        "Производная по направлению — это производная функции вдоль заданного вектора. Пусть $\\vec{u} = (u_1, u_2, \\dots, u_n)$ — нормированный вектор (то есть $||\\vec{u}|| = 1$), вдоль которого берется производная функции $f(x_1, x_2, \\dots, x_n)$. Норма вектора $\\vec{u}$ определяется как:\n",
        "\n",
        "$$\n",
        "||\\vec{u}|| = \\sqrt{u_1^2 + u_2^2 + \\dots + u_n^2}\n",
        "$$\n",
        "\n",
        "Производная функции $f$ по направлению вектора $\\vec{u}$ определяется как предел:\n",
        "\n",
        "$$\n",
        "\\lim_{{h \\to 0}} \\frac{f(\\vec{x} + h \\cdot \\vec{u}) - f(\\vec{x})}{h}\n",
        "$$\n",
        "\n",
        "где $\\vec{x} + h \\cdot \\vec{u} = (x_1 + h \\cdot u_1, x_2 + h \\cdot u_2, \\dots, x_n + h \\cdot u_n)$. Эта формула показывает изменение функции вдоль направления, заданного вектором $\\vec{u}$.\n",
        "\n",
        "### Доказательство, что градиент указывает направление наибольшего возрастания функции\n",
        "\n",
        "Рассмотрим производную функции $f$ по направлению вектора $\\vec{u}$:\n",
        "\n",
        "$$\n",
        "\\lim_{{h \\to 0}} \\frac{f(x_1 + h \\cdot u_1, x_2 + h \\cdot u_2, \\dots, x_n + h \\cdot u_n) - f(x_1, x_2, \\dots, x_n)}{h}\n",
        "$$\n",
        "\n",
        "Мы можем разложить эту производную, выделяя отдельные слагаемые для каждого индекса:\n",
        "\n",
        "$$\n",
        "\\lim_{{h \\to 0}} \\sum_{i=1}^{n} \\frac{f(x_1, \\dots, x_i + h \\cdot u_i, \\dots, x_n) - f(x_1, \\dots, x_n)}{h}\n",
        "$$\n",
        "\n",
        "Эта сумма почти представляет собой выражение для частной производной функции. Если умножить каждый член на $u_i$ и затем разделить на $u_i$, мы получим следующее:\n",
        "\n",
        "$$\n",
        "\\lim_{{h \\to 0}} \\sum_{i=1}^{n} \\frac{f(x_1, \\dots, x_i + h \\cdot u_i, \\dots, x_n) - f(x_1, \\dots, x_n)}{h \\cdot u_i} \\cdot u_i = \\nabla f^T \\cdot \\vec{u}\n",
        "$$\n",
        "\n",
        "Скалярное произведение двух векторов $\\vec{a}$ и $\\vec{b}$ можно выразить как:\n",
        "\n",
        "$$\n",
        "\\vec{a}^T \\cdot \\vec{b} = ||\\vec{a}|| \\cdot ||\\vec{b}|| \\cos(\\theta)\n",
        "$$\n",
        "\n",
        "где $\\theta$ — угол между векторами. Таким образом, для градиента мы имеем:\n",
        "\n",
        "$$\n",
        "\\nabla f^T \\cdot \\vec{u} = ||\\nabla f|| \\cdot \\cos(\\theta)\n",
        "$$\n",
        "\n",
        "Производная по направлению отражает скорость возрастания функции в заданном направлении $\\vec{u}$. Чтобы максимизировать скорость роста функции, вектор $\\vec{u}$ должен быть направлен так, чтобы угол $\\theta = 0$, то есть векторы $\\nabla f$ и $\\vec{u}$ должны совпадать по направлению. Таким образом, направление градиента действительно является направлением наибольшего возрастания функции.\n",
        "\n",
        "### Ортогональность градиента к линиям уровня\n",
        "\n",
        "Градиент также обладает свойством ортогональности к линиям уровня функции, что важно для визуализации процесса оптимизации. Доказательство этого свойства выглядит следующим образом.\n",
        "\n",
        "Пусть $x_0$ — некоторая точка, и $S(x_0) = \\{ x \\in \\mathbb{R}^d \\mid f(x) = f(x_0) \\}$ — линия уровня функции $f$, соответствующая этой точке. Разложим функцию в ряд Тейлора на линии уровня в окрестности точки $x_0$:\n",
        "\n",
        "$$\n",
        "f(x_0 + \\varepsilon) = f(x_0) + \\langle \\nabla f, \\varepsilon \\rangle + o(||\\varepsilon||),\n",
        "$$\n",
        "\n",
        "где $x_0 + \\varepsilon \\in S(x_0)$. Поскольку $f(x_0 + \\varepsilon) = f(x_0)$ (это свойство линии уровня), получаем:\n",
        "\n",
        "$$\n",
        "\\langle \\nabla f, \\varepsilon \\rangle = o(||\\varepsilon||).\n",
        "$$\n",
        "\n",
        "Разделив обе части на $||\\varepsilon||$, имеем:\n",
        "\n",
        "$$\n",
        "\\left\\langle \\nabla f, \\frac{\\varepsilon}{||\\varepsilon||} \\right\\rangle = o(1).\n",
        "$$\n",
        "\n",
        "При стремлении $||\\varepsilon||$ к нулю вектор $\\frac{\\varepsilon}{||\\varepsilon||}$ будет стремиться к касательной к линии уровня в точке $x_0$. В пределе мы получаем, что градиент ортогонален этой касательной.\n",
        "\n",
        "Таким образом, градиент функции является мощным инструментом в математике и оптимизации, указывая направление наибольшего возрастания функции и демонстрируя важные свойства, такие как ортогональность к линиям уровня. Понимание этих концепций позволяет более эффективно решать задачи оптимизации и анализировать поведение многомерных функций.\n",
        "\n",
        "\n",
        "Чтобы лучше понять концепцию градиента и его применение, давайте рассмотрим несколько конкретных примеров.\n",
        "\n",
        "### Пример 1: Градиент функции двух переменных\n",
        "\n",
        "Рассмотрим функцию:\n",
        "\n",
        "$$\n",
        "f(x, y) = x^2 + y^2\n",
        "$$\n",
        "\n",
        "**1. Вычисление градиента:**\n",
        "\n",
        "Градиент функции $f$ будет вычисляться как:\n",
        "\n",
        "$$\n",
        "\\nabla f = \\left( \\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y} \\right)\n",
        "$$\n",
        "\n",
        "Найдём частные производные:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial f}{\\partial x} = 2x, \\quad \\frac{\\partial f}{\\partial y} = 2y\n",
        "$$\n",
        "\n",
        "Таким образом, градиент равен:\n",
        "\n",
        "$$\n",
        "\\nabla f = (2x, 2y)\n",
        "$$\n",
        "\n",
        "**2. Интерпретация:**\n",
        "\n",
        "Градиент показывает направление наибольшего возрастания функции. Например, в точке $(1, 1)$:\n",
        "\n",
        "$$\n",
        "\\nabla f(1, 1) = (2 \\cdot 1, 2 \\cdot 1) = (2, 2)\n",
        "$$\n",
        "\n",
        "Это означает, что в точке $(1, 1)$ функция $f$ возрастает быстрее всего в направлении вектора $(2, 2)$.\n",
        "\n",
        "**3. Линии уровня:**\n",
        "\n",
        "Линии уровня для данной функции представляют собой круги, заданные уравнением $x^2 + y^2 = c$. Градиент будет направлен от центра этих кругов к их границе, что подтверждает, что он ортогонален линиям уровня.\n",
        "\n",
        "### Пример 2: Градиент функции с ограничениями\n",
        "\n",
        "Рассмотрим функцию:\n",
        "\n",
        "$$\n",
        "g(x, y) = 4 - x^2 - y^2\n",
        "$$\n",
        "\n",
        "**1. Вычисление градиента:**\n",
        "\n",
        "Найдём градиент функции $g$:\n",
        "\n",
        "$$\n",
        "\\nabla g = \\left( \\frac{\\partial g}{\\partial x}, \\frac{\\partial g}{\\partial y} \\right) = (-2x, -2y)\n",
        "$$\n",
        "\n",
        "**2. Интерпретация:**\n",
        "\n",
        "Градиент в точке $(1, 1)$:\n",
        "\n",
        "$$\n",
        "\\nabla g(1, 1) = (-2 \\cdot 1, -2 \\cdot 1) = (-2, -2)\n",
        "$$\n",
        "\n",
        "Это означает, что в точке $(1, 1)$ функция $g$ убывает быстрее всего в направлении вектора $(-2, -2)$.\n",
        "\n",
        "**3. Линии уровня:**\n",
        "\n",
        "Линии уровня для этой функции будут представлять собой окружности, заданные уравнением $x^2 + y^2 = c$, что также подтверждает, что градиент направлен внутрь окружностей, оставаясь ортогональным им.\n",
        "\n",
        "### Пример 3: Градиент в многомерной функции\n",
        "\n",
        "Рассмотрим функцию:\n",
        "\n",
        "$$\n",
        "h(x, y, z) = x^2 + y^2 + z^2\n",
        "$$\n",
        "\n",
        "**1. Вычисление градиента:**\n",
        "\n",
        "Найдём градиент функции $h$:\n",
        "\n",
        "$$\n",
        "\\nabla h = \\left( \\frac{\\partial h}{\\partial x}, \\frac{\\partial h}{\\partial y}, \\frac{\\partial h}{\\partial z} \\right) = (2x, 2y, 2z)\n",
        "$$\n",
        "\n",
        "**2. Интерпретация:**\n",
        "\n",
        "Градиент в точке $(1, 1, 1)$:\n",
        "\n",
        "$$\n",
        "\\nabla h(1, 1, 1) = (2 \\cdot 1, 2 \\cdot 1, 2 \\cdot 1) = (2, 2, 2)\n",
        "$$\n",
        "\n",
        "Это означает, что в точке $(1, 1, 1)$ функция $h$ возрастает быстрее всего в направлении вектора $(2, 2, 2)$.\n",
        "\n",
        "**3. Линии уровня:**\n",
        "\n",
        "Линии уровня для функции $h$ будут представлять собой сферы, заданные уравнением $x^2 + y^2 + z^2 = c$. Градиент будет направлен от центра сфер к их поверхности, что также подтверждает его ортогональность к линиям уровня.\n",
        "\n",
        "### Пример 4: Градиент в задачах оптимизации\n",
        "\n",
        "Предположим, мы хотим минимизировать функцию потерь в регрессионной модели, заданной следующей функцией:\n",
        "\n",
        "$$\n",
        "L(w) = \\sum_{i=1}^{n} (y_i - (w_1 x_{1i} + w_2 x_{2i}))^2\n",
        "$$\n",
        "\n",
        "где $w = (w_1, w_2)$ — вектор весов, $x_{1i}, x_{2i}$ — входные данные, а $y_i$ — целевые значения.\n",
        "\n",
        "**1. Вычисление градиента:**\n",
        "\n",
        "Градиент функции потерь будет иметь вид:\n",
        "\n",
        "$$\n",
        "\\nabla L(w) = \\left( \\frac{\\partial L}{\\partial w_1}, \\frac{\\partial L}{\\partial w_2} \\right)\n",
        "$$\n",
        "\n",
        "При нахождении производных получаем:\n",
        "\n",
        "$$\n",
        "\\nabla L(w) = \\left( -2 \\sum_{i=1}^{n} x_{1i} (y_i - (w_1 x_{1i} + w_2 x_{2i})), -2 \\sum_{i=1}^{n} x_{2i} (y_i - (w_1 x_{1i} + w_2 x_{2i})) \\right)\n",
        "$$\n",
        "\n",
        "**2. Интерпретация:**\n",
        "\n",
        "Градиент указывает на то, как изменить веса $w_1$ и $w_2$, чтобы минимизировать функцию потерь $L(w)$. Направление, указанное градиентом, показывает, как нужно корректировать веса, чтобы достигнуть оптимального результата.\n",
        "\n",
        "Эти примеры демонстрируют, как градиент помогает понять поведение многомерных функций, указывая направление наибольшего возрастания и предоставляя полезные инструменты для задач оптимизации. Понимание этих концепций позволяет более эффективно решать практические задачи в математике и машинном обучении. Если у вас есть вопросы или нужны дополнительные примеры, дайте знать!\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Пример применения\n",
        "\n",
        "Рассмотрим простой пример с использованием метода наименьших квадратов для линейной регрессии, основанный на наборе данных, где мы исследуем влияние цены на объем продаж.\n",
        "\n",
        "#### Данные\n",
        "\n",
        "Предположим, у нас есть следующие данные о ценах и объемах продаж:\n",
        "\n",
        "| Цена (X) | Объем продаж (Y) |\n",
        "|----------|------------------|\n",
        "| 10       | 100              |\n",
        "| 20       | 150              |\n",
        "| 30       | 200              |\n",
        "| 40       | 300              |\n",
        "| 50       | 400              |\n",
        "\n",
        "#### 1. Определим функции базиса\n",
        "\n",
        "Мы будем использовать линейные функции базиса:\n",
        "\n",
        "- $\\phi_0(x) = 1$ (константа)\n",
        "- $\\phi_1(x) = x$ (линейная функция)\n",
        "\n",
        "#### 2. Сформируем матрицу функций базиса\n",
        "\n",
        "Для нашей выборки получаем матрицу:\n",
        "\n",
        "$$\n",
        "\\Phi =\n",
        "\\begin{bmatrix}\n",
        "1 & 10 \\\\\n",
        "1 & 20 \\\\\n",
        "1 & 30 \\\\\n",
        "1 & 40 \\\\\n",
        "1 & 50\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "#### 3. Определим вектор целевой переменной\n",
        "\n",
        "Вектор целевой переменной:\n",
        "\n",
        "$$\n",
        "t =\n",
        "\\begin{bmatrix}\n",
        "100 \\\\\n",
        "150 \\\\\n",
        "200 \\\\\n",
        "300 \\\\\n",
        "400\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "#### 4. Подставим данные в формулу\n",
        "\n",
        "Теперь можем найти коэффициенты $w$:\n",
        "\n",
        "1. Вычислим $\\Phi^T \\Phi$:\n",
        "\n",
        "$$\n",
        "\\Phi^T \\Phi =\n",
        "\\begin{bmatrix}\n",
        "5 & 150 \\\\\n",
        "150 & 3850\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "2. Найдем обратную матрицу $(\\Phi^T \\Phi)^{-1}$:\n",
        "\n",
        "$$\n",
        "(\\Phi^T \\Phi)^{-1} = \\frac{1}{(5)(3850) - (150)(150)}\n",
        "\\begin{bmatrix}\n",
        "3850 & -150 \\\\\n",
        "-150 & 5\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "После вычислений получаем:\n",
        "\n",
        "$$\n",
        "(\\Phi^T \\Phi)^{-1} = \\frac{1}{(19250 - 22500)}\n",
        "\\begin{bmatrix}\n",
        "3850 & -150 \\\\\n",
        "-150 & 5\n",
        "\\end{bmatrix}\n",
        "= \\frac{1}{-3250}\n",
        "\\begin{bmatrix}\n",
        "3850 & -150 \\\\\n",
        "-150 & 5\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "-1.1846 & 0.04615 \\\\\n",
        "0.04615 & -0.0015385\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "3. Вычислим $\\Phi^T t$:\n",
        "\n",
        "$$\n",
        "\\Phi^T t =\n",
        "\\begin{bmatrix}\n",
        "1300 \\\\\n",
        "28500\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "4. Теперь можем вычислить $w$:\n",
        "\n",
        "$$\n",
        "w = (\\Phi^T \\Phi)^{-1} \\Phi^T t\n",
        "$$\n",
        "\n",
        "#### 5. Получение коэффициентов\n",
        "\n",
        "После подстановки значений мы получаем:\n",
        "\n",
        "$$\n",
        "w \\approx\n",
        "\\begin{bmatrix}\n",
        "0 \\\\\n",
        "8\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "### Интерпретация коэффициентов\n",
        "\n",
        "Полученные коэффициенты означают следующее:\n",
        "\n",
        "- $w_0 = 0$: При цене 0 объем продаж также равен 0, что логично.\n",
        "- $w_1 = 8$: Это значит, что на каждый доллар увеличения цены объем продаж увеличивается на 8 единиц.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Jq8t9ocE9kJl"
      }
    }
  ]
}