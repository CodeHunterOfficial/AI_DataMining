{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM47NmMG+a0HvqTG55JRs6L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CodeHunterOfficial/AI_DataMining/blob/main/%D0%A1%D0%BB%D1%83%D1%87%D0%B0%D0%B9%D0%BD%D1%8B%D0%B9_%D0%BB%D0%B5%D1%81_(Random_Forest).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Случайный лес (Random Forest)\n",
        "\n",
        "### Введение\n",
        "\n",
        "Случайный лес (Random Forest) — это метод ансамблевого обучения, который используется как для классификации, так и для регрессии. Он основывается на использовании множества деревьев решений и применяет концепцию \"голосования\" для принятия финального решения. Основные преимущества случайного леса включают в себя его устойчивость к переобучению, высокую точность и возможность обработки больших объемов данных с множеством признаков.\n",
        "\n",
        "### 1. Основы деревьев решений\n",
        "\n",
        "Перед тем как углубляться в случайный лес, важно понять, что такое дерево решений.\n",
        "\n",
        "#### 1.1. Структура дерева\n",
        "\n",
        "Дерево решений — это модель, которая принимает решения, представляя их в виде дерева, где каждый узел представляет собой условие по какому-либо признаку, каждая ветвь соответствует результату этого условия, а листья представляют собой конечные классы или значения.\n",
        "\n",
        "- **Корень** (Root): Начальный узел дерева, откуда начинается процесс принятия решения.\n",
        "- **Внутренние узлы** (Internal Nodes): Узлы, представляющие собой условия, на основе которых принимается решение.\n",
        "- **Листовые узлы** (Leaf Nodes): Узлы, представляющие собой конечные классы (в случае классификации) или значения (в случае регрессии).\n",
        "\n",
        "#### 1.2. Критерии разделения\n",
        "\n",
        "Для построения дерева решений используются различные критерии, среди которых:\n",
        "\n",
        "- **Индекс Джини (Gini Index)** для классификации:\n",
        "  \n",
        "$$\n",
        "  Gini(D) = 1 - \\sum_{i=1}^{C} p_i^2\n",
        "$$\n",
        "\n",
        "  где $C$ — количество классов, а $p_i$ — доля объектов класса $i$ в выборке $D$.\n",
        "\n",
        "- **Кросс-энтропия (Cross-Entropy)**:\n",
        "\n",
        "$$\n",
        "  Entropy(D) = -\\sum_{i=1}^{C} p_i \\log_2(p_i)\n",
        "$$\n",
        "\n",
        "- **Среднеквадратичная ошибка (Mean Squared Error)** для регрессии:\n",
        "\n",
        "$$\n",
        "  MSE(D) = \\frac{1}{N} \\sum_{j=1}^{N} (y_j - \\hat{y})^2\n",
        "$$\n",
        "\n",
        "  где $y_j$ — истинное значение, $\\hat{y}$ — предсказанное значение, а $N$ — количество наблюдений.\n",
        "\n",
        "### 2. Случайный лес\n",
        "\n",
        "#### 2.1. Основные идеи\n",
        "\n",
        "Случайный лес строит множество деревьев решений, каждый из которых обучается на случайной подвыборке данных и случайном подмножестве признаков. Это помогает снизить переобучение и увеличить обобщающую способность модели.\n",
        "\n",
        "#### 2.2. Процесс построения\n",
        "\n",
        "1. **Бутстрап-выборка**: Для каждого дерева выборка создается путем случайного выбора объектов из исходного набора данных с возвращением. Это означает, что одни и те же объекты могут входить в выборку несколько раз, а некоторые могут быть исключены.\n",
        "\n",
        "2. **Случайный выбор признаков**: При каждом разделении узла выбирается случайное подмножество признаков. Это уменьшает корреляцию между деревьями.\n",
        "\n",
        "3. **Обучение деревьев**: Каждое дерево обучается на своей бутстрап-выборке и с использованием случайного подмножества признаков.\n",
        "\n",
        "4. **Голосование/усреднение**:\n",
        "   - Для задач классификации: каждый узел (дерево) голосует за класс, и класс с наибольшим числом голосов выбирается как финальный результат.\n",
        "   - Для задач регрессии: результаты деревьев усредняются.\n",
        "\n",
        "### 3. Математические формулы\n",
        "\n",
        "#### 3.1. Прогнозирование\n",
        "\n",
        "Если у нас есть $M$ деревьев в случайном лесу, то предсказание для нового объекта $x$ делается следующим образом:\n",
        "\n",
        "- Для классификации:\n",
        "  \n",
        "$$\n",
        "  \\hat{y} = \\text{argmax} \\left( \\sum_{m=1}^{M} I(y_m = k) \\right)\n",
        "$$\n",
        "\n",
        "  где $I$ — индикаторная функция, равная 1, если $y_m$ — это класс $k$, и 0 в противном случае.\n",
        "\n",
        "- Для регрессии:\n",
        "\n",
        "$$\n",
        "  \\hat{y} = \\frac{1}{M} \\sum_{m=1}^{M} y_m\n",
        "$$\n",
        "\n",
        "#### 3.2. Оценка качества модели\n",
        "\n",
        "Качество модели случайного леса может оцениваться различными метриками:\n",
        "\n",
        "- **Точность (Accuracy)** для классификации:\n",
        "\n",
        "$$\n",
        "  Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}\n",
        "$$\n",
        "\n",
        "- **Средняя абсолютная ошибка (Mean Absolute Error, MAE)** для регрессии:\n",
        "\n",
        "$$\n",
        "  MAE = \\frac{1}{N} \\sum_{j=1}^{N} |y_j - \\hat{y}|\n",
        "$$\n",
        "\n",
        "### 4. Преимущества и недостатки\n",
        "\n",
        "#### 4.1. Преимущества\n",
        "\n",
        "- Высокая точность и устойчивость к переобучению.\n",
        "- Возможность работы с пропущенными данными.\n",
        "- Эффективность при работе с большим количеством признаков.\n",
        "\n",
        "#### 4.2. Недостатки\n",
        "\n",
        "- Сложность интерпретации.\n",
        "- Требует больше вычислительных ресурсов по сравнению с простыми моделями.\n",
        "- Меньшая скорость предсказания по сравнению с одиночными деревьями решений.\n",
        "\n",
        "### 5. Применение\n",
        "\n",
        "Случайный лес находит применение в различных областях:\n",
        "\n",
        "- Медицинская диагностика\n",
        "- Финансовый анализ\n",
        "- Классификация текстов\n",
        "- Обработка изображений\n",
        "\n",
        "### 6. Пример\n",
        "\n",
        "Рассмотрим числовой пример для иллюстрации работы случайного леса.\n",
        "\n",
        "**Данные:**\n",
        "\n",
        "Предположим, у нас есть выборка из 10 объектов с двумя признаками $X_1$ и $X_2$ и целевым классом $Y$.\n",
        "\n",
        "| Объект | $X_1$ | $X_2$ | $Y$ |\n",
        "|--------|---------|---------|-------|\n",
        "| 1      | 2       | 3       | 0     |\n",
        "| 2      | 1       | 2       | 0     |\n",
        "| 3      | 4       | 5       | 1     |\n",
        "| 4      | 5       | 4       | 1     |\n",
        "| 5      | 3       | 3       | 0     |\n",
        "| 6      | 4       | 2       | 1     |\n",
        "| 7      | 2       | 1       | 0     |\n",
        "| 8      | 5       | 3       | 1     |\n",
        "| 9      | 3       | 2       | 0     |\n",
        "| 10     | 4       | 4       | 1     |\n",
        "\n",
        "**Построение случайного леса:**\n",
        "\n",
        "1. **Бутстрап-выборки**: Создаем 3 бутстрап-выборки:\n",
        "   - В первой выборке: 1, 3, 4, 5, 6, 9 (некоторые объекты повторяются).\n",
        "   - Во второй выборке: 2, 3, 4, 7, 8, 10.\n",
        "   - В третьей выборке: 1, 2, 5, 6, 8, 9.\n",
        "\n",
        "2. **Случайный выбор признаков**: Для каждого узла выбираем, например, 1 признак из $X_1$ и $X_2$.\n",
        "\n",
        "3. **Обучение деревьев**: Каждое дерево обучается на своей выборке.\n",
        "\n",
        "4. **Прогнозирование**: Для нового объекта с $X_1 = 3$ и $X_2 = 4$:\n",
        "   - Дерево 1 предсказывает класс 1.\n",
        "   - Дерево 2 предсказывает класс 0.\n",
        "   - Дерево 3 предсказывает класс 1.\n",
        "\n",
        "Итоговое предсказание: класс 1, так как два дерева из трех проголосовали за класс 1.\n",
        "\n",
        "### Заключение\n",
        "\n",
        "Случайный лес является мощным инструментом в арсенале машинного обучения. Понимание его основ, преимуществ и недостатков, а также принципов работы позволяет эффективно применять его для решения различных задач в области анализа данных."
      ],
      "metadata": {
        "id": "y6535nHIcp3J"
      }
    }
  ]
}