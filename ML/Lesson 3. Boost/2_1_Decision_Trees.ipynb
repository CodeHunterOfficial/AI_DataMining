{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CodeHunterOfficial/AI_DataMining/blob/main/ML/2_1_Decision_Trees.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Деревья решений в машинном обучении\n",
        "\n",
        "Деревья решений (Decision Trees) — это один из наиболее популярных и интуитивно понятных алгоритмов машинного обучения. Деревья решений используются как для задач классификации, так и для задач регрессии. В данной лекции мы подробно рассмотрим, как работают деревья решений, математические основы и шаги построения модели, а также приведём пример применения.\n",
        "\n",
        "## 1. Основы дерева решений\n",
        "\n",
        "### 1.1. Что такое дерево решений?\n",
        "\n",
        "Дерево решений — это иерархическая структура, состоящая из узлов. Основные элементы дерева:\n",
        "\n",
        "- **Корневой узел (root node)**: это первый узел дерева, с которого начинается процесс деления. Он содержит весь набор данных.\n",
        "- **Внутренние узлы (internal nodes)**: узлы, которые представляют собой точки разделения данных на основании определённого критерия.\n",
        "- **Листовые узлы (leaf nodes)**: узлы, которые содержат окончательное решение или предсказание модели.\n",
        "\n",
        "### 1.2. Процесс работы дерева\n",
        "\n",
        "Основная идея дерева решений заключается в последовательном разделении данных на подмножества на основании выбранных признаков. Процесс продолжается до тех пор, пока не достигнут листовые узлы. На каждом этапе дерево выбирает признак и пороговое значение, которые лучше всего разделяют данные на основе заданного критерия.\n",
        "\n",
        "## 2. Критерии разделения\n",
        "\n",
        "Основной задачей при построении дерева решений является выбор признаков и пороговых значений, которые лучше всего разделяют данные. Для этого используются различные математические критерии, такие как:\n",
        "\n",
        "1. **Для классификации:**\n",
        "    - Энтропия (Entropy)\n",
        "    - Критерий Джини (Gini Index)\n",
        "2. **Для регрессии:**\n",
        "    - Среднеквадратичная ошибка (Mean Squared Error, MSE)\n",
        "\n",
        "### 2.1. Энтропия\n",
        "\n",
        "Энтропия — это мера неопределённости или неупорядоченности в наборе данных. Чем выше энтропия, тем больше смешение классов.\n",
        "\n",
        "Формула для энтропии:\n",
        "\n",
        "$$\n",
        "H(S) = - \\sum_{i=1}^{c} p_i \\log_2(p_i)\n",
        "$$\n",
        "\n",
        "где $p_i$ — доля объектов класса $i$ в выборке $S$, а $c$ — количество классов.\n",
        "\n",
        "Пример: если у нас есть выборка из 10 объектов, 6 из которых принадлежат классу A, а 4 — классу B, то энтропия будет:\n",
        "\n",
        "$$\n",
        "H(S) = - \\left( \\frac{6}{10} \\log_2 \\frac{6}{10} + \\frac{4}{10} \\log_2 \\frac{4}{10} \\right)\n",
        "$$\n",
        "\n",
        "$$\n",
        "H(S) = - \\left( 0.6 \\log_2 0.6 + 0.4 \\log_2 0.4 \\right)\n",
        "$$\n",
        "\n",
        "$$\n",
        "H(S) \\approx 0.97095\n",
        "$$\n",
        "\n",
        "Чем ближе энтропия к 0, тем «чище» подмножество данных (то есть все объекты принадлежат одному классу).\n",
        "\n",
        "### 2.2. Критерий Джини\n",
        "\n",
        "Индекс Джини также используется для оценки чистоты узлов. Чем меньше значение критерия Джини, тем лучше разделение. Формула для индекса Джини:\n",
        "\n",
        "$$\n",
        "G(S) = 1 - \\sum_{i=1}^{c} p_i^2\n",
        "$$\n",
        "\n",
        "где $p_i$ — доля объектов класса $i$ в выборке $S$.\n",
        "\n",
        "Пример: если в выборке 6 объектов класса A и 4 объекта класса B, то критерий Джини будет:\n",
        "\n",
        "$$\n",
        "G(S) = 1 - \\left( \\left( \\frac{6}{10} \\right)^2 + \\left( \\frac{4}{10} \\right)^2 \\right)\n",
        "$$\n",
        "\n",
        "$$\n",
        "G(S) = 1 - \\left( 0.36 + 0.16 \\right)\n",
        "$$\n",
        "\n",
        "$$\n",
        "G(S) = 0.48\n",
        "$$\n",
        "\n",
        "### 2.3. Среднеквадратичная ошибка (MSE) для регрессии\n",
        "\n",
        "Для задач регрессии критерий чистоты узлов измеряется с помощью среднеквадратичной ошибки. Формула для MSE:\n",
        "\n",
        "$$\n",
        "MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y})^2\n",
        "$$\n",
        "\n",
        "где $y_i$ — истинное значение целевой переменной, $\\hat{y}$ — предсказанное значение, а $n$ — количество объектов.\n",
        "\n",
        "## 3. Построение дерева решений\n",
        "\n",
        "### 3.1. Шаги построения дерева\n",
        "\n",
        "1. **Выбор критерия разделения**: Для каждого узла выбирается наилучший признак и пороговое значение для разделения данных на основании выбранного критерия (энтропия, Джини или MSE).\n",
        "2. **Разделение данных**: Выборка делится на подмножества на основании выбранного признака и порога.\n",
        "3. **Рекурсия**: Для каждого подмножества данных создаются новые узлы, процесс продолжается до тех пор, пока не будет достигнуто одно из условий остановки (например, максимальная глубина дерева или минимальное количество объектов в узле).\n",
        "4. **Присвоение меток**: Листовым узлам присваиваются метки классов (для задач классификации) или значения (для регрессии).\n",
        "\n",
        "### 3.2. Обрезка дерева (Pruning)\n",
        "\n",
        "Глубокие деревья могут склонны к переобучению. Чтобы избежать этого, применяют обрезку дерева (pruning), что позволяет контролировать его сложность. Есть два основных метода обрезки:\n",
        "\n",
        "- **Предварительная обрезка (pre-pruning)**: процесс остановки роста дерева на ранней стадии, например, по достижении определённой глубины дерева.\n",
        "- **Постобрезка (post-pruning)**: построение полного дерева и затем удаление неинформативных узлов.\n",
        "\n",
        "## 4. Пример построения дерева решений для задачи классификации\n",
        "\n",
        "Предположим, у нас есть набор данных с двумя признаками (Feature1 и Feature2) и целевая переменная с двумя классами (A и B). Вот таблица данных:\n",
        "\n",
        "| Feature1 | Feature2 | Class |\n",
        "|----------|----------|-------|\n",
        "| 2.7      | 3.1      | A     |\n",
        "| 1.5      | 2.3      | B     |\n",
        "| 3.3      | 3.8      | A     |\n",
        "| 1.1      | 1.9      | B     |\n",
        "| 2.9      | 3.3      | A     |\n",
        "\n",
        "### Шаг 1: Вычисление энтропии исходного набора\n",
        "\n",
        "Для начального набора данных:\n",
        "\n",
        "- 3 объекта класса A\n",
        "- 2 объекта класса B\n",
        "\n",
        "Энтропия исходного набора данных:\n",
        "\n",
        "$$\n",
        "H(S) = - \\left( \\frac{3}{5} \\log_2 \\frac{3}{5} + \\frac{2}{5} \\log_2 \\frac{2}{5} \\right)\n",
        "$$\n",
        "\n",
        "$$\n",
        "H(S) = - \\left( 0.6 \\log_2 0.6 + 0.4 \\log_2 0.4 \\right)\n",
        "$$\n",
        "\n",
        "$$\n",
        "H(S) \\approx 0.97095\n",
        "$$\n",
        "\n",
        "### Шаг 2: Разделение данных по признаку Feature1\n",
        "\n",
        "Предположим, мы выбираем пороговое значение $Feature1 = 2.5$. Делим данные на две части:\n",
        "\n",
        "- Левое подмножество (Feature1 ≤ 2.5): объекты 2 и 4 (оба класса B)\n",
        "- Правое подмножество (Feature1 > 2.5): объекты 1, 3 и 5 (все класса A)\n",
        "\n",
        "Энтропия для левого подмножества:\n",
        "\n",
        "$$\n",
        "H(Left) = - \\left( \\frac{0}{2} \\log_2 \\frac{0}{2} + \\frac{2}{2} \\log_2 \\frac{2}{2} \\right) = 0\n",
        "$$\n",
        "\n",
        "Энтропия для правого подмножества:\n",
        "\n",
        "$$\n",
        "H(Right) = - \\left( \\frac{3}{3} \\log_2 \\frac{3}{3} + \\frac{0}{3} \\log_2 \\frac{0}{3} \\right) = 0\n",
        "$$\n",
        "\n",
        "### Шаг 3: Вычисление прироста информации\n",
        "\n",
        "Прирост информации (Information Gain) — это разница между энтропией исходного набора и средневзвешенной энтропией подмножеств.\n",
        "\n",
        "Формула для прироста информации:\n",
        "\n",
        "$$\n",
        "IG = H(S) - \\left( \\frac{|Left|}{|S|} H(Left) + \\frac{|Right|}{|S|} H(Right) \\right)\n",
        "$$\n",
        "\n",
        "Где $|Left|$ и $|Right|$ — размеры левого и правого подмножеств соответственно.\n",
        "\n",
        "$$\n",
        "IG = H(S) - \\left( \\frac{2}{5} \\cdot 0 + \\frac{3}{5} \\cdot 0 \\right) = H(S)\n",
        "$$\n",
        "\n",
        "Таким образом, в данном примере прирост информации будет равен 0.97095.\n",
        "\n",
        "## Заключение\n",
        "\n",
        "Деревья решений — мощный инструмент в арсенале методов машинного обучения, который хорошо интерпретируется и может быть использован в различных задачах. Однако, как и любой другой метод, они имеют свои ограничения, такие как склонность к переобучению. Однако с помощью методов обрезки можно минимизировать этот эффект.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Классификация\n",
        "\n",
        "Давайте подробно решим пример построения дерева решений шаг за шагом, используя набор данных, который мы обсуждали ранее. Мы будем использовать два признака (Feature1 и Feature2) и целевую переменную (Class).\n",
        "\n",
        "### Набор данных\n",
        "\n",
        "| Feature1 | Feature2 | Class |\n",
        "|----------|----------|-------|\n",
        "| 2.7      | 3.1      | A     |\n",
        "| 1.5      | 2.3      | B     |\n",
        "| 3.3      | 3.8      | A     |\n",
        "| 1.1      | 1.9      | B     |\n",
        "| 2.9      | 3.3      | A     |\n",
        "\n",
        "### Шаг 1: Вычисление начальной энтропии\n",
        "\n",
        "Сначала вычислим энтропию исходного набора данных. У нас 5 объектов: 3 объекта класса A и 2 объекта класса B.\n",
        "\n",
        "**Формула для энтропии:**\n",
        "\n",
        "$$\n",
        "H(S) = - \\sum_{i=1}^{c} p_i \\log_2(p_i)\n",
        "$$\n",
        "\n",
        "Где $p_i$ — доля объектов класса $i$.\n",
        "\n",
        "**Доля классов:**\n",
        "\n",
        "- $p_A = \\frac{3}{5} = 0.6$\n",
        "- $p_B = \\frac{2}{5} = 0.4$\n",
        "\n",
        "**Подставим значения в формулу:**\n",
        "\n",
        "$$\n",
        "H(S) = - \\left( 0.6 \\log_2(0.6) + 0.4 \\log_2(0.4) \\right)\n",
        "$$\n",
        "\n",
        "Вычислим логарифмы:\n",
        "\n",
        "$$\n",
        "\\log_2(0.6) \\approx -0.7369656\n",
        "$$\n",
        "$$\n",
        "\\log_2(0.4) \\approx -1.3219281\n",
        "$$\n",
        "\n",
        "Теперь подставим их в формулу:\n",
        "\n",
        "$$\n",
        "H(S) = - \\left( 0.6 \\cdot (-0.7369656) + 0.4 \\cdot (-1.3219281) \\right)\n",
        "$$\n",
        "\n",
        "Вычислим:\n",
        "\n",
        "$$\n",
        "H(S) = - \\left( -0.44217936 - 0.52877124 \\right)\n",
        "$$\n",
        "$$\n",
        "H(S) = 0.97095\n",
        "$$\n",
        "\n",
        "### Шаг 2: Разделение данных по признакам\n",
        "\n",
        "Теперь мы будем рассматривать разные варианты разделения данных по признакам Feature1 и Feature2. Начнём с Feature1.\n",
        "\n",
        "**Пробуем порог 2.5 для Feature1:**\n",
        "\n",
        "1. **Левое подмножество (Feature1 ≤ 2.5):**\n",
        "   - (1.5, 2.3) → B\n",
        "   - (1.1, 1.9) → B\n",
        "   - Количество: 2B, 0A\n",
        "2. **Правое подмножество (Feature1 > 2.5):**\n",
        "   - (2.7, 3.1) → A\n",
        "   - (3.3, 3.8) → A\n",
        "   - (2.9, 3.3) → A\n",
        "   - Количество: 3A, 0B\n",
        "\n",
        "**Энтропия для левого подмножества:**\n",
        "\n",
        "У нас 2 объекта класса B:\n",
        "\n",
        "$$\n",
        "H(Left) = - \\left( \\frac{2}{2} \\log_2 \\frac{2}{2} \\right) = 0\n",
        "$$\n",
        "\n",
        "**Энтропия для правого подмножества:**\n",
        "\n",
        "У нас 3 объекта класса A:\n",
        "\n",
        "$$\n",
        "H(Right) = - \\left( \\frac{3}{3} \\log_2 \\frac{3}{3} \\right) = 0\n",
        "$$\n",
        "\n",
        "### Шаг 3: Вычисление прироста информации\n",
        "\n",
        "Теперь мы вычислим прирост информации (Information Gain) для этого разделения.\n",
        "\n",
        "**Формула для прироста информации:**\n",
        "\n",
        "$$\n",
        "IG = H(S) - \\left( \\frac{|Left|}{|S|} H(Left) + \\frac{|Right|}{|S|} H(Right) \\right)\n",
        "$$\n",
        "\n",
        "Где $|Left| = 2$, $|Right| = 3$, и $|S| = 5$.\n",
        "\n",
        "**Подставим значения в формулу:**\n",
        "\n",
        "$$\n",
        "IG = H(S) - \\left( \\frac{2}{5} \\cdot 0 + \\frac{3}{5} \\cdot 0 \\right)\n",
        "$$\n",
        "\n",
        "$$\n",
        "IG = 0.97095 - 0 = 0.97095\n",
        "$$\n",
        "\n",
        "### Шаг 4: Попробуем другой порог для Feature1\n",
        "\n",
        "Попробуем порог 2.9 для Feature1:\n",
        "\n",
        "1. **Левое подмножество (Feature1 ≤ 2.9):**\n",
        "   - (2.7, 3.1) → A\n",
        "   - (1.5, 2.3) → B\n",
        "   - (1.1, 1.9) → B\n",
        "   - Количество: 1A, 2B\n",
        "2. **Правое подмножество (Feature1 > 2.9):**\n",
        "   - (3.3, 3.8) → A\n",
        "   - (2.9, 3.3) → A\n",
        "   - Количество: 2A, 0B\n",
        "\n",
        "**Энтропия для левого подмножества:**\n",
        "\n",
        "У нас 1 объект класса A и 2 объекта класса B:\n",
        "\n",
        "$$\n",
        "H(Left) = - \\left( \\frac{1}{3} \\log_2 \\frac{1}{3} + \\frac{2}{3} \\log_2 \\frac{2}{3} \\right)\n",
        "$$\n",
        "\n",
        "Вычисляем логарифмы:\n",
        "\n",
        "$$\n",
        "\\log_2\\left(\\frac{1}{3}\\right) \\approx -1.58496\n",
        "$$\n",
        "$$\n",
        "\\log_2\\left(\\frac{2}{3}\\right) \\approx -0.58496\n",
        "$$\n",
        "\n",
        "Теперь подставим:\n",
        "\n",
        "$$\n",
        "H(Left) = - \\left( \\frac{1}{3} \\cdot (-1.58496) + \\frac{2}{3} \\cdot (-0.58496) \\right)\n",
        "$$\n",
        "$$\n",
        "H(Left) = - \\left( -0.52832 - 0.38936 \\right)\n",
        "$$\n",
        "$$\n",
        "H(Left) \\approx 0.91700\n",
        "$$\n",
        "\n",
        "**Энтропия для правого подмножества:**\n",
        "\n",
        "У нас 2 объекта класса A:\n",
        "\n",
        "$$\n",
        "H(Right) = - \\left( \\frac{2}{2} \\log_2 \\frac{2}{2} \\right) = 0\n",
        "$$\n",
        "\n",
        "### Шаг 5: Вычисление прироста информации для нового порога\n",
        "\n",
        "Теперь вычислим прирост информации для порога 2.9.\n",
        "\n",
        "**Подставим значения:**\n",
        "\n",
        "$$\n",
        "IG = H(S) - \\left( \\frac{3}{5} H(Left) + \\frac{2}{5} H(Right) \\right)\n",
        "$$\n",
        "\n",
        "Где $|Left| = 3$, $|Right| = 2$.\n",
        "\n",
        "$$\n",
        "IG = 0.97095 - \\left( \\frac{3}{5} \\cdot 0.91700 + \\frac{2}{5} \\cdot 0 \\right)\n",
        "$$\n",
        "\n",
        "Вычислим:\n",
        "\n",
        "$$\n",
        "IG = 0.97095 - \\left( 0.5502 \\right) = 0.42075\n",
        "$$\n",
        "\n",
        "### Шаг 6: Выбор наилучшего признака\n",
        "\n",
        "После рассмотрения нескольких порогов для Feature1 мы видим, что:\n",
        "\n",
        "- Для порога 2.5 прирост информации $IG \\approx 0.97095$\n",
        "- Для порога 2.9 прирост информации $IG \\approx 0.42075$\n",
        "\n",
        "Наибольшее значение прироста информации наблюдается для порога 2.5. Это означает, что мы будем использовать **Feature1** с порогом **2.5** для первого разбиения нашего дерева.\n",
        "\n",
        "### Шаг 7: Проверка другого признака (Feature2)\n",
        "\n",
        "Теперь давайте проверим, как будет работать **Feature2** с порогом 3.0.\n",
        "\n",
        "1. **Левое подмножество (Feature2 ≤ 3.0):**\n",
        "   - (2.7, 3.1) → A\n",
        "   - (1.5, 2.3) → B\n",
        "   - (3.3, 3.8) → A\n",
        "   - (1.1, 1.9) → B\n",
        "   - (2.9, 3.3) → A\n",
        "   - Количество: 3A, 2B\n",
        "2. **Правое подмножество (Feature2 > 3.0):**\n",
        "   - (None)\n",
        "   - Количество: 0A, 0B\n",
        "\n",
        "**Энтропия для левого подмножества:**\n",
        "\n",
        "$$\n",
        "H(Left) = - \\left( \\frac{3}{5} \\log_2 \\frac{3}{5} + \\frac{2}{5} \\log_2 \\frac{2}{5} \\right)\n",
        "$$\n",
        "\n",
        "Мы уже посчитали, что:\n",
        "\n",
        "$$\n",
        "H(Left) \\approx 0.97095\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "**Энтропия для правого подмножества:**\n",
        "\n",
        "$$\n",
        "H(Right) = 0\n",
        "$$\n",
        "\n",
        "### Шаг 8: Прирост информации для Feature2\n",
        "\n",
        "Теперь вычислим прирост информации для Feature2:\n",
        "\n",
        "$$\n",
        "IG = H(S) - \\left( \\frac{5}{5} H(Left) + 0 \\right)\n",
        "$$\n",
        "\n",
        "$$\n",
        "IG = 0.97095 - (1 \\cdot 0.97095) = 0\n",
        "$$\n",
        "\n",
        "### Шаг 9: Заключение\n",
        "\n",
        "Мы видим, что наилучшее разделение происходит при использовании **Feature1** с порогом **2.5**, которое даёт максимальный прирост информации, равный 0.97095.\n",
        "\n",
        "Дерево решений будет выглядеть следующим образом:\n",
        "\n",
        "```\n",
        "             Feature1 <= 2.5\n",
        "                /          \\\n",
        "              B (Leaf)    A (Leaf)\n",
        "```\n",
        "\n",
        "### Итог\n",
        "\n",
        "Мы шаг за шагом рассмотрели процесс построения дерева решений, вычисляя энтропию, разделяя данные и вычисляя прирост информации. Мы увидели, что использование **Feature1** с порогом **2.5** даёт наилучший результат.\n",
        "\n",
        "\n",
        "##Регрессия\n",
        "\n",
        "Рассмотрим пример применения деревьев решений для задачи регрессии. Деревья решений для регрессии работают аналогично деревьям классификации, но вместо того, чтобы предсказывать категориальные значения, они предсказывают непрерывные значения.\n",
        "\n",
        "### Набор данных\n",
        "\n",
        "Предположим, у нас есть следующий набор данных, где мы хотим предсказать значение целевой переменной (например, цену дома) на основе двух признаков: `Площадь (кв. м)` и `Количество комнат`.\n",
        "\n",
        "| Площадь (кв. м) | Количество комнат | Цена (тыс. руб.) |\n",
        "|------------------|------------------|-------------------|\n",
        "| 50               | 1                | 1.5               |\n",
        "| 60               | 2                | 2.0               |\n",
        "| 70               | 2                | 2.5               |\n",
        "| 80               | 3                | 3.0               |\n",
        "| 90               | 3                | 3.5               |\n",
        "\n",
        "### Шаг 1: Вычисление начального среднего значения\n",
        "\n",
        "Сначала вычислим среднее значение целевой переменной (цены) для всего набора данных.\n",
        "\n",
        "$$\n",
        "\\text{Среднее} = \\frac{1.5 + 2.0 + 2.5 + 3.0 + 3.5}{5} = \\frac{12.5}{5} = 2.5\n",
        "$$\n",
        "\n",
        "### Шаг 2: Вычисление начального среднеквадратичного отклонения (MSE)\n",
        "\n",
        "Теперь рассчитаем среднеквадратичную ошибку (MSE) для всего набора данных. MSE показывает, насколько предсказанные значения отклоняются от реальных.\n",
        "\n",
        "**Формула для MSE:**\n",
        "\n",
        "$$\n",
        "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\bar{y})^2\n",
        "$$\n",
        "\n",
        "Где $y_i$ — реальные значения, $\\bar{y}$ — среднее значение, $n$ — количество объектов.\n",
        "\n",
        "Подставим наши данные:\n",
        "\n",
        "$$\n",
        "\\text{MSE} = \\frac{1}{5} \\left( (1.5 - 2.5)^2 + (2.0 - 2.5)^2 + (2.5 - 2.5)^2 + (3.0 - 2.5)^2 + (3.5 - 2.5)^2 \\right)\n",
        "$$\n",
        "\n",
        "Вычислим:\n",
        "\n",
        "$$\n",
        "= \\frac{1}{5} \\left( (-1)^2 + (-0.5)^2 + 0^2 + 0.5^2 + 1^2 \\right)\n",
        "$$\n",
        "$$\n",
        "= \\frac{1}{5} \\left( 1 + 0.25 + 0 + 0.25 + 1 \\right) = \\frac{2.5}{5} = 0.5\n",
        "$$\n",
        "\n",
        "### Шаг 3: Разделение данных по признакам\n",
        "\n",
        "Теперь мы попробуем разделить данные по признаку `Площадь` с разными порогами и посмотрим, как это повлияет на MSE.\n",
        "\n",
        "**Пробуем порог 70:**\n",
        "\n",
        "1. **Левое подмножество (Площадь ≤ 70):**\n",
        "   - 50, 1.5\n",
        "   - 60, 2.0\n",
        "   - 70, 2.5\n",
        "   - Средняя цена:\n",
        "   $$\n",
        "   \\text{Средняя}_{left} = \\frac{1.5 + 2.0 + 2.5}{3} = \\frac{6}{3} = 2.0\n",
        "   $$\n",
        "2. **Правое подмножество (Площадь > 70):**\n",
        "   - 80, 3.0\n",
        "   - 90, 3.5\n",
        "   - Средняя цена:\n",
        "   $$\n",
        "   \\text{Средняя}_{right} = \\frac{3.0 + 3.5}{2} = \\frac{6.5}{2} = 3.25\n",
        "   $$\n",
        "\n",
        "### Шаг 4: Вычисление MSE для подмножеств\n",
        "\n",
        "Теперь вычислим MSE для обоих подмножеств.\n",
        "\n",
        "**MSE для левого подмножества:**\n",
        "\n",
        "$$\n",
        "\\text{MSE}_{left} = \\frac{1}{3} \\left( (1.5 - 2.0)^2 + (2.0 - 2.0)^2 + (2.5 - 2.0)^2 \\right)\n",
        "$$\n",
        "\n",
        "Вычислим:\n",
        "\n",
        "$$\n",
        "= \\frac{1}{3} \\left( (-0.5)^2 + 0^2 + (0.5)^2 \\right) = \\frac{1}{3} \\left( 0.25 + 0 + 0.25 \\right) = \\frac{0.5}{3} \\approx 0.1667\n",
        "$$\n",
        "\n",
        "**MSE для правого подмножества:**\n",
        "\n",
        "$$\n",
        "\\text{MSE}_{right} = \\frac{1}{2} \\left( (3.0 - 3.25)^2 + (3.5 - 3.25)^2 \\right)\n",
        "$$\n",
        "\n",
        "Вычислим:\n",
        "\n",
        "$$\n",
        "= \\frac{1}{2} \\left( (-0.25)^2 + (0.25)^2 \\right) = \\frac{1}{2} \\left( 0.0625 + 0.0625 \\right) = \\frac{0.125}{2} = 0.0625\n",
        "$$\n",
        "\n",
        "### Шаг 5: Общая MSE после разделения\n",
        "\n",
        "Теперь вычислим общую MSE после разделения:\n",
        "\n",
        "**Формула для общей MSE:**\n",
        "\n",
        "$$\n",
        "\\text{MSE}_{total} = \\frac{|Left|}{|S|} \\cdot \\text{MSE}_{left} + \\frac{|Right|}{|S|} \\cdot \\text{MSE}_{right}\n",
        "$$\n",
        "\n",
        "Где $|Left| = 3$, $|Right| = 2$, и $|S| = 5$.\n",
        "\n",
        "$$\n",
        "\\text{MSE}_{total} = \\frac{3}{5} \\cdot 0.1667 + \\frac{2}{5} \\cdot 0.0625\n",
        "$$\n",
        "\n",
        "Вычислим:\n",
        "\n",
        "$$\n",
        "= 0.10002 + 0.025 = 0.12502\n",
        "$$\n",
        "\n",
        "### Шаг 6: Прирост снижения ошибки (Gain)\n",
        "\n",
        "Теперь вычислим прирост снижения ошибки (Reduction in Error) при использовании этого разделения:\n",
        "\n",
        "$$\n",
        "\\text{Gain} = \\text{MSE}_{initial} - \\text{MSE}_{total}\n",
        "$$\n",
        "\n",
        "Где $\\text{MSE}_{initial} = 0.5$.\n",
        "\n",
        "$$\n",
        "\\text{Gain} = 0.5 - 0.12502 = 0.37498\n",
        "$$\n",
        "\n",
        "### Шаг 7: Проверка другого порога\n",
        "\n",
        "Теперь проверим порог 80 для `Площадь`.\n",
        "\n",
        "1. **Левое подмножество (Площадь ≤ 80):**\n",
        "   - 50, 1.5\n",
        "   - 60, 2.0\n",
        "   - 70, 2.5\n",
        "   - 80, 3.0\n",
        "   - Средняя цена:\n",
        "   $$\n",
        "   \\text{Средняя}_{left} = \\frac{1.5 + 2.0 + 2.5 + 3.0}{4} = \\frac{9.0}{4} = 2.25\n",
        "   $$\n",
        "2. **Правое подмножество (Площадь > 80):**\n",
        "   - 90, 3.5\n",
        "   - Средняя цена:\n",
        "   $$\n",
        "   \\text{Средняя}_{right} = 3.5\n",
        "   $$\n",
        "\n",
        "### Шаг 8: Вычисление MSE для нового порога\n",
        "\n",
        "Теперь вычислим MSE для нового подмножества.\n",
        "\n",
        "**MSE для левого подмножества:**\n",
        "\n",
        "$$\n",
        "\\text{MSE}_{left} = \\frac{1}{4} \\left( (1.5 - 2.25)^2 + (2.0 - 2.25)^2 + (2.5 - 2.25)^2 + (3.0 - 2.25)^2 \\right)\n",
        "$$\n",
        "\n",
        "Вычислим:\n",
        "\n",
        "$$\n",
        "= \\frac{1}{4} \\left( (-0.75)^2 + (-0.25)^2 + (0.25)^2 + (0.75)^2 \\right)\n",
        "$$\n",
        "$$\n",
        "= \\frac{1}{4} \\left( 0.5625 + 0.0625 + 0.0625 + 0.5625 \\right) = \\frac{1.25}{4} = 0.3125\n",
        "$$\n",
        "\n",
        "**MSE для правого подмножества:**\n",
        "\n",
        "$$\n",
        "\\text{MSE}_{right} = 0\n",
        "$$\n",
        "\n",
        "### Шаг 9: Общая MSE после нового разделения\n",
        "\n",
        "Теперь вычислим общую MSE:\n",
        "\n",
        "$$\n",
        "\\text{MSE}_{total} = \\frac{|Left|}{|S|} \\cdot \\text{MSE}_{left} + \\frac{|Right|}{|S|\n",
        "\n",
        "} \\cdot \\text{MSE}_{right}\n",
        "$$\n",
        "\n",
        "Где $|Left| = 4$, $|Right| = 1$, и $|S| = 5$.\n",
        "\n",
        "$$\n",
        "\\text{MSE}_{total} = \\frac{4}{5} \\cdot 0.3125 + \\frac{1}{5} \\cdot 0\n",
        "$$\n",
        "\n",
        "Вычислим:\n",
        "\n",
        "$$\n",
        "= 0.25\n",
        "$$\n",
        "\n",
        "### Шаг 10: Прирост снижения ошибки для нового порога\n",
        "\n",
        "Теперь вычислим прирост снижения ошибки для нового порога 80:\n",
        "\n",
        "$$\n",
        "\\text{Gain} = \\text{MSE}_{initial} - \\text{MSE}_{total}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\text{Gain} = 0.5 - 0.25 = 0.25\n",
        "$$\n",
        "\n",
        "### Шаг 11: Сравнение прироста для различных порогов\n",
        "\n",
        "Теперь мы сравнили приросты снижения ошибки для двух порогов:\n",
        "\n",
        "- Для порога 70: $\\text{Gain} \\approx 0.37498$\n",
        "- Для порога 80: $\\text{Gain} = 0.25$\n",
        "\n",
        "Наилучший прирост снижения ошибки наблюдается при пороге 70.\n",
        "\n",
        "### Итоговое дерево решений\n",
        "\n",
        "На основе этих расчетов, мы можем построить дерево решений:\n",
        "\n",
        "```\n",
        "               Площадь <= 70\n",
        "                  /          \\\n",
        "                2.0         3.25\n",
        "```\n",
        "\n",
        "### Заключение\n",
        "\n",
        "Таким образом, мы шаг за шагом рассмотрели процесс построения дерева решений для задачи регрессии, вычисляя средние значения, MSE и прирост снижения ошибки. В результате наилучшим порогом для разделения данных оказался порог 70, что позволяет предсказать цены домов на основе площади."
      ],
      "metadata": {
        "id": "21h8cqJliOBN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Создание набора данных\n",
        "data = {\n",
        "    'Площадь (кв. м)': [50, 60, 70, 80, 90],\n",
        "    'Количество комнат': [1, 2, 2, 3, 3],\n",
        "    'Цена (тыс. руб.)': [1.5, 2.0, 2.5, 3.0, 3.5]\n",
        "}\n",
        "\n",
        "# Преобразуем данные в DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Определим признаки и целевую переменную\n",
        "X = df[['Площадь (кв. м)', 'Количество комнат']]\n",
        "y = df['Цена (тыс. руб.)']\n",
        "\n",
        "# Разделение данных на обучающую и тестовую выборки\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Создание модели дерева решений для регрессии\n",
        "model = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
        "\n",
        "# Обучение модели\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Прогнозирование на тестовой выборке\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Вычисление MSE\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "# Вывод результатов\n",
        "print(\"Предсказанные значения:\", y_pred)\n",
        "print(\"Реальные значения:\", y_test.values)\n",
        "print(\"Среднеквадратичная ошибка (MSE):\", mse)\n",
        "\n",
        "# Визуализация дерева решений (необязательно)\n",
        "from sklearn.tree import export_text\n",
        "\n",
        "# Печать текстового представления дерева\n",
        "tree_rules = export_text(model, feature_names=list(X.columns))\n",
        "print(\"\\nПравила дерева решений:\\n\", tree_rules)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wzhjZzMjsiH",
        "outputId": "461595e7-fa9b-4db6-8a3b-75955cd1f28d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Предсказанные значения: [1.5]\n",
            "Реальные значения: [2.]\n",
            "Среднеквадратичная ошибка (MSE): 0.25\n",
            "\n",
            "Правила дерева решений:\n",
            " |--- Площадь (кв. м) <= 60.00\n",
            "|   |--- value: [1.50]\n",
            "|--- Площадь (кв. м) >  60.00\n",
            "|   |--- Количество комнат <= 2.50\n",
            "|   |   |--- value: [2.50]\n",
            "|   |--- Количество комнат >  2.50\n",
            "|   |   |--- value: [3.25]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Вопросы для самопроверки:\n",
        "\n",
        "1. Что такое дерево решений в контексте машинного обучения?\n",
        "2. Какие основные элементы составляют структуру дерева решений?\n",
        "3. Чем различаются задачи классификации и регрессии при использовании деревьев решений?\n",
        "4. Какие критерии используются для разделения узлов в дереве решений для классификации?\n",
        "5. Как рассчитывается энтропия и что она означает?\n",
        "6. Что такое индекс Джини, и как его использовать в построении дерева решений?\n",
        "7. Как рассчитывается среднеквадратичная ошибка (MSE) для деревьев решений в задаче регрессии?\n",
        "8. Что такое прирост информации (Information Gain), и как он помогает в выборе лучшего разделения?\n",
        "9. В чём заключается процесс рекурсивного построения дерева решений?\n",
        "10. Какие факторы могут привести к переобучению дерева решений?\n",
        "11. Что такое обрезка дерева решений (pruning), и какие существуют её виды?\n",
        "12. Как работает предварительная обрезка (pre-pruning)?\n",
        "13. Как работает постобрезка (post-pruning)?\n",
        "14. Какие преимущества и недостатки имеют деревья решений по сравнению с другими методами машинного обучения?\n",
        "15. В каких случаях деревья решений используются совместно с методами ансамблей, такими как случайные леса или бустинг?\n",
        "\n",
        "### Задачи для самостоятельной работы:\n",
        "\n",
        "1. Постройте дерево решений для задачи классификации на основе произвольного набора данных. Пример набора: оценка покупательной способности клиентов по возрасту и доходу.\n",
        "2. Рассчитайте энтропию набора данных, содержащего три класса (A: 50%, B: 30%, C: 20%). Объясните результат.\n",
        "3. Примените критерий Джини к набору данных с двумя классами (A: 60%, B: 40%). Вычислите значение индекса Джини и проанализируйте чистоту узлов.\n",
        "4. Используя метод прироста информации, определите, какой признак выбрать для первого разбиения набора данных: вес (≤ 70 кг или > 70 кг) или рост (≤ 170 см или > 170 см).\n",
        "5. Используйте дерево решений для задачи регрессии. Пример данных: предсказание цены дома на основе площади и количества комнат. Постройте модель и оцените её точность.\n",
        "6. Проведите анализ переобучения на глубоком дереве решений и попробуйте применить обрезку дерева для улучшения обобщающей способности модели.\n",
        "7. Рассчитайте среднеквадратичную ошибку для дерева решений на основе данных о росте и весе, где целевая переменная – индекс массы тела (BMI).\n",
        "8. Проведите эксперимент с выбором разной глубины дерева решений для задачи классификации. Оцените качество модели с помощью метрик точности и полноты.\n",
        "9. Постройте дерево решений для задачи классификации, используя критерий Джини, и проведите анализ результатов.\n",
        "10. Используйте метод постобрезки на полном дереве решений, и сравните его производительность с деревом без обрезки.\n",
        "11. Реализуйте дерево решений для задачи классификации на основе набора данных с непрерывными признаками. Используйте критерий энтропии.\n",
        "12. Проанализируйте влияние размера выборки на производительность дерева решений. Измените количество объектов в выборке и оцените качество модели.\n",
        "13. Используя библиотеку Scikit-learn, постройте дерево решений для задачи классификации. Настройте гиперпараметры модели, такие как глубина дерева и минимальное количество объектов в узле.\n",
        "14. Постройте дерево решений для задачи регрессии с помощью Scikit-learn. Проведите анализ на переобучение и примените методы обрезки.\n",
        "15. Реализуйте процесс кросс-валидации для оценки качества модели дерева решений и объясните, как этот метод помогает избежать переобучения.\n",
        "16. Рассчитайте энтропию для набора данных, состоящего из 4 объектов класса A и 6 объектов класса B. Объясните, как энтропия отражает степень смешения классов.\n",
        "17. Постройте дерево решений с разбиением по количественному признаку и оцените его качество с использованием метрики F1-score.\n",
        "18. Проведите эксперимент по разбиению выборки на основе различных пороговых значений признаков. Определите наилучшее разбиение для построения первого узла.\n",
        "19. Используйте деревья решений для решения задачи классификации с множеством классов (более двух). Оцените производительность модели.\n",
        "20. Проведите анализ и визуализацию дерева решений на основе реального набора данных. Используйте графическое представление дерева для лучшего понимания его структуры.\n",
        "21. Используйте алгоритм CART для построения дерева решений и сравните его результаты с другими методами, такими как энтропия и критерий Джини.\n",
        "22. Проанализируйте влияние аномальных значений в наборе данных на структуру дерева решений. Проведите эксперимент с аномальными данными и оцените результат.\n",
        "23. Постройте дерево решений для задачи классификации, используя набор данных о пациентах с диабетом. Оцените важность признаков для модели.\n",
        "24. Реализуйте на практике метод предварительной обрезки дерева решений и объясните, как он помогает уменьшить переобучение.\n",
        "25. Реализуйте дерево решений для задачи классификации, используя данные о производительности студентов. Оцените качество модели с помощью метрики ROC-AUC.\n",
        "26. Постройте дерево решений для задачи регрессии, используя набор данных о заработной плате сотрудников на основе их опыта и образования.\n",
        "27. Примените дерево решений для задачи классификации изображений, преобразовав изображения в числовые признаки, такие как интенсивность пикселей.\n",
        "28. Проведите исследование зависимости производительности дерева решений от количества признаков в наборе данных. Используйте синтетические данные для эксперимента.\n",
        "29. Постройте дерево решений для задачи анализа кредитного риска. Используйте критерий Джини для разбиения узлов и проанализируйте результаты.\n",
        "30. Используйте метод дерева решений для анализа данных о погоде и предсказания вероятности дождя. Визуализируйте полученную модель."
      ],
      "metadata": {
        "id": "9BjYaV34suA6"
      }
    }
  ]
}