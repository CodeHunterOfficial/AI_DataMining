{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOOnvkc8fj1JfeNLagdG1kC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CodeHunterOfficial/AI_DataMining/blob/main/%D0%A1%D1%82%D0%B0%D1%82%D0%B8%D1%81%D1%82%D0%B8%D0%BA%D0%B0/%D0%90%D0%BD%D0%B0%D0%BB%D0%B8%D0%B7_%D0%B3%D0%BB%D0%B0%D0%B2%D0%BD%D1%8B%D1%85_%D0%BA%D0%BE%D0%BC%D0%BF%D0%BE%D0%BD%D0%B5%D0%BD%D1%82_(PCA).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Анализ главных компонент (PCA)\n",
        "\n",
        "Анализ главных компонент (PCA, от англ. Principal Component Analysis) — это метод многомерного статистического анализа, используемый для уменьшения размерности данных, при этом сохраняя как можно больше информации. Этот метод широко применяется в различных областях, таких как обработка изображений, распознавание образов, анализ данных и машинное обучение.\n",
        "\n",
        "## Цели и задачи PCA\n",
        "\n",
        "1. **Снижение размерности**: Уменьшение количества переменных, что облегчает визуализацию и анализ.\n",
        "2. **Удаление коррелированности**: PCA позволяет преобразовать коррелированные переменные в независимые компоненты.\n",
        "3. **Упрощение модели**: Упрощение алгоритмов машинного обучения, уменьшая количество признаков.\n",
        "4. **Улучшение интерпретируемости**: Упрощение понимания данных и выявление скрытых структур.\n",
        "\n",
        "## Основные концепции\n",
        "\n",
        "### 1. Стандартизация данных\n",
        "\n",
        "Перед применением PCA важно стандартизировать данные, чтобы каждая переменная имела нулевое среднее и единичную дисперсию. Для этого используется формула:\n",
        "\n",
        "$$\n",
        "Z_{ij} = \\frac{X_{ij} - \\mu_j}{\\sigma_j}\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $Z_{ij}$ — стандартизированное значение,\n",
        "- $X_{ij}$ — исходное значение,\n",
        "- $\\mu_j$ — среднее значение переменной $j$,\n",
        "- $\\sigma_j$ — стандартное отклонение переменной $j$.\n",
        "\n",
        "### 2. Ковариационная матрица\n",
        "\n",
        "После стандартизации данных необходимо вычислить ковариационную матрицу. Ковариационная матрица позволяет понять, как переменные соотносятся друг с другом. Формула для вычисления ковариационной матрицы $C$ выглядит так:\n",
        "\n",
        "$$\n",
        "C = \\frac{1}{n-1} Z^T Z\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $Z$ — матрица стандартизированных данных,\n",
        "- $n$ — число наблюдений.\n",
        "\n",
        "### 3. Собственные значения и собственные векторы\n",
        "\n",
        "Следующим шагом является нахождение собственных значений и собственных векторов ковариационной матрицы. Собственные значения показывают, сколько дисперсии объясняет каждая из компонент, а собственные векторы указывают направление этих компонент.\n",
        "\n",
        "Для нахождения собственных значений $\\lambda$ и собственных векторов $v$ решается уравнение:\n",
        "\n",
        "$$\n",
        "C v = \\lambda v\n",
        "$$\n",
        "\n",
        "### 4. Сортировка собственных векторов\n",
        "\n",
        "Собственные векторы сортируются в порядке убывания собственных значений. Это позволяет выбрать первые $k$ главных компонент, которые объясняют наибольшую долю вариации в данных.\n",
        "\n",
        "### 5. Проекция данных\n",
        "\n",
        "После выбора главных компонент исходные данные проецируются на пространство этих компонент с помощью следующей формулы:\n",
        "\n",
        "$$\n",
        "Y = Z W\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $Y$ — проекционные данные,\n",
        "- $W$ — матрица собственных векторов, выбранных в предыдущем шаге.\n",
        "\n",
        "## Пример применения PCA\n",
        "\n",
        "Рассмотрим пример, чтобы наглядно продемонстрировать все шаги PCA.\n",
        "\n",
        "### Исходные данные\n",
        "\n",
        "Предположим, у нас есть следующие данные о двух признаках (например, рост и вес) для пяти человек:\n",
        "\n",
        "| Человек | Рост (см) | Вес (кг) |\n",
        "|---------|-----------|----------|\n",
        "| 1       | 170       | 70       |\n",
        "| 2       | 160       | 60       |\n",
        "| 3       | 180       | 80       |\n",
        "| 4       | 165       | 65       |\n",
        "| 5       | 175       | 75       |\n",
        "\n",
        "### 1. Стандартизация данных\n",
        "\n",
        "Сначала найдем средние значения и стандартные отклонения:\n",
        "\n",
        "- Средний рост: $\\mu_{рост} = \\frac{170 + 160 + 180 + 165 + 175}{5} = 170$\n",
        "- Средний вес: $\\mu_{вес} = \\frac{70 + 60 + 80 + 65 + 75}{5} = 66$\n",
        "\n",
        "- Стандартное отклонение роста: $\\sigma_{рост} = \\sqrt{\\frac{(170-170)^2 + (160-170)^2 + (180-170)^2 + (165-170)^2 + (175-170)^2}{4}} \\approx 7.07$\n",
        "- Стандартное отклонение веса: $\\sigma_{вес} = \\sqrt{\\frac{(70-66)^2 + (60-66)^2 + (80-66)^2 + (65-66)^2 + (75-66)^2}{4}} \\approx 7.07$\n",
        "\n",
        "Теперь стандартизируем данные:\n",
        "\n",
        "| Человек | Рост (Z) | Вес (Z) |\n",
        "|---------|----------|---------|\n",
        "| 1       | 0       | 0.57    |\n",
        "| 2       | -1.41    | -0.85   |\n",
        "| 3       | 1.41     | 1.41    |\n",
        "| 4       | -0.71    | -0.14   |\n",
        "| 5       | 0.71     | 1.14    |\n",
        "\n",
        "### 2. Ковариационная матрица\n",
        "\n",
        "Теперь вычислим ковариационную матрицу:\n",
        "\n",
        "$$\n",
        "C = \\begin{pmatrix}\n",
        "\\text{cov}(Z_{рост}, Z_{рост}) & \\text{cov}(Z_{рост}, Z_{вес}) \\\\\n",
        "\\text{cov}(Z_{вес}, Z_{рост}) & \\text{cov}(Z_{вес}, Z_{вес})\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "Где:\n",
        "\n",
        "$$\n",
        "\\text{cov}(Z_{i}, Z_{j}) = \\frac{1}{n-1} \\sum_{k=1}^{n} (Z_{ik} - \\bar{Z}_{i})(Z_{jk} - \\bar{Z}_{j})\n",
        "$$\n",
        "\n",
        "Вычисляя ковариацию между ростом и весом, получаем:\n",
        "\n",
        "$$\n",
        "C = \\begin{pmatrix}\n",
        "1 & 0.85 \\\\\n",
        "0.85 & 1\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "### 3. Собственные значения и собственные векторы\n",
        "\n",
        "Решаем характеристическое уравнение:\n",
        "\n",
        "$$\n",
        "\\text{det}(C - \\lambda I) = 0\n",
        "$$\n",
        "\n",
        "Где $I$ — единичная матрица.\n",
        "\n",
        "Решая это уравнение, находим собственные значения $\\lambda_1 \\approx 1.85$ и $\\lambda_2 \\approx 0.15$.\n",
        "\n",
        "Находим собственные векторы для этих значений:\n",
        "\n",
        "$$\n",
        "v_1 \\approx \\begin{pmatrix} 0.707 \\\\ 0.707 \\end{pmatrix}, \\quad v_2 \\approx \\begin{pmatrix} -0.707 \\\\ 0.707 \\end{pmatrix}\n",
        "$$\n",
        "\n",
        "### 4. Сортировка собственных векторов\n",
        "\n",
        "Собственные векторы уже отсортированы в порядке убывания собственных значений.\n",
        "\n",
        "### 5. Проекция данных\n",
        "\n",
        "Теперь проецируем стандартизированные данные на главные компоненты. Мы берем первый собственный вектор:\n",
        "\n",
        "$$\n",
        "Y = Z v_1\n",
        "$$\n",
        "\n",
        "Результат будет выглядеть следующим образом:\n",
        "\n",
        "| Человек | PCA (Главная компонента) |\n",
        "|---------|---------------------------|\n",
        "| 1       | 0                         |\n",
        "| 2       | -1.41                     |\n",
        "| 3       | 1.41                      |\n",
        "| 4       | -0.71                     |\n",
        "| 5       | 0.71                      |\n",
        "\n",
        "## Применение PCA в Python\n",
        "\n",
        "В Python для выполнения PCA можно использовать библиотеку `scikit-learn`. Пример кода:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Данные\n",
        "data = np.array([[170, 70],\n",
        "                 [160, 60],\n",
        "                 [180, 80],\n",
        "                 [165, 65],\n",
        "                 [175, 75]])\n",
        "\n",
        "# Стандартизация\n",
        "scaler = StandardScaler()\n",
        "data_standardized = scaler.fit_transform(data)\n",
        "\n",
        "# PCA\n",
        "pca = PCA(n_components=1)  # Количество компонент\n",
        "data_pca = pca.fit_transform(data_standardized)\n",
        "\n",
        "print(\"Проекция на главную компоненту:\\n\", data_pca)\n",
        "```\n",
        "\n",
        "## Заключение\n",
        "\n",
        "Анализ главных компонент (PCA) — это мощный инструмент для уменьшения размерности и упрощения анализа данных. Он позволяет извлекать важную информацию из многомерных данных, улучшая их визуализацию и интерпретацию. Следуя описанным шагам и примерам, можно эффективно применять PCA в различных задачах анализа данных.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Давайте рассмотрим подробные математические примеры применения анализа главных компонент (PCA). Мы разобьем процесс на этапы и предоставим детальные математические решения.\n",
        "\n",
        "## Пример 1: Анализ данных о росте и весе\n",
        "\n",
        "### Данные\n",
        "\n",
        "Рассмотрим набор данных о росте и весе для 5 человек:\n",
        "\n",
        "| Человек | Рост (см) | Вес (кг) |\n",
        "|---------|-----------|----------|\n",
        "| 1       | 170       | 70       |\n",
        "| 2       | 160       | 60       |\n",
        "| 3       | 180       | 80       |\n",
        "| 4       | 165       | 65       |\n",
        "| 5       | 175       | 75       |\n",
        "\n",
        "### Шаг 1: Стандартизация данных\n",
        "\n",
        "Сначала найдем средние значения и стандартные отклонения для каждого признака.\n",
        "\n",
        "#### Средние значения\n",
        "\n",
        "$$\n",
        "\\mu_{\\text{рост}} = \\frac{170 + 160 + 180 + 165 + 175}{5} = \\frac{850}{5} = 170\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\mu_{\\text{вес}} = \\frac{70 + 60 + 80 + 65 + 75}{5} = \\frac{350}{5} = 70\n",
        "$$\n",
        "\n",
        "#### Стандартные отклонения\n",
        "\n",
        "Стандартное отклонение для роста:\n",
        "\n",
        "$$\n",
        "\\sigma_{\\text{рост}} = \\sqrt{\\frac{(170 - 170)^2 + (160 - 170)^2 + (180 - 170)^2 + (165 - 170)^2 + (175 - 170)^2}{5 - 1}} = \\sqrt{\\frac{0 + 100 + 100 + 25 + 25}{4}} = \\sqrt{\\frac{250}{4}} = \\sqrt{62.5} \\approx 7.91\n",
        "$$\n",
        "\n",
        "Стандартное отклонение для веса:\n",
        "\n",
        "$$\n",
        "\\sigma_{\\text{вес}} = \\sqrt{\\frac{(70 - 70)^2 + (60 - 70)^2 + (80 - 70)^2 + (65 - 70)^2 + (75 - 70)^2}{5 - 1}} = \\sqrt{\\frac{0 + 100 + 100 + 25 + 25}{4}} = \\sqrt{\\frac{250}{4}} = \\sqrt{62.5} \\approx 7.91\n",
        "$$\n",
        "\n",
        "#### Стандартизированные данные\n",
        "\n",
        "Теперь можем стандартизировать данные:\n",
        "\n",
        "$$\n",
        "Z_{ij} = \\frac{X_{ij} - \\mu_j}{\\sigma_j}\n",
        "$$\n",
        "\n",
        "| Человек | Рост (Z)               | Вес (Z)                |\n",
        "|---------|-----------------------|-----------------------|\n",
        "| 1       | $Z_{1,1} = \\frac{170 - 170}{7.91} = 0$  | $Z_{1,2} = \\frac{70 - 70}{7.91} = 0$    |\n",
        "| 2       | $Z_{2,1} = \\frac{160 - 170}{7.91} \\approx -1.27$ | $Z_{2,2} = \\frac{60 - 70}{7.91} \\approx -1.27$ |\n",
        "| 3       | $Z_{3,1} = \\frac{180 - 170}{7.91} \\approx 1.27$  | $Z_{3,2} = \\frac{80 - 70}{7.91} \\approx 1.27$  |\n",
        "| 4       | $Z_{4,1} = \\frac{165 - 170}{7.91} \\approx -0.63$ | $Z_{4,2} = \\frac{65 - 70}{7.91} \\approx -0.63$ |\n",
        "| 5       | $Z_{5,1} = \\frac{175 - 170}{7.91} \\approx 0.63$  | $Z_{5,2} = \\frac{75 - 70}{7.91} \\approx 0.63$  |\n",
        "\n",
        "Теперь у нас есть матрица стандартизированных данных:\n",
        "\n",
        "$$\n",
        "Z = \\begin{pmatrix}\n",
        "0     & 0     \\\\\n",
        "-1.27 & -1.27 \\\\\n",
        "1.27  & 1.27  \\\\\n",
        "-0.63 & -0.63 \\\\\n",
        "0.63  & 0.63\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "### Шаг 2: Вычисление ковариационной матрицы\n",
        "\n",
        "Ковариационная матрица $C$ вычисляется как:\n",
        "\n",
        "$$\n",
        "C = \\frac{1}{n-1} Z^T Z\n",
        "$$\n",
        "\n",
        "где $n$ — количество наблюдений.\n",
        "\n",
        "Подсчитаем:\n",
        "\n",
        "$$\n",
        "Z^T = \\begin{pmatrix}\n",
        "0     & -1.27 & 1.27  & -0.63 & 0.63  \\\\\n",
        "0     & -1.27 & 1.27  & -0.63 & 0.63\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "Теперь вычислим произведение $Z^T Z$:\n",
        "\n",
        "$$\n",
        "Z^T Z = \\begin{pmatrix}\n",
        "0^2 + (-1.27)^2 + (1.27)^2 + (-0.63)^2 + (0.63)^2 & 0 \\cdot 0 + (-1.27) \\cdot (-1.27) + 1.27 \\cdot 1.27 + (-0.63) \\cdot (-0.63) + 0.63 \\cdot 0.63 \\\\\n",
        "0 \\cdot 0 + (-1.27) \\cdot (-1.27) + 1.27 \\cdot 1.27 + (-0.63) \\cdot (-0.63) + 0.63 \\cdot 0.63 & 0^2 + (-1.27)^2 + (1.27)^2 + (-0.63)^2 + (0.63)^2\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "Таким образом, получим:\n",
        "\n",
        "$$\n",
        "Z^T Z = \\begin{pmatrix}\n",
        "0 + 1.6129 + 1.6129 + 0.3969 + 0.3969 & 0 \\\\\n",
        "0 & 0 + 1.6129 + 1.6129 + 0.3969 + 0.3969\n",
        "\\end{pmatrix}\n",
        "= \\begin{pmatrix}\n",
        "4.0196 & 0 \\\\\n",
        "0 & 4.0196\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "Теперь вычислим ковариационную матрицу:\n",
        "\n",
        "$$\n",
        "C = \\frac{1}{5-1} Z^T Z = \\frac{1}{4} \\begin{pmatrix}\n",
        "4.0196 & 0 \\\\\n",
        "0 & 4.0196\n",
        "\\end{pmatrix} = \\begin{pmatrix}\n",
        "1.0049 & 0 \\\\\n",
        "0 & 1.0049\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "### Шаг 3: Нахождение собственных значений и собственных векторов\n",
        "\n",
        "Теперь найдем собственные значения и собственные векторы матрицы ковариации $C$.\n",
        "\n",
        "Решаем характеристическое уравнение:\n",
        "\n",
        "$$\n",
        "\\text{det}(C - \\lambda I) = 0\n",
        "$$\n",
        "\n",
        "где $I$ — единичная матрица. То есть:\n",
        "\n",
        "$$\n",
        "\\text{det}\\left(\\begin{pmatrix}\n",
        "1.0049 - \\lambda & 0 \\\\\n",
        "0 & 1.0049 - \\lambda\n",
        "\\end{pmatrix}\\right) = (1.0049 - \\lambda)^2 = 0\n",
        "$$\n",
        "\n",
        "Таким образом, находим собственные значения:\n",
        "\n",
        "$$\n",
        "\\lambda_1 = \\lambda_2 = 1.0049\n",
        "$$\n",
        "\n",
        "Теперь найдем собственные векторы. Подставим $\\lambda$ в уравнение:\n",
        "\n",
        "$$\n",
        "(C - \\lambda I)v = 0\n",
        "$$\n",
        "\n",
        "Получаем:\n",
        "\n",
        "$$\n",
        "\\begin{pmatrix}\n",
        "1.0049 - \\lambda & 0 \\\\\n",
        "0 & 1.0049 - \\lambda\n",
        "\\end{pmatrix} \\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix} = 0\n",
        "$$\n",
        "\n",
        "Поскольку оба собственных значения равны, это говорит о том, что у нас два одинаковых собственных вектора:\n",
        "\n",
        "$$\n",
        "v = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\quad \\text{и} \\quad v = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\n",
        "$$\n",
        "\n",
        "### Шаг 4: Сортировка собственных векторов\n",
        "\n",
        "В данном случае оба собственных вектора имеют одинаковую важность, и мы можем выбрать любой из них для проекции данных.\n",
        "\n",
        "### Шаг 5: Проекция данных\n",
        "\n",
        "Проектируем стандартизированные данные на первую главную компоненту:\n",
        "\n",
        "$$\n",
        "Y = Z v\n",
        "$$\n",
        "\n",
        "Выберем, например, собственный вектор $v = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$:\n",
        "\n",
        "$$\n",
        "Y = Z \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} =\n",
        "\n",
        " \\begin{pmatrix}\n",
        "0 \\\\\n",
        "-1.27 \\\\\n",
        "1.27 \\\\\n",
        "-0.63 \\\\\n",
        "0.63\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "### Результаты\n",
        "\n",
        "Мы получили проекцию исходных данных на главную компоненту. Эта информация может быть использована для дальнейшего анализа.\n",
        "\n",
        "## Пример 2: Анализ изображений\n",
        "\n",
        "Рассмотрим простой пример с изображениями, где каждый пиксель будет представлять собой переменную.\n",
        "\n",
        "### Данные\n",
        "\n",
        "Предположим, у нас есть 4 изображения (по 4 пикселя каждое), которые представлены следующим образом:\n",
        "\n",
        "| Изображение | Пиксель 1 | Пиксель 2 | Пиксель 3 | Пиксель 4 |\n",
        "|-------------|-----------|-----------|-----------|-----------|\n",
        "| 1           | 1         | 2         | 3         | 4         |\n",
        "| 2           | 2         | 3         | 4         | 5         |\n",
        "| 3           | 3         | 4         | 5         | 6         |\n",
        "| 4           | 4         | 5         | 6         | 7         |\n",
        "\n",
        "### Шаг 1: Стандартизация данных\n",
        "\n",
        "Найдем средние значения и стандартные отклонения для каждого пикселя.\n",
        "\n",
        "#### Средние значения\n",
        "\n",
        "$$\n",
        "\\mu_{1} = \\frac{1 + 2 + 3 + 4}{4} = \\frac{10}{4} = 2.5\n",
        "$$\n",
        "$$\n",
        "\\mu_{2} = \\frac{2 + 3 + 4 + 5}{4} = \\frac{14}{4} = 3.5\n",
        "$$\n",
        "$$\n",
        "\\mu_{3} = \\frac{3 + 4 + 5 + 6}{4} = \\frac{18}{4} = 4.5\n",
        "$$\n",
        "$$\n",
        "\\mu_{4} = \\frac{4 + 5 + 6 + 7}{4} = \\frac{22}{4} = 5.5\n",
        "$$\n",
        "\n",
        "#### Стандартные отклонения\n",
        "\n",
        "$$\n",
        "\\sigma_{1} = \\sqrt{\\frac{(1-2.5)^2 + (2-2.5)^2 + (3-2.5)^2 + (4-2.5)^2}{4 - 1}} = \\sqrt{\\frac{2.25 + 0.25 + 0.25 + 2.25}{3}} = \\sqrt{\\frac{5}{3}} \\approx 1.29\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\sigma_{2} = \\sqrt{\\frac{(2-3.5)^2 + (3-3.5)^2 + (4-3.5)^2 + (5-3.5)^2}{3}} = \\sqrt{\\frac{2.25 + 0.25 + 0.25 + 2.25}{3}} = \\sqrt{\\frac{5}{3}} \\approx 1.29\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\sigma_{3} = \\sqrt{\\frac{(3-4.5)^2 + (4-4.5)^2 + (5-4.5)^2 + (6-4.5)^2}{3}} = \\sqrt{\\frac{2.25 + 0.25 + 0.25 + 2.25}{3}} = \\sqrt{\\frac{5}{3}} \\approx 1.29\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\sigma_{4} = \\sqrt{\\frac{(4-5.5)^2 + (5-5.5)^2 + (6-5.5)^2 + (7-5.5)^2}{3}} = \\sqrt{\\frac{2.25 + 0.25 + 0.25 + 2.25}{3}} = \\sqrt{\\frac{5}{3}} \\approx 1.29\n",
        "$$\n",
        "\n",
        "#### Стандартизированные данные\n",
        "\n",
        "Теперь стандартизируем данные:\n",
        "\n",
        "$$\n",
        "Z_{ij} = \\frac{X_{ij} - \\mu_j}{\\sigma_j}\n",
        "$$\n",
        "\n",
        "| Изображение | Пиксель 1 (Z) | Пиксель 2 (Z) | Пиксель 3 (Z) | Пиксель 4 (Z) |\n",
        "|-------------|----------------|----------------|----------------|----------------|\n",
        "| 1           | $\\frac{1 - 2.5}{1.29} \\approx -1.16$ | $\\frac{2 - 3.5}{1.29} \\approx -1.16$ | $\\frac{3 - 4.5}{1.29} \\approx -1.16$ | $\\frac{4 - 5.5}{1.29} \\approx -1.16$ |\n",
        "| 2           | $\\frac{2 - 2.5}{1.29} \\approx -0.39$ | $\\frac{3 - 3.5}{1.29} \\approx -0.39$ | $\\frac{4 - 4.5}{1.29} \\approx -0.39$ | $\\frac{5 - 5.5}{1.29} \\approx -0.39$ |\n",
        "| 3           | $\\frac{3 - 2.5}{1.29} \\approx 0.39$ | $\\frac{4 - 3.5}{1.29} \\approx 0.39$ | $\\frac{5 - 4.5}{1.29} \\approx 0.39$ | $\\frac{6 - 5.5}{1.29} \\approx 0.39$ |\n",
        "| 4           | $\\frac{4 - 2.5}{1.29} \\approx 1.16$ | $\\frac{5 - 3.5}{1.29} \\approx 1.16$ | $\\frac{6 - 4.5}{1.29} \\approx 1.16$ | $\\frac{7 - 5.5}{1.29} \\approx 1.16$ |\n",
        "\n",
        "Стандартизированная матрица:\n",
        "\n",
        "$$\n",
        "Z = \\begin{pmatrix}\n",
        "-1.16 & -1.16 & -1.16 & -1.16 \\\\\n",
        "-0.39 & -0.39 & -0.39 & -0.39 \\\\\n",
        "0.39  & 0.39  & 0.39  & 0.39  \\\\\n",
        "1.16  & 1.16  & 1.16  & 1.16\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "### Шаг 2: Вычисление ковариационной матрицы\n",
        "\n",
        "$$\n",
        "C = \\frac{1}{n-1} Z^T Z\n",
        "$$\n",
        "\n",
        "Вычисляем $Z^T$ и $Z^T Z$:\n",
        "\n",
        "$$\n",
        "Z^T = \\begin{pmatrix}\n",
        "-1.16 & -0.39 & 0.39 & 1.16 \\\\\n",
        "-1.16 & -0.39 & 0.39 & 1.16 \\\\\n",
        "-1.16 & -0.39 & 0.39 & 1.16 \\\\\n",
        "-1.16 & -0.39 & 0.39 & 1.16\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "Теперь найдем:\n",
        "\n",
        "$$\n",
        "Z^T Z = \\begin{pmatrix}\n",
        "(-1.16)^2 + (-0.39)^2 + (0.39)^2 + (1.16)^2 & ... \\\\\n",
        "... & ...\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "Вычисляя каждую ячейку, мы получаем:\n",
        "\n",
        "$$\n",
        "Z^T Z = \\begin{pmatrix}\n",
        "4 & 4 & 4 & 4 \\\\\n",
        "4 & 4 & 4 & 4 \\\\\n",
        "4 & 4 & 4 & 4 \\\\\n",
        "4 & 4 & 4 & 4\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "Теперь вычисляем ковариационную матрицу:\n",
        "\n",
        "$$\n",
        "C = \\frac{1}{4 - 1} Z^T Z = \\frac{1}{3} \\begin{pmatrix}\n",
        "4 & 4 & 4 & 4 \\\\\n",
        "4 & 4 & 4 & 4 \\\\\n",
        "4 & 4 & 4 & 4 \\\\\n",
        "4 & 4 & 4 & 4\n",
        "\\end{pmatrix} = \\begin{pmatrix}\n",
        "1.33 & 1.33 & 1.33 & 1.33 \\\\\n",
        "1.33 & 1.33 & 1.33 & 1.33 \\\\\n",
        "1.33 & 1.33 & 1.33 & 1.33 \\\\\n",
        "1.33 & 1.33 & 1.33 & 1.33\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "### Шаг 3: Нахождение собственных значений и собственных векторов\n",
        "\n",
        "Решаем характеристическое уравнение:\n",
        "\n",
        "$$\n",
        "\\text{det}(C - \\lambda I) = 0\n",
        "$$\n",
        "\n",
        "Т.е.\n",
        "\n",
        "$$\n",
        "\\text{det}\\left(\\begin{pmatrix}\n",
        "1.33 - \\lambda & 1.33 & 1.33 & 1.33 \\\\\n",
        "1.33\n",
        "\n",
        " & 1.33 - \\lambda & 1.33 & 1.33 \\\\\n",
        "1.33 & 1.33 & 1.33 - \\lambda & 1.33 \\\\\n",
        "1.33 & 1.33 & 1.33 & 1.33 - \\lambda\n",
        "\\end{pmatrix}\\right) = 0\n",
        "$$\n",
        "\n",
        "Решая это уравнение, находим собственные значения и собственные векторы.\n",
        "\n",
        "### Шаг 4: Проекция данных\n",
        "\n",
        "Для проекции используем найденные собственные векторы и получим уменьшенное представление данных.\n",
        "\n",
        "### Заключение\n",
        "\n",
        "PCA — мощный метод анализа данных, который помогает уменьшить размерность и выявить основные компоненты в наборе данных. Мы рассмотрели два примера с детальными математическими шагами, которые иллюстрируют, как проводить анализ главных компонент на практических данных.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Давайте реализуем анализ главных компонент (PCA) на Python. Мы воспользуемся библиотеками `numpy` и `pandas` для работы с данными и `matplotlib` для визуализации.\n",
        "\n",
        "### Пример 1: Анализ данных о росте и весе\n",
        "\n",
        "#### Установка необходимых библиотек\n",
        "\n",
        "Если у вас еще не установлены библиотеки, вы можете установить их с помощью следующей команды:\n",
        "\n",
        "```bash\n",
        "pip install numpy pandas matplotlib scikit-learn\n",
        "```\n",
        "\n",
        "#### Код\n",
        "\n",
        "Теперь давайте напишем код для выполнения PCA на примере данных о росте и весе.\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Данные о росте и весе\n",
        "data = {\n",
        "    'Рост': [170, 160, 180, 165, 175],\n",
        "    'Вес': [70, 60, 80, 65, 75]\n",
        "}\n",
        "\n",
        "# Создаем DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Стандартизация данных\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(df)\n",
        "\n",
        "# Применяем PCA\n",
        "pca = PCA(n_components=1)\n",
        "principal_components = pca.fit_transform(scaled_data)\n",
        "\n",
        "# Создаем DataFrame с главными компонентами\n",
        "principal_df = pd.DataFrame(data=principal_components, columns=['Первая главная компонента'])\n",
        "\n",
        "# Визуализация\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(principal_df['Первая главная компонента'], np.zeros_like(principal_df['Первая главная компонента']),\n",
        "            color='blue', alpha=0.6)\n",
        "plt.title('PCA: Первая главная компонента')\n",
        "plt.xlabel('Первая главная компонента')\n",
        "plt.yticks([])\n",
        "plt.grid()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "### Пример 2: Анализ изображений\n",
        "\n",
        "Теперь давайте реализуем PCA для простого примера с изображениями (по 4 пикселя каждое).\n",
        "\n",
        "#### Код\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Данные об изображениях (4 пикселя на изображение)\n",
        "data = {\n",
        "    'Пиксель_1': [1, 2, 3, 4],\n",
        "    'Пиксель_2': [2, 3, 4, 5],\n",
        "    'Пиксель_3': [3, 4, 5, 6],\n",
        "    'Пиксель_4': [4, 5, 6, 7]\n",
        "}\n",
        "\n",
        "# Создаем DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Стандартизация данных\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(df)\n",
        "\n",
        "# Применяем PCA\n",
        "pca = PCA(n_components=2)  # Уменьшаем до 2-х компонентов для визуализации\n",
        "principal_components = pca.fit_transform(scaled_data)\n",
        "\n",
        "# Создаем DataFrame с главными компонентами\n",
        "principal_df = pd.DataFrame(data=principal_components, columns=['Первая главная компонента', 'Вторая главная компонента'])\n",
        "\n",
        "# Визуализация\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(principal_df['Первая главная компонента'], principal_df['Вторая главная компонента'], color='red', alpha=0.6)\n",
        "plt.title('PCA: Главные компоненты изображений')\n",
        "plt.xlabel('Первая главная компонента')\n",
        "plt.ylabel('Вторая главная компонента')\n",
        "plt.grid()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "### Запуск кода\n",
        "\n",
        "1. Скопируйте каждый блок кода в отдельные ячейки вашего Python-окружения или в один файл Python.\n",
        "2. Запустите код, и вы должны увидеть визуализации PCA для обоих примеров.\n",
        "\n",
        "### Результаты\n",
        "\n",
        "- Для первого примера (рост и вес) вы получите график, отображающий первую главную компоненту.\n",
        "- Для второго примера (изображения) будет отображен график с первыми двумя главными компонентами.\n",
        "\n",
        "Если у вас есть дополнительные вопросы или вам нужна помощь с конкретной частью, дайте знать!"
      ],
      "metadata": {
        "id": "DpqB84rOO7Ik"
      }
    }
  ]
}