{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMQfKQhN9fi4GmtEkSxJ7AW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CodeHunterOfficial/AI_DataMining/blob/main/NLP/Lesson%202.%20%20%D0%92%D0%B5%D0%BA%D1%82%D0%BE%D1%80%D0%BD%D1%8B%D0%B5_%D0%BF%D1%80%D0%B5%D0%B4%D1%81%D1%82%D0%B0%D0%B2%D0%BB%D0%B5%D0%BD%D0%B8%D1%8F_%D1%81%D0%BB%D0%BE%D0%B2_(Word2Vec%2CGloVe%2CELMo%2CSkip_Gram)/2_0_4_%D0%91%D0%B8%D0%B0%D0%B3%D1%80%D0%B0%D0%BC%D0%BC%D1%8B%2C_%D0%A2%D1%80%D0%B8%D0%B3%D1%80%D0%B0%D0%BC%D0%BC%D1%8B_%D0%B8_N_%D0%B3%D1%80%D0%B0%D0%BC%D0%BC%D1%8B_%D0%B2_%D0%BE%D0%B1%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%BA%D0%B5_%D1%82%D0%B5%D0%BA%D1%81%D1%82%D0%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Биаграммы, Триграммы и N-граммы в обработке текста\n",
        "\n",
        "#### Введение в N-граммы\n",
        "**N-граммы** — это последовательности из N элементов (слов, букв или других символов), которые извлекаются из текста для анализа его структуры и содержания. Такие последовательности широко используются в задачах обработки естественного языка (NLP), машинного обучения, а также в статистическом анализе текста. N-граммы дают возможность исследовать текст с точки зрения его лексических и синтаксических особенностей, позволяют оценить вероятности словосочетаний и предсказать следующее слово.\n",
        "\n",
        "#### Классификация N-грамм\n",
        "N-граммы можно классифицировать по количеству элементов:\n",
        "1. **Униграмма (1-грамма)**: состоит из одного слова или символа. Пример: [\"солнце\", \"светит\", \"ярко\"].\n",
        "2. **Биаграмма (2-грамма)**: включает в себя два слова или символа. Пример: [\"солнце светит\", \"светит ярко\"].\n",
        "3. **Триграмма (3-грамма)**: состоит из трех слов или символов. Пример: [\"солнце светит ярко\"].\n",
        "4. **N-граммы**: общее название для последовательностей, состоящих из N элементов. Например, 4-грамма: [\"ярко светит солнце сегодня\"].\n",
        "\n",
        "### Применение и задачи для N-грамм\n",
        "N-граммы применяются в различных задачах анализа текста:\n",
        "1. **Анализ частоты**: позволяет определить наиболее частые последовательности слов, что помогает выявлять ключевые фразы и важные термины.\n",
        "2. **Предсказание следующего слова**: в задачах машинного перевода и автозаполнения. Например, с помощью биграмм можно предсказать, что после слова \"солнце\" с высокой вероятностью может следовать \"светит\".\n",
        "3. **Обработка грамматики и синтаксиса**: триграммы и биграммы часто используются для обучения грамматическим структурам текста.\n",
        "4. **Анализ семантики и тональности текста**: с помощью N-грамм можно оценить эмоциональную окраску текста, найти повторы или определить шаблоны в текстах.\n",
        "\n",
        "### Как создаются N-граммы\n",
        "Для создания N-грамм необходимо:\n",
        "1. Разбить текст на последовательности из N элементов (например, слов или символов).\n",
        "2. Каждая последовательность фиксируется в виде отдельного N-грамм-объекта.\n",
        "3. Собранные N-граммы обрабатываются: подсчитывается частота каждой последовательности или оценивается их вероятность.\n",
        "\n",
        "Пример текста: \"солнце светит ярко на небе\"\n",
        "- Биграммы: [\"солнце светит\", \"светит ярко\", \"ярко на\", \"на небе\"]\n",
        "- Триграммы: [\"солнце светит ярко\", \"светит ярко на\", \"ярко на небе\"]\n"
      ],
      "metadata": {
        "id": "XCpBrZq8DE6w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Давайте рассмотрим примеры на Python для разбиения текста на биграммы, триграммы и N-граммы. В этом примере код гибкий и позволяет задавать любое значение \\( N \\), чтобы разбить текст на N-граммы.\n",
        "\n",
        "### Пример кода для разбиения текста на N-граммы\n"
      ],
      "metadata": {
        "id": "3sgelYRfDGDW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Tuple\n",
        "\n",
        "# Функция для разбиения текста на N-граммы\n",
        "def generate_ngrams(text: str, n: int) -> List[Tuple[str, ...]]:\n",
        "    words = text.lower().replace('.', '').split()  # Приведение текста к нижнему регистру, удаление точек и разбиение на слова\n",
        "    ngrams = [tuple(words[i:i + n]) for i in range(len(words) - n + 1)]\n",
        "    return ngrams\n",
        "\n",
        "# Пример текста\n",
        "text = \"Кошка ловит мышь. Кошка играет с мышью. Кошка мяукает.\"\n",
        "\n",
        "# Генерация биграмм\n",
        "bigrams = generate_ngrams(text, 2)\n",
        "print(\"Биграммы:\", bigrams)\n",
        "\n",
        "# Генерация триграмм\n",
        "trigrams = generate_ngrams(text, 3)\n",
        "print(\"Триграммы:\", trigrams)\n",
        "\n",
        "# Генерация 4-грамм (пример N-граммы, где N=4)\n",
        "fourgrams = generate_ngrams(text, 4)\n",
        "print(\"4-граммы:\", fourgrams)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zuApFr0XDGUM",
        "outputId": "10174c76-ab2f-4dd9-94b8-1534a92adad1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Биграммы: [('кошка', 'ловит'), ('ловит', 'мышь'), ('мышь', 'кошка'), ('кошка', 'играет'), ('играет', 'с'), ('с', 'мышью'), ('мышью', 'кошка'), ('кошка', 'мяукает')]\n",
            "Триграммы: [('кошка', 'ловит', 'мышь'), ('ловит', 'мышь', 'кошка'), ('мышь', 'кошка', 'играет'), ('кошка', 'играет', 'с'), ('играет', 'с', 'мышью'), ('с', 'мышью', 'кошка'), ('мышью', 'кошка', 'мяукает')]\n",
            "4-граммы: [('кошка', 'ловит', 'мышь', 'кошка'), ('ловит', 'мышь', 'кошка', 'играет'), ('мышь', 'кошка', 'играет', 'с'), ('кошка', 'играет', 'с', 'мышью'), ('играет', 'с', 'мышью', 'кошка'), ('с', 'мышью', 'кошка', 'мяукает')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Пояснение кода\n",
        "\n",
        "1. **Функция `generate_ngrams`**: Принимает текст и значение $n$, указывающее, какого размера должны быть N-граммы (например, 2 для биграмм, 3 для триграмм и т.д.).\n",
        "   - Текст обрабатывается: переводится в нижний регистр, удаляются точки и текст разбивается на слова.\n",
        "   - Генерация N-грамм осуществляется с помощью срезов списка: на каждой итерации создается кортеж, состоящий из \\( n \\) последовательных слов.\n",
        "2. **Использование функции**: Текст разбивается на биграммы, триграммы и 4-граммы с помощью функции `generate_ngrams`.\n"
      ],
      "metadata": {
        "id": "oOi3sI4VDIUN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Математическое представление N-грамм\n",
        "Для вероятностного анализа последовательности слов используется **марковская модель**, где вероятность появления слова зависит от предыдущих $N-1$ слов (контекста).\n",
        "\n",
        "**Пример**: Для биграммы $P(w_i|w_{i-1})$ вероятность слова $w_i$ при условии, что до него стояло $w_{i-1}$, можно вычислить как:\n",
        "\n",
        "$$\n",
        "P(w_i|w_{i-1}) = \\frac{C(w_{i-1}, w_i)}{C(w_{i-1})}\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $C(w_{i-1}, w_i)$ — частота встречаемости биграммы (w_{i-1}, w_i).\n",
        "- $C(w_{i-1})$ — частота встречаемости слова w_{i-1}.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Рассмотрим пример для вероятности биграммы $P(w_i | w_{i-1})$ на конкретных данных.\n",
        "\n",
        "Допустим, у нас есть небольшой текст:\n",
        "\n",
        "> \"Кошка ловит мышь. Кошка играет с мышью. Кошка мяукает.\"\n",
        "\n",
        "Для этой фразы мы найдем вероятность слова $w_i$ при условии, что перед ним стоит $w_{i-1}$.\n",
        "\n",
        "### Шаг 1: Подсчет частоты встречаемости слов и биграмм\n",
        "Для начала выпишем все биграммы, встречающиеся в тексте, и подсчитаем их частоты.\n",
        "\n",
        "**Слова и частоты встречаемости:**\n",
        "1. **Кошка** — 3 раза\n",
        "2. **ловит** — 1 раз\n",
        "3. **мышь** — 1 раз\n",
        "4. **играет** — 1 раз\n",
        "5. **с** — 1 раз\n",
        "6. **мышью** — 1 раз\n",
        "7. **мяукает** — 1 раз\n",
        "\n",
        "**Биграммы и частоты встречаемости:**\n",
        "1. **Кошка ловит** — 1 раз\n",
        "2. **ловит мышь** — 1 раз\n",
        "3. **Кошка играет** — 1 раз\n",
        "4. **играет с** — 1 раз\n",
        "5. **с мышью** — 1 раз\n",
        "6. **Кошка мяукает** — 1 раз\n",
        "\n",
        "### Шаг 2: Вычисление вероятностей для биграмм\n",
        "Теперь, используя формулу:\n",
        "\n",
        "$$\n",
        "P(w_i|w_{i-1}) = \\frac{C(w_{i-1}, w_i)}{C(w_{i-1})}\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $C(w_{i-1}, w_i)$ — частота биграммы (w_{i-1}, w_i),\n",
        "- $C(w_{i-1})$ — частота слова $w_{i-1}$,\n",
        "\n",
        "рассчитаем вероятность для каждой биграммы.\n",
        "\n",
        "#### Пример 1: Вероятность биграммы \"Кошка ловит\"\n",
        "Для биграммы **\"Кошка ловит\"**:\n",
        "- $C(\\text{\"Кошка ловит\"}) = 1$\n",
        "- $C(\\text{\"Кошка\"}) = 3$\n",
        "\n",
        "Подставляем значения в формулу:\n",
        "\n",
        "$$\n",
        "P(\\text{\"ловит\"}|\\text{\"Кошка\"}) = \\frac{C(\\text{\"Кошка ловит\"})}{C(\\text{\"Кошка\"})} = \\frac{1}{3} \\approx 0.33\n",
        "$$\n",
        "\n",
        "Это значит, что вероятность встретить слово \"ловит\" после слова \"Кошка\" в этом тексте составляет примерно 0.33 (33%).\n",
        "\n",
        "#### Пример 2: Вероятность биграммы \"Кошка играет\"\n",
        "Для биграммы **\"Кошка играет\"**:\n",
        "- $C(\\text{\"Кошка играет\"}) = 1$\n",
        "- $C(\\text{\"Кошка\"}) = 3$\n",
        "\n",
        "Подставляем значения:\n",
        "\n",
        "$$\n",
        "P(\\text{\"играет\"}|\\text{\"Кошка\"}) = \\frac{C(\\text{\"Кошка играет\"})}{C(\\text{\"Кошка\"})} = \\frac{1}{3} \\approx 0.33\n",
        "$$\n",
        "\n",
        "Вероятность встретить \"играет\" после \"Кошка\" тоже составляет примерно 0.33 (33%).\n",
        "\n",
        "#### Пример 3: Вероятность биграммы \"Кошка мяукает\"\n",
        "Для биграммы **\"Кошка мяукает\"**:\n",
        "- $C(\\text{\"Кошка мяукает\"}) = 1$\n",
        "- $C(\\text{\"Кошка\"}) = 3$\n",
        "\n",
        "Подставляем значения:\n",
        "\n",
        "$$\n",
        "P(\\text{\"мяукает\"}|\\text{\"Кошка\"}) = \\frac{C(\\text{\"Кошка мяукает\"})}{C(\\text{\"Кошка\"})} = \\frac{1}{3} \\approx 0.33\n",
        "$$\n",
        "\n",
        "Эта вероятность также составляет примерно 0.33 (33%).\n",
        "\n",
        "### Итоговые вероятности\n",
        "Мы рассчитали, что после слова \"Кошка\" с равной вероятностью (по 0.33) может идти любое из следующих слов: \"ловит\", \"играет\" или \"мяукает\".\n",
        "\n",
        "### Интерпретация\n",
        "Если бы эта модель использовалась, например, для предсказания следующего слова после \"Кошка\", то модель предложила бы с одинаковой вероятностью одно из трех слов (\"ловит\", \"играет\" или \"мяукает\") на основе имеющегося текста.\n",
        "\n",
        "### Ограничения примера\n",
        "В реальной обработке текстов частоты биграмм вычисляются на гораздо больших объемах данных, что позволяет получить более точные вероятности.\n",
        "\n",
        "\n",
        "### Плюсы и минусы N-граммного подхода\n",
        "\n",
        "#### Плюсы:\n",
        "- **Простота реализации**: создание и использование N-грамм в анализе текста не требует сложных алгоритмов.\n",
        "- **Локальная зависимость**: N-граммы хорошо моделируют зависимости между словами на малых интервалах, что полезно для задач автозаполнения и предсказания.\n",
        "- **Улучшение точности классификации текста**: N-граммы помогают учитывать контекст и улучшают результаты в задачах классификации текстов.\n",
        "\n",
        "#### Минусы:\n",
        "- **Пространственные ограничения**: большие N-граммы создают длинные последовательности, что требует больших вычислительных мощностей и памяти.\n",
        "- **Проблема \"разрыва зависимости\"**: N-граммы фиксированной длины плохо отражают длинные зависимости между словами.\n",
        "- **Склонность к переобучению**: при слишком большом значении N модель может запомнить редкие N-граммы, которые нечасто встречаются в реальных текстах.\n",
        "\n",
        "\n",
        "### Взвешивание N-грамм и сглаживание\n",
        "\n",
        "Одной из проблем в статистическом моделировании языка является обработка **редких** или **невидимых N-грамм** — таких, которые не встречаются в обучающих данных. Это приводит к **нулевым вероятностям**, что затрудняет предсказание или анализ текста. Например, если мы обучаем модель на большом количестве текста, но в нем не встречается биграмма, то вероятность ее появления в тестовой выборке будет равна нулю.\n",
        "\n",
        "Для решения этой проблемы разработаны различные методы сглаживания, которые помогают скорректировать оценки вероятностей для редких или невидимых N-грамм.\n",
        "\n",
        "### 1. **Аддитивное сглаживание (Лапласовское сглаживание)**\n",
        "\n",
        "**Аддитивное сглаживание** — это простой метод, при котором к каждой частоте встречаемости добавляется некоторое фиксированное значение. Обычно это значение равно 1. Это сглаживание позволяет избежать нулевых вероятностей, добавляя «небольшую уверенность» в том, что все возможные события могут произойти, даже если их не наблюдали в тренировочных данных.\n",
        "\n",
        "#### Формула для аддитивного сглаживания:\n",
        "Для вычисления вероятности биграммы $P(w_i | w_{i-1})$ с применением аддитивного сглаживания:\n",
        "\n",
        "$$\n",
        "P(w_i | w_{i-1}) = \\frac{C(w_{i-1}, w_i) + \\alpha}{C(w_{i-1}) + \\alpha \\cdot V}\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $C(w_{i-1}, w_i)$ — частота биграммы (пары слов $w_{i-1}, w_i$),\n",
        "- $C(w_{i-1})$ — частота слова $w_{i-1}$,\n",
        "- $\\alpha$ — коэффициент сглаживания (обычно $\\alpha = 1$),\n",
        "- $V$ — размер словаря (общее количество уникальных слов в данных).\n",
        "\n",
        "**Пример**:\n",
        "\n",
        "Допустим, у нас есть простой текст: \"Кошка ловит мышь\", и мы хотим вычислить вероятность биграммы $P(\\text{ловит} | \\text{кошка})$.\n",
        "\n",
        "Предположим, что частота биграммы \"Кошка ловит\" — 1, а частота слова \"Кошка\" — 2. Кроме того, в нашем корпусе словарь состоит из 5 уникальных слов.\n",
        "\n",
        "Используя Лапласовское сглаживание с $\\alpha = 1$, вероятность $P(\\text{ловит} | \\text{кошка})$ будет вычисляться так:\n",
        "\n",
        "$$\n",
        "P(\\text{ловит} | \\text{кошка}) = \\frac{C(\\text{кошка}, \\text{ловит}) + 1}{C(\\text{кошка}) + 1 \\cdot 5} = \\frac{1 + 1}{2 + 5} = \\frac{2}{7} \\approx 0.286\n",
        "$$\n",
        "\n",
        "Таким образом, вероятность биграммы \"Кошка ловит\" с применением аддитивного сглаживания составляет около 0.286, несмотря на то, что изначально частота была всего 1.\n",
        "\n",
        "\n",
        "### 2. **Сглаживание Кнезера-Нея**\n",
        "\n",
        "**Сглаживание Кнезера-Нея** (Kneser-Ney smoothing) — это метод сглаживания, который используется для более точной оценки вероятностей N-грамм, особенно когда речь идет о редких или невидимых N-граммах в тексте. Этот метод является улучшением традиционных методов, таких как Лапласовское сглаживание, и эффективно корректирует вероятности для редко встречающихся N-грамм.\n",
        "\n",
        "#### Основная идея метода\n",
        "\n",
        "Сглаживание Кнезера-Нея основано на **рекурсии** и оценке вероятностей N-грамм более высокого порядка (например, триграмм) через использование N-грамм меньших порядков (например, биграмм). Суть заключается в том, чтобы использовать частоты меньших N-грамм для оценки вероятностей более сложных N-грамм, что позволяет корректно обрабатывать редкие и невидимые случаи.\n",
        "\n",
        "Метод применяет **скидку** к частотам, чтобы уменьшить вероятность появления редких N-грамм, и перераспределяет эту вероятность на менее редкие события. Это сглаживание также помогает избежать **нулевых вероятностей** для N-грамм, которые не встречаются в обучающем корпусе.\n",
        "\n",
        "#### Формулы сглаживания Кнезера-Нея\n",
        "\n",
        "1. **Для биграмм и N-грамм, которые встречаются хотя бы один раз**, вероятность вычисляется как обычная частота с применением скидки.\n",
        "\n",
        "2. **Для редких или невидимых N-грамм** (таких, которые не встречались в обучающих данных) используется рекурсия: вероятность этой N-граммы оценивается с использованием частоты меньших N-грамм (например, биграмм).\n",
        "\n",
        "##### Формула для биграмм с сглаживанием Кнезера-Нея:\n",
        "\n",
        "$$\n",
        "P(w_i | w_{i-1}) = \\frac{\\max(C(w_{i-1}, w_i) - D, 0)}{C(w_{i-1})} + \\lambda(w_{i-1}) P_{\\text{continuation}}(w_i)\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $C(w_{i-1}, w_i)$ — частота биграммы $w_{i-1}, w_i$,\n",
        "- $C(w_{i-1})$ — частота слова $w_{i-1}$,\n",
        "- $D$ — параметр скидки (обычно $D = 0.75$),\n",
        "- $\\lambda(w_{i-1})$ — нормализующий коэффициент, чтобы сумма всех вероятностей для слова $w_{i-1}$ была равна 1. Он рассчитывается как:\n",
        "\n",
        "$$\n",
        "\\lambda(w_{i-1}) = \\frac{C(w_{i-1})}{C(w_{i-1}) - D \\sum_{w_i : C(w_{i-1}, w_i) > 0} 1}\n",
        "$$\n",
        "\n",
        "- $P_{\\text{continuation}}(w_i)$ — вероятность того, что слово $w_i$ будет продолжением биграммы, т.е., вероятность того, что $w_i$ когда-либо встречается в корпусе, независимо от того, с каким словом оно встречается.\n",
        "\n",
        "##### Формула для триграмм (и N-грамм более высокого порядка):\n",
        "\n",
        "Для триграмм (и N-грамм более высокого порядка) используется рекурсия, то есть вероятность триграммы $P(w_3 | w_1, w_2)$ вычисляется через биграмму $P(w_3 | w_2)$, а вероятность биграммы $P(w_2 | w_1)$ уже с применением скидки.\n",
        "\n",
        "$$\n",
        "P(w_3 | w_1, w_2) = \\frac{\\max(C(w_1, w_2, w_3) - D, 0)}{C(w_1, w_2)} + \\lambda(w_1, w_2) P(w_3 | w_2)\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $C(w_1, w_2, w_3)$ — частота триграммы $w_1, w_2, w_3$,\n",
        "- $C(w_1, w_2)$ — частота биграммы $w_1, w_2$,\n",
        "- $\\lambda(w_1, w_2)$ — нормализующий коэффициент для биграмм.\n",
        "\n",
        "##### Для редких или невидимых N-грамм (где $C(w_1, w_2, w_3) = 0$):\n",
        "\n",
        "Если триграмма не встречается в обучающих данных, то мы рассчитываем вероятность через биграмму $P(w_3 | w_2)$ с использованием сглаживания Кнезера-Нея:\n",
        "\n",
        "$$\n",
        "P(w_3 | w_2) = \\frac{\\max(C(w_2, w_3) - D, 0)}{C(w_2)} + \\lambda(w_2) P_{\\text{continuation}}(w_3)\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $C(w_2, w_3)$ — частота биграммы $w_2, w_3$,\n",
        "- $C(w_2)$ — частота слова $w_2$,\n",
        "- $\\lambda(w_2)$ — нормализующий коэффициент для биграмм.\n",
        "\n",
        "#### Пример работы с формулами\n",
        "\n",
        "Предположим, что мы работаем с триграммами, и нужно вычислить вероятность триграммы $P(w_3 | w_1, w_2)$. Мы знаем:\n",
        "\n",
        "- $C(w_1, w_2, w_3) = 2$ (триграмма $w_1, w_2, w_3$ встречалась 2 раза),\n",
        "- $C(w_1, w_2) = 5$ (биграмма $w_1, w_2$ встречалась 5 раз),\n",
        "- $C(w_1) = 10$ (слово $w_1$ встречается 10 раз),\n",
        "- Параметр скидки $D = 0.75$.\n",
        "\n",
        "Сначала вычислим вероятность триграммы:\n",
        "\n",
        "$$\n",
        "P(w_3 | w_1, w_2) = \\frac{\\max(2 - 0.75, 0)}{5} + \\lambda(w_1, w_2) P(w_3 | w_2)\n",
        "$$\n",
        "\n",
        "$$\n",
        "P(w_3 | w_1, w_2) = \\frac{1.25}{5} + \\lambda(w_1, w_2) P(w_3 | w_2)\n",
        "$$\n",
        "\n",
        "Далее, нужно вычислить $\\lambda(w_1, w_2)$ для нормализации:\n",
        "\n",
        "$$\n",
        "\\lambda(w_1, w_2) = \\frac{5}{5 - 0.75 \\times 3} = \\frac{5}{3.25} \\approx 1.538\n",
        "$$\n",
        "\n",
        "После этого вычисляем вероятность биграммы $P(w_3 | w_2)$ и подставляем её в формулу для триграммы.\n",
        "\n",
        "#### Почему сглаживание Кнезера-Нея лучше?\n",
        "\n",
        "1. **Эффективность для редких N-грамм**: Это сглаживание позволяет более точно оценивать вероятность редких и невидимых N-грамм, поскольку оно использует информацию о более частых N-граммах меньших порядков.\n",
        "\n",
        "2. **Рекурсивная природа**: Вместо того чтобы просто добавлять константу, как в Лапласовском сглаживании, Кнезер-Ней использует рекурсию и перераспределяет вероятность на более частые N-граммы, что делает модель более гибкой и точной.\n",
        "\n",
        "3. **Меньше информации для обработки**: В отличие от методов, которые требуют знания полной вероятности для каждой возможной N-граммы, Кнезер-Ней работает с более высокоуровневыми оценками, что упрощает работу с редкими событиями и позволяет моделям быть более обобщаемыми.\n",
        "\n",
        "### Пример на Python с применением сглаживания Кнезера-Нея\n",
        "\n"
      ],
      "metadata": {
        "id": "gVVxhfdGFdEF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "def kneser_ney_smoothing(trigram, bigram_counts, trigram_counts, word_counts, discount=0.75):\n",
        "    w1, w2, w3 = trigram\n",
        "    # Частоты триграмм и биграмм\n",
        "    C_w1_w2_w3 = trigram_counts.get((w1, w2, w3), 0)\n",
        "    C_w1_w2 = bigram_counts.get((w1, w2), 0)\n",
        "    C_w2 = word_counts.get(w2, 0)\n",
        "\n",
        "    if C_w1_w2_w3 > 0:\n",
        "        # Если триграмма встречается хотя бы один раз\n",
        "        return (C_w1_w2_w3 - discount) / C_w1_w2\n",
        "    else:\n",
        "        # Для невидимых триграмм (рекурсия через биграмму)\n",
        "        # Здесь предполагается, что использование более простых N-грамм дает хорошую оценку\n",
        "        continuation_prob = sum(1 for trigram in trigram_counts if trigram[1] == w2) / len(trigram_counts)\n",
        "        return discount * continuation_prob / C_w2\n",
        "\n",
        "# Пример данных\n",
        "word_counts = Counter({'кот': 5, 'ловит': 3, 'мышь': 2, 'смотрит': 2, 'на': 4})\n",
        "bigram_counts = Counter({('кот', 'ловит'): 3, ('ловит', 'мышь'): 2, ('мышь', 'смотрит'): 1})\n",
        "trigram_counts = Counter({('кот', 'ловит', 'мышь'): 2, ('ловит', 'мышь', 'смотрит'): 1})\n",
        "\n",
        "# Пример триграммы\n",
        "trigram = ('кот', 'ловит', 'мышь')\n",
        "prob = kneser_ney_smoothing(trigram, bigram_counts, trigram_counts, word_counts)\n",
        "print(f\"Вероятность триграммы {trigram}: {prob:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "beBryuLwFOSZ",
        "outputId": "7fe82a1b-3c32-43b5-df90-2d4c1c7c0a41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Вероятность триграммы ('кот', 'ловит', 'мышь'): 0.417\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### 3. **Сглаживание Гуд-Тьюринга**\n",
        "\n",
        "**Сглаживание Гуд-Тьюринга** — это еще один метод сглаживания, который используется для оценки редких событий, таких как невидимые N-граммы. Основная идея этого метода заключается в том, чтобы перераспределить вероятность редких событий, основываясь на частоте их появления в данных.\n",
        "\n",
        "#### Суть метода:\n",
        "- Оценка вероятности событий, которые не встречаются в тренировочных данных, производится с помощью вероятности событий, которые встречаются один раз.\n",
        "- Сглаживание Гуд-Тьюринга перераспределяет вероятность от часто встречающихся событий к менее встречающимся и невидимым.\n",
        "\n",
        "**Формула для сглаживания Гуд-Тьюринга** для биграмм:\n",
        "\n",
        "$$\n",
        "P_{\\text{GT}}(w_i | w_{i-1}) = \\frac{C(w_{i-1}, w_i) - 1}{C(w_{i-1})} \\cdot \\frac{C(w_{i-1}, w_i) + 1}{C(w_{i-1}) + 1}\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $C(w_{i-1}, w_i)$ — частота биграммы,\n",
        "- $C(w_{i-1})$ — частота первого слова в биграмме.\n",
        "\n",
        "Этот метод особенно полезен для слов, которые встречаются очень редко или вообще не встречаются в обучающих данных.\n",
        "\n",
        "#### Пример:\n",
        "Предположим, что для биграммы \"кошка ловит\" частота встречаемости равна 1, и частота слова \"кошка\" — 3. Тогда вероятность биграммы с применением сглаживания Гуд-Тьюринга будет вычисляться по формуле.\n",
        "\n",
        "### 4. **Сравнение методов**\n",
        "\n",
        "1. **Лапласовское сглаживание (аддитивное)**:\n",
        "   - Простота реализации.\n",
        "   - Добавляет фиксированную величину (обычно 1) к частоте, что может быть недостаточно точным для сложных моделей.\n",
        "\n",
        "2. **Сглаживание Кнезера-Нея**:\n",
        "   - Лучше, чем Лапласовское сглаживание, для более высоких N-грамм, так как оно использует частоты более низких N-грамм для более точной оценки вероятности.\n",
        "   - Потребует больше вычислений и может быть сложным для реализации на больших объемах данных.\n",
        "\n",
        "3. **Сглаживание Гуд-Тьюринга**:\n",
        "   - Применяется для оценки редких событий.\n",
        "   - Хорошо работает для ситуаций, где многие N-граммы не встречаются в обучающих данных.\n",
        "   - Сложно для реализации, но может значительно улучшить модель в случае редких данных.\n",
        "\n",
        "###  Реализация сглаживания в Python\n",
        "\n"
      ],
      "metadata": {
        "id": "7OPcIUM8FNBb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from typing import List, Tuple\n",
        "\n",
        "# Функция для разбиения текста на N-граммы\n",
        "def generate_ngrams(text: str, n: int) -> List[Tuple[str, ...]]:\n",
        "    words = text.lower().replace('.', '').split()  # Приведение текста к нижнему регистру, удаление точек и разбиение на слова\n",
        "    ngrams = [tuple(words[i:i + n]) for i in range(len(words) - n + 1)]\n",
        "    return ngrams\n",
        "\n",
        "# Лапласовское (аддитивное) сглаживание\n",
        "def laplace_smoothing(bigram: Tuple[str, str], word_counts: Counter, bigram_counts: Counter, alpha: int, V: int) -> float:\n",
        "    w1, w2 = bigram\n",
        "    C_w1_w2 = bigram_counts.get((w1, w2), 0)\n",
        "    C_w1 = word_counts[w1]\n",
        "    return (C_w1_w2 + alpha) / (C_w1 + alpha * V)\n",
        "\n",
        "# Сглаживание Кнезера-Нея\n",
        "def kneser_ney_smoothing(bigram: Tuple[str, str], word_counts: Counter, bigram_counts: Counter, discount: float, V: int) -> float:\n",
        "    w1, w2 = bigram\n",
        "    C_w1_w2 = bigram_counts.get((w1, w2), 0)\n",
        "    C_w1 = word_counts[w1]\n",
        "\n",
        "    # Если биграмма встречается более одного раза, используем стандартную вероятность\n",
        "    if C_w1_w2 > 0:\n",
        "        return (C_w1_w2 - discount) / C_w1\n",
        "    else:\n",
        "        # Если биграмма не встречается, используем сглаживание для \"не встреченных\" биграмм\n",
        "        C_w1_v = sum(1 for bigram in bigram_counts if bigram[0] == w1)\n",
        "        return discount * C_w1_v / C_w1\n",
        "\n",
        "# Сглаживание Гуд-Тьюринга\n",
        "def good_turing_smoothing(bigram: Tuple[str, str], word_counts: Counter, bigram_counts: Counter, V: int) -> float:\n",
        "    C_w1_w2 = bigram_counts.get(bigram, 0)\n",
        "    C_w1 = word_counts[bigram[0]]\n",
        "\n",
        "    # Если биграмма не встречалась, мы используем корректировку\n",
        "    if C_w1_w2 == 0:\n",
        "        # Для невидимых биграмм\n",
        "        N = sum(1 for count in bigram_counts.values() if count == 1)\n",
        "        return N / (C_w1 * V)\n",
        "    else:\n",
        "        # Для видимых биграмм\n",
        "        C_w1_w2_count = sum(1 for count in bigram_counts.values() if count == C_w1_w2)\n",
        "        return (C_w1_w2 + 1) * C_w1_w2_count / (C_w1 * len(bigram_counts))\n",
        "\n",
        "# Основная функция\n",
        "def calculate_probabilities(text: str, n: int, alpha: int = 1, discount: float = 0.75) -> None:\n",
        "    # Разбиваем текст на n-граммы\n",
        "    ngrams = generate_ngrams(text, n)\n",
        "\n",
        "    # Подсчитываем частоты слов и n-грамм\n",
        "    word_counts = Counter([word for word in text.lower().replace('.', '').split()])\n",
        "    ngram_counts = Counter(ngrams)\n",
        "\n",
        "    # Размер словаря\n",
        "    V = len(word_counts)\n",
        "\n",
        "    # Применение разных методов сглаживания\n",
        "    print(f\"\\nРасчёт вероятностей для {n}-грамм с разными методами сглаживания:\")\n",
        "    for ngram in ngram_counts:\n",
        "        # Лапласовское сглаживание\n",
        "        laplace_prob = laplace_smoothing(ngram, word_counts, ngram_counts, alpha, V)\n",
        "        print(f\"P({ngram[-1]} | {ngram[0]}) с Лапласовским сглаживанием: {laplace_prob:.3f}\")\n",
        "\n",
        "        # Сглаживание Кнезера-Нея\n",
        "        kneser_ney_prob = kneser_ney_smoothing(ngram, word_counts, ngram_counts, discount, V)\n",
        "        print(f\"P({ngram[-1]} | {ngram[0]}) с Кнезера-Нея: {kneser_ney_prob:.3f}\")\n",
        "\n",
        "        # Сглаживание Гуд-Тьюринга\n",
        "        good_turing_prob = good_turing_smoothing(ngram, word_counts, ngram_counts, V)\n",
        "        print(f\"P({ngram[-1]} | {ngram[0]}) с Гуд-Тьюрингом: {good_turing_prob:.3f}\")\n",
        "\n",
        "# Пример текста\n",
        "text = \"Кошка ловит мышь. Кошка играет с мышью. Кошка мяукает.\"\n",
        "\n",
        "# Применяем функцию для биграмм\n",
        "calculate_probabilities(text, 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z27E7rgcEX53",
        "outputId": "b764267b-9ff2-4fc1-b947-2ecd0cf239ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Расчёт вероятностей для 2-грамм с разными методами сглаживания:\n",
            "P(ловит | кошка) с Лапласовским сглаживанием: 0.200\n",
            "P(ловит | кошка) с Кнезера-Нея: 0.083\n",
            "P(ловит | кошка) с Гуд-Тьюрингом: 0.667\n",
            "P(мышь | ловит) с Лапласовским сглаживанием: 0.250\n",
            "P(мышь | ловит) с Кнезера-Нея: 0.250\n",
            "P(мышь | ловит) с Гуд-Тьюрингом: 2.000\n",
            "P(кошка | мышь) с Лапласовским сглаживанием: 0.250\n",
            "P(кошка | мышь) с Кнезера-Нея: 0.250\n",
            "P(кошка | мышь) с Гуд-Тьюрингом: 2.000\n",
            "P(играет | кошка) с Лапласовским сглаживанием: 0.200\n",
            "P(играет | кошка) с Кнезера-Нея: 0.083\n",
            "P(играет | кошка) с Гуд-Тьюрингом: 0.667\n",
            "P(с | играет) с Лапласовским сглаживанием: 0.250\n",
            "P(с | играет) с Кнезера-Нея: 0.250\n",
            "P(с | играет) с Гуд-Тьюрингом: 2.000\n",
            "P(мышью | с) с Лапласовским сглаживанием: 0.250\n",
            "P(мышью | с) с Кнезера-Нея: 0.250\n",
            "P(мышью | с) с Гуд-Тьюрингом: 2.000\n",
            "P(кошка | мышью) с Лапласовским сглаживанием: 0.250\n",
            "P(кошка | мышью) с Кнезера-Нея: 0.250\n",
            "P(кошка | мышью) с Гуд-Тьюрингом: 2.000\n",
            "P(мяукает | кошка) с Лапласовским сглаживанием: 0.200\n",
            "P(мяукает | кошка) с Кнезера-Нея: 0.083\n",
            "P(мяукает | кошка) с Гуд-Тьюрингом: 0.667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Пример кода для расчета вероятностей биграмм в Python"
      ],
      "metadata": {
        "id": "iO61Jw92FwcQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_H2FJ8QBy-o",
        "outputId": "14827680-b5e9-4ad4-ec2a-96ad2a27ea7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Частоты слов:\n",
            "кошка: 3\n",
            "ловит: 1\n",
            "мышь: 1\n",
            "играет: 1\n",
            "с: 1\n",
            "мышью: 1\n",
            "мяукает: 1\n",
            "\n",
            "Частоты биграмм:\n",
            "('кошка', 'ловит'): 1\n",
            "('ловит', 'мышь'): 1\n",
            "('мышь', 'кошка'): 1\n",
            "('кошка', 'играет'): 1\n",
            "('играет', 'с'): 1\n",
            "('с', 'мышью'): 1\n",
            "('мышью', 'кошка'): 1\n",
            "('кошка', 'мяукает'): 1\n",
            "\n",
            "Вероятности биграмм:\n",
            "P(ловит | кошка) = 0.33\n",
            "P(мышь | ловит) = 1.00\n",
            "P(кошка | мышь) = 1.00\n",
            "P(играет | кошка) = 0.33\n",
            "P(с | играет) = 1.00\n",
            "P(мышью | с) = 1.00\n",
            "P(кошка | мышью) = 1.00\n",
            "P(мяукает | кошка) = 0.33\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Шаг 1: Заданный текст и его обработка\n",
        "text = \"Кошка ловит мышь. Кошка играет с мышью. Кошка мяукает.\"\n",
        "words = text.lower().replace('.', '').split()  # Приведение к нижнему регистру, удаление точек и разбиение на слова\n",
        "\n",
        "# Шаг 2: Генерация списка биграмм\n",
        "bigrams = [(words[i], words[i + 1]) for i in range(len(words) - 1)]\n",
        "\n",
        "# Шаг 3: Подсчет частоты встречаемости слов и биграмм\n",
        "word_counts = Counter(words)\n",
        "bigram_counts = Counter(bigrams)\n",
        "\n",
        "# Шаг 4: Вычисление вероятностей для биграмм\n",
        "bigram_probabilities = {}\n",
        "for (w1, w2), bigram_count in bigram_counts.items():\n",
        "    # Вероятность P(w2 | w1) = C(w1, w2) / C(w1)\n",
        "    bigram_probabilities[(w1, w2)] = bigram_count / word_counts[w1]\n",
        "\n",
        "# Вывод результатов\n",
        "print(\"Частоты слов:\")\n",
        "for word, count in word_counts.items():\n",
        "    print(f\"{word}: {count}\")\n",
        "\n",
        "print(\"\\nЧастоты биграмм:\")\n",
        "for bigram, count in bigram_counts.items():\n",
        "    print(f\"{bigram}: {count}\")\n",
        "\n",
        "print(\"\\nВероятности биграмм:\")\n",
        "for bigram, prob in bigram_probabilities.items():\n",
        "    print(f\"P({bigram[1]} | {bigram[0]}) = {prob:.2f}\")"
      ]
    }
  ]
}