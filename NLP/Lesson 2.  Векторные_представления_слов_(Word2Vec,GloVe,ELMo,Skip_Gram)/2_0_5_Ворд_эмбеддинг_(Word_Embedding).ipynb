{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNdjO37y2iAViA1FzHv8Nmk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CodeHunterOfficial/AI_DataMining/blob/main/NLP/Lesson%202.%20%20%D0%92%D0%B5%D0%BA%D1%82%D0%BE%D1%80%D0%BD%D1%8B%D0%B5_%D0%BF%D1%80%D0%B5%D0%B4%D1%81%D1%82%D0%B0%D0%B2%D0%BB%D0%B5%D0%BD%D0%B8%D1%8F_%D1%81%D0%BB%D0%BE%D0%B2_(Word2Vec%2CGloVe%2CELMo%2CSkip_Gram)/2_0_5_%D0%92%D0%BE%D1%80%D0%B4_%D1%8D%D0%BC%D0%B1%D0%B5%D0%B4%D0%B4%D0%B8%D0%BD%D0%B3_(Word_Embedding).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Ворд эмбеддинг (Word Embedding)\n",
        "\n",
        "## Введение\n",
        "\n",
        "Ворд эмбеддинг (word embedding) — это метод представления слов в виде плотных векторных представлений в многомерном пространстве, где каждое слово отображается как вектор фиксированной длины. Эти вектора кодируют семантическую информацию о словах и позволяют эффективно проводить операции на тексте, такие как вычисление схожести между словами, кластеризация, а также обучение нейросетей для различных задач обработки естественного языка (NLP).\n",
        "\n",
        "Существуют различные методы для получения таких представлений, включая подходы на основе статистики (например, модель **TF-IDF**) и нейронные сети (например, **Word2Vec**, **GloVe** и **FastText**). Однако все эти методы следуют одной ключевой идее: представление каждого слова в виде вектора в многомерном пространстве, где схожие слова находятся близко друг к другу.\n",
        "\n",
        "В этой лекции мы рассмотрим основные теории и методы создания word embedding, а также математическое обоснование подходов.\n",
        "\n",
        "## 1. Почему нам нужны ворд эмбеддинги?\n",
        "\n",
        "Прежде чем углубиться в теорию, давайте разберемся, зачем вообще нужны ворд эмбеддинги. В традиционном подходе к обработке текста, слова обычно представляются как дискретные символы или последовательности символов. Однако такие представления имеют несколько ограничений:\n",
        "\n",
        "- **Векторное представление слов** не сохраняет семантическую близость между словами. Например, для слов «король» и «королева» в простых представлениях они будут находиться в разных точках пространства, несмотря на то что они семантически близки.\n",
        "- **Размерность пространства**: если каждое слово кодируется одним числом, как в случае с \"мешком слов\" (bag-of-words), это приводит к огромной размерности признакового пространства, что делает обучение модели крайне трудным и неэффективным.\n",
        "\n",
        "Ворд эмбеддинг позволяет решать эти проблемы, представляя слова в компактных, но при этом информативных многомерных векторах.\n",
        "\n",
        "## 2. Как работают ворд эмбеддинги?\n",
        "\n",
        "Процесс обучения ворд эмбеддингов заключается в том, чтобы преобразовать слова в векторы, которые хорошо отображают их контекст. Это предполагает, что слова, которые часто встречаются в одном и том же контексте, будут иметь схожие векторные представления.\n",
        "\n",
        "**Основные подходы:**\n",
        "- **Модели на основе контекста**: Здесь каждый вектор представляет не само слово, а его контекст. Примером таких моделей является **Word2Vec** и **GloVe**.\n",
        "- **Модели на основе трансформеров**: Современные архитектуры, такие как **BERT** и **GPT**, генерируют контекстуализированные эмбеддинги для слов в предложении, т.е. один и тот же токен может иметь разные представления в зависимости от контекста.\n",
        "\n",
        "## 3. Модель Word2Vec\n",
        "\n",
        "Один из самых популярных методов для создания word embedding — это **Word2Vec**, который был предложен командой Google в 2013 году. Word2Vec использует нейронную сеть для обучения векторных представлений слов. Основная идея Word2Vec заключается в том, что похожие слова появляются в схожем контексте.\n",
        "\n",
        "## 4. Модель GloVe (Global Vectors for Word Representation)\n",
        "\n",
        "**GloVe** — это еще один популярный алгоритм для обучения эмбеддингов, который отличается от Word2Vec тем, что в основе GloVe лежит статистический подход, а не нейронные сети.\n",
        "\n",
        "GloVe пытается моделировать взаимосвязи между словами на основе **совместных вероятностей**. Идея заключается в том, чтобы сохранить информацию о частотах появления пар слов и использовать эти данные для построения векторных представлений.\n",
        "\n",
        "## 5. Применение ворд эмбеддингов\n",
        "\n",
        "Ворд эмбеддинги нашли широкое применение в различных задачах обработки естественного языка, включая:\n",
        "- **Поиск по тексту**: использование векторных представлений для поиска схожих документов или ответов на запросы.\n",
        "- **Семантический анализ**: оценка схожести между словами, предложениями или документами.\n",
        "- **Тематическое моделирование**: использование эмбеддингов для кластеризации текстов по темам.\n",
        "- **Перевод текста**: использование векторных представлений для построения моделей машинного перевода.\n",
        "\n",
        "## Заключение\n",
        "\n",
        "Ворд эмбеддинги являются важной составляющей современных методов обработки естественного языка. Они предоставляют компактные и информативные представления слов, которые позволяют эффективно решать задачи, такие как классификация текста, анализ тональности и машинный перевод. Модели, такие как Word2Vec и GloVe, заложили основу для дальнейшего развития технологий в этой области, включая трансформеры, такие как BERT и GPT, которые предлагают еще более сложные и мощные методы для создания контекстуализированных эмбеддингов.\n",
        "\n"
      ],
      "metadata": {
        "id": "x1JUcD3CGLg8"
      }
    }
  ]
}