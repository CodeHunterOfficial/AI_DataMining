{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPWuXbTMjLpVhy9BjUD72ph",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CodeHunterOfficial/AI_DataMining/blob/main/NLP/Lesson%202.%20%20%D0%92%D0%B5%D0%BA%D1%82%D0%BE%D1%80%D0%BD%D1%8B%D0%B5_%D0%BF%D1%80%D0%B5%D0%B4%D1%81%D1%82%D0%B0%D0%B2%D0%BB%D0%B5%D0%BD%D0%B8%D1%8F_%D1%81%D0%BB%D0%BE%D0%B2_(Word2Vec%2CGloVe%2CELMo%2CSkip_Gram)/2_0_2_Byte_Pair_Encoding_(BPE).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Byte Pair Encoding (BPE)\n",
        "\n",
        "Byte Pair Encoding (BPE) – это алгоритм сжатия данных, изначально предложенный Гейвом Сазерлендом и Филипом Гейджем в 1994 году, который также используется в обработке естественного языка для построения эффективных токенизаций и улучшения работы языковых моделей. В контексте языковых моделей и машинного обучения он применяется для токенизации, то есть для разбиения текста на более короткие \"токены\", которые облегчают моделям обработку текстовых данных.\n",
        "\n",
        "### 1. Основные принципы Byte Pair Encoding\n",
        "\n",
        "BPE использует простую, но эффективную идею, основанную на статистическом анализе частоты сочетаний символов. Весь процесс заключается в следующем:\n",
        "\n",
        "1. **Частотный анализ пар символов**. Для заданного текста (или корпуса текстов) BPE определяет наиболее часто встречающиеся пары символов (или символов и уже известных сочетаний).\n",
        "2. **Итеративная замена**. Эти наиболее частотные пары заменяются на новый \"объединенный\" символ или последовательность символов, которая добавляется в словарь токенов. Это сокращает длину текста, так как пары объединяются в один токен.\n",
        "3. **Повторение процесса**. Алгоритм повторяет эти шаги, пока не будет достигнут заданный лимит длины словаря или количества итераций. В результате часто встречающиеся сочетания символов заменяются одним токеном, что позволяет эффективно представлять слова с общими корнями и префиксами/суффиксами.\n",
        "\n",
        "### 2. Применение BPE в обработке естественного языка\n",
        "\n",
        "В NLP (Natural Language Processing – обработка естественного языка) BPE помогает разбивать слова на токены, что особенно полезно для моделей, работающих с огромным количеством данных. Это облегчает задачу моделям, поскольку:\n",
        "\n",
        "- **Уменьшает количество неизвестных слов**. В отличие от простого разбиения по словам (word-level токенизация), BPE разбивает слова на токены, которые могут представлять не только полные слова, но и части слов (например, корни, суффиксы и т.д.). Это делает модель способной понимать новые слова, комбинируя известные ей токены.\n",
        "- **Меньший словарь**. BPE позволяет сгенерировать ограниченный и более компактный словарь, в котором каждая часть слова (например, \"про-\", \"-екта\" и т.д.) представлена своим токеном.\n",
        "- **Более высокая устойчивость к морфологии и агглютинации**. В языках с богатой морфологией или со сложной структурой слова, таких как немецкий, финский или турецкий, слова могут быть очень длинными и сложными. BPE помогает справиться с этим за счет разбиения таких слов на более короткие и повторяющиеся элементы.\n",
        "\n",
        "### 3. Пошаговый процесс применения Byte Pair Encoding\n",
        "\n",
        "Для лучшего понимания рассмотрим, как работает BPE на практике с простым примером.\n",
        "\n",
        "#### Пример использования BPE на наборе символов\n",
        "\n",
        "Допустим, у нас есть текст: `aaabdaaabac`\n",
        "\n",
        "1. **Инициализация словаря**. Разбиваем текст на начальные символы: `a a a b d a a a b a c`.\n",
        "2. **Подсчет частот пар символов**:\n",
        "    - Найдём частоту для каждой пары:\n",
        "      - `aa` — встречается 3 раза\n",
        "      - `ab` — встречается 2 раза\n",
        "      - `ba` — встречается 1 раз\n",
        "      - `da` — встречается 1 раз\n",
        "      - `ac` — встречается 1 раз\n",
        "3. **Выбор наиболее частотной пары**: самая частотная пара — `aa`, и мы её заменяем на новый токен, скажем `X`.\n",
        "    - Новая последовательность: `X b d X b a c`.\n",
        "4. **Повторение процесса**:\n",
        "   - Теперь снова подсчитываем пары:\n",
        "     - `Xb` — 2 раза\n",
        "     - `bd` — 1 раз\n",
        "     - `ba` — 1 раз\n",
        "     - `ac` — 1 раз\n",
        "   - Выбираем `Xb`, так как это самая частая пара, и заменяем её на новый символ, например `Y`.\n",
        "     - Теперь последовательность: `Y d Y a c`.\n",
        "5. **Продолжение итераций** до достижения заданного числа токенов или пока не останется часто встречающихся пар.\n",
        "\n",
        "После нескольких итераций мы сократили исходный текст до меньшего количества токенов.\n",
        "\n",
        "#### Пример применения BPE для NLP\n",
        "\n",
        "Рассмотрим текст \"программирование\", \"программист\", \"программно\", \"программа\". Сначала BPE разделит слова на отдельные буквы, затем будет постепенно объединять наиболее частые пары, такие как `пр`, `о`, `г`, и так далее. После нескольких итераций результат может быть следующим:\n",
        "\n",
        "- `программирование` → `програм` + `мир` + `ование`\n",
        "- `программист` → `програм` + `мир` + `ст`\n",
        "- `программа` → `програм` + `ма`\n",
        "\n",
        "Здесь корень `програм` будет представлен как один токен, так как он встречается во всех словах.\n",
        "\n",
        "### 4. Преимущества и недостатки BPE\n",
        "\n",
        "#### Преимущества\n",
        "\n",
        "1. **Компактный и универсальный словарь**. Модели могут оперировать меньшим количеством токенов, что снижает требования к памяти и увеличивает скорость работы.\n",
        "2. **Обработка неизвестных слов**. В случае слов, которые не были в обучающем корпусе, модель может распознать части слова, что увеличивает её общее понимание текста.\n",
        "3. **Эффективная работа с морфологией**. BPE особенно полезен для языков с богатой морфологией, где слова часто формируются через агглютинацию.\n",
        "\n",
        "#### Недостатки\n",
        "\n",
        "1. **Неоптимальность для некоторых языков**. BPE может не работать оптимально для языков, где существуют тонкие лингвистические нюансы. Например, иероглифические языки (китайский) или некоторые африканские языки могут требовать другой токенизации.\n",
        "2. **Ограничения по размеру словаря**. Алгоритм требует настройки параметра, определяющего, сколько токенов будет создано. Если задать его слишком маленьким, модель потеряет детали. Если слишком большим, это увеличит сложность модели.\n",
        "\n",
        "### 5. Современные приложения и модификации BPE\n",
        "\n",
        "Сегодня BPE используется во многих крупных моделях, таких как GPT, BERT, RoBERTa и других, а также в библиотеках, таких как Hugging Face, которые предоставляют инструменты для быстрой и удобной токенизации. BPE был также модифицирован для создания Unigram модели токенизации и SentencePiece – библиотеки, используемой в NLP для создания токенов без пробелов.\n",
        "\n",
        "Таким образом, Byte Pair Encoding – это простой, но мощный метод, который оказал значительное влияние на обработку естественного языка. Этот алгоритм преобразил подход к токенизации текста, улучшив работу моделей на многих языках и в различных контекстах.\n"
      ],
      "metadata": {
        "id": "L_TO3ySN2UsZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ysDy_uEQ1MCb",
        "outputId": "f321fd4d-8c9c-4b09-e63c-dfc959e994e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Начальный словарь: {'l o w </w>': 1, 'l o w e r </w>': 1, 'n e w e s t </w>': 1, 'w i d e s t </w>': 1}\n",
            "Итерация 1, объединяем пару: ('l', 'o')\n",
            "Обновленный словарь: {'lo w </w>': 1, 'lo w e r </w>': 1, 'n e w e s t </w>': 1, 'w i d e s t </w>': 1}\n",
            "Итерация 2, объединяем пару: ('lo', 'w')\n",
            "Обновленный словарь: {'low </w>': 1, 'low e r </w>': 1, 'n e w e s t </w>': 1, 'w i d e s t </w>': 1}\n",
            "Итерация 3, объединяем пару: ('e', 's')\n",
            "Обновленный словарь: {'low </w>': 1, 'low e r </w>': 1, 'n e w es t </w>': 1, 'w i d es t </w>': 1}\n",
            "Итерация 4, объединяем пару: ('es', 't')\n",
            "Обновленный словарь: {'low </w>': 1, 'low e r </w>': 1, 'n e w est </w>': 1, 'w i d est </w>': 1}\n",
            "Итерация 5, объединяем пару: ('est', '</w>')\n",
            "Обновленный словарь: {'low </w>': 1, 'low e r </w>': 1, 'n e w est</w>': 1, 'w i d est</w>': 1}\n",
            "Итерация 6, объединяем пару: ('low', '</w>')\n",
            "Обновленный словарь: {'low</w>': 1, 'low e r </w>': 1, 'n e w est</w>': 1, 'w i d est</w>': 1}\n",
            "Итерация 7, объединяем пару: ('low', 'e')\n",
            "Обновленный словарь: {'low</w>': 1, 'lowe r </w>': 1, 'n e w est</w>': 1, 'w i d est</w>': 1}\n",
            "Итерация 8, объединяем пару: ('lowe', 'r')\n",
            "Обновленный словарь: {'low</w>': 1, 'lower </w>': 1, 'n e w est</w>': 1, 'w i d est</w>': 1}\n",
            "Итерация 9, объединяем пару: ('lower', '</w>')\n",
            "Обновленный словарь: {'low</w>': 1, 'lower</w>': 1, 'n e w est</w>': 1, 'w i d est</w>': 1}\n",
            "Итерация 10, объединяем пару: ('n', 'e')\n",
            "Обновленный словарь: {'low</w>': 1, 'lower</w>': 1, 'ne w est</w>': 1, 'w i d est</w>': 1}\n",
            "\n",
            "Итоговый словарь BPE:\n",
            "{'low</w>': 1, 'lower</w>': 1, 'ne w est</w>': 1, 'w i d est</w>': 1}\n"
          ]
        }
      ],
      "source": [
        "import collections\n",
        "\n",
        "# Функция для инициализации и преобразования слов в символные списки\n",
        "def init_vocab(words):\n",
        "    vocab = {}\n",
        "    for word in words:\n",
        "        # Преобразуем слово в формат, разделяя символы пробелами для удобства\n",
        "        tokens = \" \".join(list(word)) + \" </w>\"\n",
        "        vocab[tokens] = vocab.get(tokens, 0) + 1\n",
        "    return vocab\n",
        "\n",
        "# Функция для нахождения самых частых пар символов\n",
        "def get_stats(vocab):\n",
        "    pairs = collections.defaultdict(int)\n",
        "    for word, freq in vocab.items():\n",
        "        symbols = word.split()\n",
        "        for i in range(len(symbols) - 1):\n",
        "            pairs[(symbols[i], symbols[i + 1])] += freq\n",
        "    return pairs\n",
        "\n",
        "# Функция для объединения самой частой пары символов\n",
        "def merge_vocab(pair, vocab):\n",
        "    bigram = ' '.join(pair)\n",
        "    new_vocab = {}\n",
        "    for word in vocab:\n",
        "        new_word = word.replace(bigram, ''.join(pair))\n",
        "        new_vocab[new_word] = vocab[word]\n",
        "    return new_vocab\n",
        "\n",
        "# Основной алгоритм BPE\n",
        "def byte_pair_encoding(words, num_merges):\n",
        "    vocab = init_vocab(words)\n",
        "    print(\"Начальный словарь:\", vocab)\n",
        "\n",
        "    for i in range(num_merges):\n",
        "        pairs = get_stats(vocab)\n",
        "        if not pairs:\n",
        "            break\n",
        "        # Находим самую частую пару символов\n",
        "        best_pair = max(pairs, key=pairs.get)\n",
        "        print(f\"Итерация {i + 1}, объединяем пару: {best_pair}\")\n",
        "\n",
        "        # Объединяем эту пару в словаре\n",
        "        vocab = merge_vocab(best_pair, vocab)\n",
        "        print(\"Обновленный словарь:\", vocab)\n",
        "\n",
        "    return vocab\n",
        "\n",
        "# Тестируем BPE на заданных словах\n",
        "words = ['low', 'lower', 'newest', 'widest']\n",
        "num_merges = 10\n",
        "final_vocab = byte_pair_encoding(words, num_merges)\n",
        "\n",
        "print(\"\\nИтоговый словарь BPE:\")\n",
        "print(final_vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для Byte Pair Encoding (BPE) в Python существуют готовые библиотеки, которые значительно упрощают работу с этим алгоритмом, особенно в задачах обработки естественного языка. Одними из самых популярных библиотек, предоставляющих реализацию BPE, являются **`tokenizers`** от Hugging Face и **`SentencePiece`** от Google. Давайте рассмотрим, как использовать каждую из них для применения BPE.\n",
        "\n",
        "### 1. Использование библиотеки `tokenizers` от Hugging Face\n",
        "\n",
        "Библиотека `tokenizers` от Hugging Face поддерживает различные типы токенизации, включая BPE. Она предоставляет гибкий и быстрый интерфейс для работы с текстовыми данными.\n",
        "\n",
        "#### Установка библиотеки `tokenizers`\n",
        "\n"
      ],
      "metadata": {
        "id": "NNNF-6Lr3MGz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tokenizers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6qYW3hT3XrW",
        "outputId": "51067e53-a4af-42e8-ea84-746c61fbb4b5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (0.20.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers) (0.26.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Пример использования BPE в `tokenizers`\n",
        "\n"
      ],
      "metadata": {
        "id": "mDL4GKdB3cwl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import ByteLevelBPETokenizer\n",
        "\n",
        "# Создание и обучение BPE-токенизатора\n",
        "tokenizer = ByteLevelBPETokenizer()\n",
        "\n",
        "# Допустим, у нас есть небольшой корпус текста\n",
        "corpus = [\"low\", \"lower\", \"newest\", \"widest\"]\n",
        "\n",
        "# Сохранение корпуса в текстовый файл\n",
        "with open(\"corpus.txt\", \"w\") as f:\n",
        "    for line in corpus:\n",
        "        f.write(f\"{line}\\n\")\n",
        "\n",
        "# Обучение токенизатора на этом корпусе\n",
        "tokenizer.train(files=[\"corpus.txt\"], vocab_size=50, min_frequency=1, special_tokens=[\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\"])\n",
        "\n",
        "# Токенизация примера текста\n",
        "encoded = tokenizer.encode(\"lower\")\n",
        "print(\"Токены:\", encoded.tokens)  # Выводит список токенов\n",
        "print(\"Идентификаторы:\", encoded.ids)  # Выводит числовые идентификаторы токенов"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZvMZjge3heU",
        "outputId": "b307fc86-a6f2-4163-8e5e-b5e869762de2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Токены: ['l', 'o', 'w', 'e', 'r']\n",
            "Идентификаторы: [80, 83, 91, 73, 86]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "#### Объяснение параметров\n",
        "\n",
        "- `vocab_size` — размер словаря токенов.\n",
        "- `min_frequency` — минимальная частота, при которой пары символов будут добавлены в словарь.\n",
        "- `special_tokens` — специальные токены, которые будут использоваться в моделях.\n",
        "\n",
        "#### Вывод результата\n",
        "\n",
        "При токенизации слова `\"lower\"`, вы увидите токены, на которые оно было разбито, и их идентификаторы.\n",
        "\n",
        "### 2. Использование `SentencePiece`\n",
        "\n",
        "`SentencePiece` — это библиотека, разработанная Google для токенизации текста, которая также поддерживает BPE, а также Unigram-модели. Она широко используется в проектах, связанных с трансформерами и моделями машинного перевода.\n",
        "\n",
        "#### Установка `SentencePiece`\n"
      ],
      "metadata": {
        "id": "Z-FwNI1m3mNp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Uh45Ie_37WA",
        "outputId": "04e15276-c50e-4124-dcad-82798d1f8c05"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Пример использования BPE в SentencePiece\n"
      ],
      "metadata": {
        "id": "c0dayETG3-XC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "# Подготовка корпуса для обучения\n",
        "corpus = [\"low\", \"lower\", \"newest\", \"widest\"]\n",
        "with open(\"corpus.txt\", \"w\") as f:\n",
        "    for line in corpus:\n",
        "        f.write(f\"{line}\\n\")\n",
        "\n",
        "# Обучение модели SentencePiece с использованием BPE\n",
        "spm.SentencePieceTrainer.train(input='corpus.txt', model_prefix='bpe_model', vocab_size=30, model_type='bpe')\n",
        "\n",
        "# Загрузка обученной модели\n",
        "sp = spm.SentencePieceProcessor(model_file='bpe_model.model')\n",
        "\n",
        "# Токенизация текста\n",
        "encoded = sp.encode(\"lower\", out_type=str)\n",
        "print(\"Токены:\", encoded)\n",
        "\n",
        "# Если необходимо получить числовые идентификаторы токенов\n",
        "encoded_ids = sp.encode(\"lower\", out_type=int)\n",
        "print(\"Идентификаторы токенов:\", encoded_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4InmKCaE4EYc",
        "outputId": "a3ab7a35-b95a-4ee9-98c5-a7ee31f7cedd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Токены: ['▁lower']\n",
            "Идентификаторы токенов: [15]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "#### Объяснение параметров\n",
        "\n",
        "- `model_prefix` — префикс для выходных файлов модели (создаст `bpe_model.model` и `bpe_model.vocab`).\n",
        "- `vocab_size` — размер словаря модели.\n",
        "- `model_type='bpe'` — выбор BPE как метода токенизации.\n",
        "\n",
        "#### Результат\n",
        "\n",
        "В результате мы получаем список токенов и идентификаторов для слова `\"lower\"`. В зависимости от обучающего корпуса и параметров, токены могут включать как отдельные символы, так и более длинные объединения.\n",
        "\n",
        "Таким образом, использование `tokenizers` или `SentencePiece` позволяет легко и быстро применять BPE-токенизацию без необходимости реализовывать алгоритм с нуля. Эти библиотеки оптимизированы для работы с большими объемами данных и позволяют гибко управлять параметрами токенизации."
      ],
      "metadata": {
        "id": "PPC95-UO4I2q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Этот результат означает, что модель создала один токен `▁lower` для слова `\"lower\"`, и этот токен имеет числовой идентификатор `15`.\n",
        "\n",
        "В SentencePiece символ `▁` (подчеркивание) указывает на начало нового слова. Поскольку у нас маленький корпус, алгоритм решил закодировать слово `\"lower\"` как единый токен, а не разбирать его на более мелкие части. Это может происходить, если:\n",
        "\n",
        "1. **Маленький корпус**: В небольшом корпусе данных не так много разнообразия в комбинациях символов. Поэтому алгоритм обучается на небольшом количестве примеров и предпочитает более длинные токены, охватывающие целые слова.\n",
        "\n",
        "2. **Размер словаря**: Хотя мы уменьшили `vocab_size`, он все равно может быть немного велик для этого корпуса. При обучении на более крупных данных (с множеством слов и разнообразными корнями) BPE обычно разбивает слова на более короткие части.\n",
        "\n",
        "### Как сделать токены короче?\n",
        "\n",
        "Чтобы увидеть разбиение на более мелкие части, можно либо добавить больше разнообразных слов в корпус, либо уменьшить `vocab_size` до меньшего значения, например `10`.\n",
        "\n",
        "Пример с уменьшенным `vocab_size`:\n",
        "\n",
        "```python\n",
        "spm.SentencePieceTrainer.train(input='corpus.txt', model_prefix='bpe_model', vocab_size=10, model_type='bpe')\n",
        "```\n",
        "\n",
        "После этого, вероятно, слова начнут разбиваться на более короткие токены. Например, `\"lower\"` может быть закодировано как `['▁lo', 'wer']` или что-то подобное."
      ],
      "metadata": {
        "id": "HG_jU-AO5rou"
      }
    }
  ]
}