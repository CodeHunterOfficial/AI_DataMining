{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNriupA+c6QxTkkfwBsM2uO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CodeHunterOfficial/AI_DataMining/blob/main/NLP/Lesson%202.%20%20%D0%92%D0%B5%D0%BA%D1%82%D0%BE%D1%80%D0%BD%D1%8B%D0%B5_%D0%BF%D1%80%D0%B5%D0%B4%D1%81%D1%82%D0%B0%D0%B2%D0%BB%D0%B5%D0%BD%D0%B8%D1%8F_%D1%81%D0%BB%D0%BE%D0%B2_(Word2Vec%2CGloVe%2CELMo%2CSkip_Gram)/2_0_1_%D0%A1%D0%BE%D1%81%D1%82%D0%B0%D0%B2%D0%BB%D0%B5%D0%BD%D0%B8%D0%B5_%D1%81%D0%BB%D0%BE%D0%B2%D0%B0%D1%80%D1%8F_%D0%B8_%D0%A2%D0%BE%D0%BA%D0%B5%D0%BD%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F_%D1%81%D0%BB%D0%BE%D0%B2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Составление словаря и Токенизация слов\n",
        "\n",
        "**Цель лекции:** Познакомить с основами обработки текста, объяснить, что такое токенизация и составление словаря, почему это важно, и какие методы используются для их реализации. Эти знания являются фундаментальными в обработке текстов и применяются при решении задач компьютерной лингвистики, машинного обучения и анализа данных.\n",
        "\n",
        "### Часть 1: Составление словаря\n",
        "\n",
        "**Составление словаря** — это процесс создания набора уникальных слов или токенов, которые присутствуют в тексте. Это важный этап в обработке естественного языка (NLP), так как созданный словарь позволяет системе лучше ориентироваться в лексике текста и помогает в его дальнейшей обработке.\n",
        "\n",
        "#### 1.1 Что такое словарь?\n",
        "\n",
        "В контексте обработки текста словарь — это коллекция всех уникальных слов, встречающихся в одном или нескольких текстах. В зависимости от задач словарь может быть разного объема и содержать различные формы слов, например:\n",
        "\n",
        "- **Простой словарь:** Список уникальных слов из текста.\n",
        "- **Расширенный словарь:** Может включать и другие формы слов, такие как различные падежи, роды, числа для одного слова.\n",
        "- **Контекстуальный словарь:** Учитывает зависимости слов, их контексты и частоту встречаемости.\n",
        "\n",
        "#### 1.2 Зачем нужен словарь?\n",
        "\n",
        "- **Упрощение обработки текста:** Вместо обработки полного текста, модели опираются на заранее созданный словарь.\n",
        "- **Уменьшение объема данных:** Уникальные слова позволяют сократить избыточные данные, исключив повторяющиеся элементы.\n",
        "- **Формирование признаков для модели:** Словарь позволяет составлять признаки для машинного обучения, например, через векторизацию слов.\n",
        "- **Фильтрация лишней информации:** На этапе составления словаря можно исключить стоп-слова или незначительные токены, уменьшая \"шум\" данных.\n",
        "\n",
        "#### 1.3 Основные этапы составления словаря\n",
        "\n",
        "1. **Сбор текстовых данных:** На этом этапе собирается корпус текста, который будет анализироваться. Это могут быть данные из новостных статей, книги, форумы, твиты и т.д.\n",
        "  \n",
        "2. **Предобработка текста:** Включает в себя несколько шагов:\n",
        "   - Приведение к единому регистру (например, все слова в нижний регистр).\n",
        "   - Удаление пунктуации и специальных символов.\n",
        "   - Удаление стоп-слов — часто встречающихся и малоинформативных слов (например, \"и\", \"но\", \"что\").\n",
        "   - Лемматизация или стемминг — приведение слов к их базовой форме (например, \"бегать\" вместо \"бегал\", \"бежит\").\n",
        "\n",
        "3. **Создание словаря:** После предобработки данных составляется список уникальных слов. Этот список часто структурируется в виде словаря с частотностью — сколько раз каждое слово встречалось в корпусе текста.\n",
        "\n",
        "4. **Формирование индексации:** Каждому уникальному слову присваивается индекс — уникальный номер, который позволяет легко идентифицировать его в будущем. Это особенно важно для моделей машинного обучения.\n",
        "\n",
        "#### 1.4 Частотный словарь и методы его создания\n",
        "\n",
        "Частотный словарь — это словарь, где к каждому уникальному слову добавлено количество его упоминаний в тексте.\n",
        "\n",
        "- **Мешок слов (Bag-of-Words):** Метод, в котором текст представляется как набор слов без учета их порядка, но с указанием частотности.\n",
        "- **TF-IDF (Term Frequency-Inverse Document Frequency):** Статистика, которая позволяет оценить важность слова в документе относительно всего корпуса текстов. TF — частота слова в данном документе, а IDF — обратная частота слова в других документах. Чем выше TF-IDF, тем значимее слово в конкретном документе.\n",
        "  \n",
        "#### 1.5 Важные соображения при составлении словаря\n",
        "\n",
        "- **Ограничение объема словаря:** Слишком большой словарь может замедлить обработку данных и обучение моделей, поэтому часто используется ограничение на количество слов (например, только 5000 самых часто встречающихся).\n",
        "- **Работа с редкими словами:** Можно исключить слова, которые встречаются очень редко, чтобы снизить размер словаря.\n",
        "- **Особенности языка:** Слова в разных языках могут иметь разное написание, склонение и т.д., поэтому перед составлением словаря важно учитывать специфику языка.\n",
        "\n",
        "### Часть 2: Токенизация слов\n",
        "\n",
        "**Токенизация** — это процесс разбиения текста на отдельные элементы, называемые токенами. Токенизация является базовым этапом в обработке текста, поскольку она позволяет работать с текстом не как с единым блоком, а как с набором отдельных элементов (слов, предложений и т.д.).\n",
        "\n",
        "#### 2.1 Что такое токен?\n",
        "\n",
        "**Токен** — это минимальная единица текста, на которую можно разбить текстовый документ. Токенами обычно являются слова, но они также могут быть символами, предложениями или целыми фразами в зависимости от задач обработки текста.\n",
        "\n",
        "#### 2.2 Типы токенизации\n",
        "\n",
        "1. **Токенизация на уровне слов:** Разбиение текста на отдельные слова.\n",
        "   - Подходит для большинства задач обработки текста.\n",
        "   - Слова могут быть разбиты по пробелам, а также с учетом пунктуации.\n",
        "\n",
        "2. **Токенизация на уровне предложений:** Разделение текста на предложения.\n",
        "   - Используется для анализа текста на уровне предложений.\n",
        "   - Включает в себя распознавание границ предложений с учетом аббревиатур и сокращений (например, \"и т.д.\").\n",
        "\n",
        "3. **Токенизация символов:** Разделение текста на отдельные символы.\n",
        "   - Используется для специфических задач, таких как анализ и предсказание текста в языках с неразделенными словами (например, китайский язык).\n",
        "  \n",
        "4. **Субсловная токенизация:** Разделение слов на морфемы или подслова.\n",
        "   - Применяется в методах обработки естественного языка, таких как BPE (Byte Pair Encoding) и WordPiece, используемые в современных NLP-моделях.\n",
        "   - Позволяет работать с редкими словами и обрабатывать корни, префиксы и суффиксы, что особенно полезно в языках со сложной морфологией.\n",
        "\n",
        "#### 2.3 Методы токенизации\n",
        "\n",
        "1. **Токенизация по пробелам:** Самый простой метод, при котором слова разделяются по пробелам.\n",
        "   - Пример: \"Привет, как дела?\" превращается в [\"Привет\", \"как\", \"дела\"].\n",
        "   - Однако метод не учитывает пунктуацию и сложные случаи (например, \"г-н\" или \"д-р\").\n",
        "\n",
        "2. **Регулярные выражения:** Использование регулярных выражений для настройки правил токенизации.\n",
        "   - Пример: Регулярное выражение `\\w+` находит все слова, игнорируя знаки препинания.\n",
        "\n",
        "3. **Лексическая токенизация:** Более сложный метод, который учитывает грамматические и лексические особенности текста.\n",
        "   - Использует словари и правила для различных языков и специфических текстов.\n",
        "   - Пример: Токенизатор SpaCy использует словари и правила для различных языков, что делает его точным для сложных текстов.\n",
        "\n",
        "4. **Модели субсловной токенизации:** BPE и WordPiece\n",
        "   - Byte Pair Encoding (BPE): Алгоритм, который объединяет часто встречающиеся пары символов, образуя подслова.\n",
        "   - WordPiece: Похож на BPE, но используется в современных NLP-моделях (например, BERT).\n",
        "   - Эти методы позволяют эффективно работать с редкими словами и лучше справляются с морфологией языка.\n",
        "\n",
        "#### 2.4 Проблемы и решения при токенизации\n",
        "\n",
        "1. **Неоднозначность:** В некоторых случаях одно слово может быть частью другого слова, а некоторые слова могут иметь несколько смыслов.\n",
        "   - **Решение:** Лексический анализ или использование контекстуальных токенизаторов, которые учитывают контекст слова.\n",
        "\n",
        "2. **Обработка пунктуации:** Пунктуация может создавать ложные разделения слов.\n",
        "   - **Решение:** Регулярные выражения или специализированные токенизаторы, которые умеют обрабатывать знаки препинания.\n",
        "\n",
        "3. **Языковые особенности:** В разных языках разная структура предложений и использование пробелов.\n",
        "   - **Решение:** Использование специализированных токенизаторов для каждого языка, например, китайского или японского.\n",
        "\n",
        "4. **Сложные слова и сокращения:** Многозначные слова и сокращения (например, \"г-н\", \"г-жа\").\n",
        "   - **Решение:** Использование предварительно обученных токенизаторов и дополнительных правил для обработки сокращений.\n",
        "\n",
        "### Часть 3: Практическое применение и инструменты\n",
        "\n",
        "#### 3.1 Инструменты для токенизации и создания словаря\n",
        "\n",
        "- **NLTK (Natural Language Toolkit):** Библиотека для обработки естественного языка на Python, которая включает токенизаторы, такие как `word_tokenize` и `sent_tokenize`.\n",
        "\n",
        "\n",
        "- **SpaCy:** Мощная библиотека для NLP, поддерживающая токенизацию, лемматизацию и частеречную разметку. Она также предоставляет готовые модели для различных языков.\n",
        "- **Transformers от Hugging Face:** Библиотека для работы с моделями трансформеров, включает токенизаторы WordPiece и BPE, подходящие для современных NLP-моделей.\n",
        "- **Gensim:** Библиотека для создания частотных словарей и работы с текстами, которая поддерживает представление текста через Word2Vec и другие методы векторизации.\n",
        "\n",
        "#### 3.2 Векторизация текста с использованием словаря\n",
        "\n",
        "После создания словаря и токенизации текст можно преобразовать в числовое представление:\n",
        "- **Мешок слов:** Представление текста в виде векторов, основанных на частотности слов.\n",
        "- **TF-IDF:** Учет частоты и важности слов.\n",
        "- **Word Embeddings:** Представление слов через вложения (embedding), такие как Word2Vec или GloVe, учитывающие семантические связи.\n",
        "\n",
        "Таким образом, составление словаря и токенизация — это базовые этапы обработки текста, на которых строятся многие алгоритмы машинного обучения и NLP. Эти процессы позволяют преобразовать текст в структуру, удобную для анализа и обучения моделей, что делает их незаменимыми в анализе текстовых данных."
      ],
      "metadata": {
        "id": "2zt16BFXzTXQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "AF3eG1QN0za3"
      }
    }
  ]
}