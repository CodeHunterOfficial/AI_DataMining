{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM3MCgXEcmdEkGUK+UuuC4L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CodeHunterOfficial/AI_DataMining/blob/main/NLP/Lesson%202.%20%20%D0%92%D0%B5%D0%BA%D1%82%D0%BE%D1%80%D0%BD%D1%8B%D0%B5_%D0%BF%D1%80%D0%B5%D0%B4%D1%81%D1%82%D0%B0%D0%B2%D0%BB%D0%B5%D0%BD%D0%B8%D1%8F_%D1%81%D0%BB%D0%BE%D0%B2_(Word2Vec%2CGloVe%2CELMo%2CSkip_Gram)/Lecture_2_%D0%92%D0%B5%D0%BA%D1%82%D0%BE%D1%80%D0%BD%D0%BE%D0%B5_%D0%BF%D1%80%D0%B5%D0%B4%D1%81%D1%82%D0%B0%D0%B2%D0%BB%D0%B5%D0%BD%D0%B8%D0%B5_%D1%81%D0%BB%D0%BE%D0%B2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Векторное представление слов\n",
        "\n",
        "## Введение\n",
        "\n",
        "Векторное представление слов — это метод, с помощью которого слова преобразуются в векторы чисел для удобства анализа и работы с текстом. Основная идея в том, что близкие по смыслу слова будут иметь схожие векторные представления. Векторизация позволяет применить методы линейной алгебры и машинного обучения для работы с текстовыми данными.\n",
        "\n",
        "Ранее, при работе с текстом, слова представлялись в виде простых индексов, что не учитывало их семантику. Векторное представление же позволяет улавливать смысловые связи, что стало прорывом в обработке естественного языка (NLP).\n",
        "\n",
        "---\n",
        "\n",
        "## Основные методы векторного представления слов\n",
        "\n",
        "Существует несколько подходов для создания векторного представления слов. Рассмотрим основные:\n",
        "\n",
        "1. **One-Hot Encoding**\n",
        "2. **Bag of Words (BoW)**\n",
        "3. **TF-IDF (Term Frequency-Inverse Document Frequency)**\n",
        "4. **Word2Vec (CBOW и Skip-Gram)**\n",
        "5. **GloVe**\n",
        "6. **FastText**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## 1. One-Hot Encoding (Один горячий код)\n",
        "\n",
        "**One-Hot Encoding (OHE)** — это один из самых простых методов для представления слов в виде числовых векторов. Суть метода заключается в том, что каждому слову в словаре соответствует уникальный вектор, в котором только одна позиция равна 1 (или \"горячая\"), а все остальные позиции равны 0 (или \"холодные\").\n",
        "\n",
        "#### Принцип работы One-Hot Encoding\n",
        "\n",
        "Предположим, у нас есть небольшой словарь, состоящий из трех слов:\n",
        "- `cat` (кот),\n",
        "- `dog` (собака),\n",
        "- `mouse` (мышь).\n",
        "\n",
        "Этот словарь можно представить как набор индексов для каждого слова:\n",
        "- `cat` — индекс 0,\n",
        "- `dog` — индекс 1,\n",
        "- `mouse` — индекс 2.\n",
        "\n",
        "Для представления слов с помощью OHE мы создаём вектор размерности, равной количеству слов в словаре (в нашем случае 3). Каждый вектор будет содержать 1 на позиции, соответствующей индексу слова, и 0 на всех остальных позициях. Таким образом, для каждого слова в словаре будет свой уникальный вектор:\n",
        "\n",
        "- `cat` -> [1, 0, 0]\n",
        "- `dog` -> [0, 1, 0]\n",
        "- `mouse` -> [0, 0, 1]\n",
        "\n",
        "#### Пример 1: Кодирование нескольких слов\n",
        "\n",
        "Допустим, у нас есть предложение: \"cat and dog\".\n",
        "\n",
        "Предположим, что у нас словарь состоит из следующих слов: {\"cat\", \"dog\", \"mouse\", \"and\", \"the\"}.\n",
        "\n",
        "Представление слов в этом предложении будет следующим:\n",
        "\n",
        "- \"cat\" -> [1, 0, 0, 0, 0]\n",
        "- \"and\" -> [0, 0, 0, 1, 0]\n",
        "- \"dog\" -> [0, 1, 0, 0, 0]\n",
        "\n",
        "Векторное представление слов будет иметь размерность 5, так как словарь содержит 5 слов.\n",
        "\n",
        "#### Недостатки One-Hot Encoding\n",
        "\n",
        "1. **Высокая размерность**:\n",
        "    - При большом словаре размерность векторов становится очень большой. Например, если словарь содержит 10,000 уникальных слов, то каждый вектор будет иметь 10,000 элементов. Это приводит к значительному увеличению объема данных и необходимости работы с высокоразмерными векторами, что требует значительных вычислительных ресурсов.\n",
        "  \n",
        "2. **Проблема с синтаксической и семантической информацией**:\n",
        "    - Вектора One-Hot Encoding не содержат никакой информации о значении слов или их контексте. Все слова, независимо от их значений, представляются одинаково в виде векторов с одним активным элементом. Например, слова \"cat\" и \"dog\" могут быть семантически схожи (оба — это животные), но в представлении One-Hot они будут абсолютно разные и не будет явной связи между ними в пространстве векторов.\n",
        "\n",
        "#### Пример 2: Несоответствие в контексте\n",
        "\n",
        "Предположим, у нас есть два слова \"cat\" (кот) и \"dog\" (собака). Они имеют схожие значения, потому что оба относятся к животным, но при использовании One-Hot Encoding их представления будут такими:\n",
        "\n",
        "- `cat` -> [1, 0, 0]\n",
        "- `dog` -> [0, 1, 0]\n",
        "\n",
        "Между ними нет явной связи в векторном пространстве. Это является существенным ограничением One-Hot Encoding. Например, при расчете **косинусного сходства** между этими двумя векторами получится, что их сходство равно нулю, так как они находятся в разных точках многомерного пространства.\n",
        "\n",
        "\\[\n",
        "\\text{cos}(\\vec{cat}, \\vec{dog}) = \\frac{\\vec{cat} \\cdot \\vec{dog}}{\\|\\vec{cat}\\| \\|\\vec{dog}\\|} = \\frac{0}{1} = 0\n",
        "\\]\n",
        "\n",
        "Таким образом, One-Hot Encoding не может захватывать смысловое сходство между словами, что ограничивает его применение в задачах обработки естественного языка (NLP), где важна семантика и контекст.\n",
        "\n",
        "#### Пример 3: Увеличение размерности\n",
        "\n",
        "Если у нас будет больше слов в словаре, например, 1,000 слов, то размерность вектора увеличится до 1,000, и это приведет к значительному увеличению объема данных. Например, если необходимо представить набор документов или большие коллекции текста, то размерность этих векторов будет очень велика, и обработка таких данных станет дорогостоящей.\n",
        "\n",
        "Если словарь увеличится до 10,000 слов, то для представления каждого слова потребуется вектор длиной 10,000, что приведет к увеличению нагрузки на память и процессор. Это делает One-Hot Encoding непригодным для работы с большими корпусами текста.\n",
        "\n",
        "Таким образом:\n",
        "\n",
        "**One-Hot Encoding** — это простой и интуитивно понятный метод представления слов, но он имеет значительные ограничения:\n",
        "- Высокая размерность.\n",
        "- Отсутствие учета семантического сходства между словами.\n",
        "\n",
        "Этот метод подходит для небольших и простых задач, но для более сложных задач, например, анализа текста, где важна семантика и контекст, требуется использование более сложных методов представления слов, таких как **Word2Vec** или **GloVe**, которые могут эффективно учитывать семантические связи между словами.\n",
        "\n",
        "\n",
        "\n",
        "Чтобы реализовать **One-Hot Encoding** на Python, можно использовать несколько подходов. Один из самых популярных методов — это использование библиотеки **`sklearn`**, которая предоставляет класс **`OneHotEncoder`** для кодирования данных. Также можно реализовать этот метод вручную, создавая векторы с помощью базовых операций Python.\n",
        "\n",
        "Рассмотрим оба варианта.\n",
        "\n",
        "### 1. Реализация с использованием `sklearn`\n",
        "\n",
        "Библиотека **`sklearn`** предоставляет простой и удобный способ кодирования данных с помощью класса **`OneHotEncoder`**. Однако этот метод обычно используется для числовых данных или категориальных признаков. Мы можем использовать его и для строковых данных, как в случае с текстом.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tIia3lBSGbs-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "\n",
        "# Пример словаря\n",
        "words = ['cat', 'dog', 'mouse']\n",
        "\n",
        "# Преобразуем список в массив\n",
        "words_array = np.array(words).reshape(-1, 1)\n",
        "\n",
        "# Создаем объект OneHotEncoder\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "\n",
        "# Применяем OneHotEncoding\n",
        "one_hot_encoded = encoder.fit_transform(words_array)\n",
        "\n",
        "# Выводим результат\n",
        "print(\"One-Hot Encoding representation:\")\n",
        "print(one_hot_encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "xSPD6l1WHBtI",
        "outputId": "a5f7318f-ffa6-42e0-fbb0-73d828950004"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-b12f88c5d409>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Создаем объект OneHotEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Применяем OneHotEncoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "### Пояснение:\n",
        "- Мы преобразуем список слов в массив с одной колонкой (это необходимо для корректной работы `OneHotEncoder`).\n",
        "- Метод **`fit_transform`** создает и применяет One-Hot кодирование.\n",
        "- Параметр `sparse=False` позволяет получить результат в виде обычного массива (если оставить `sparse=True`, результат будет в виде разреженной матрицы).\n",
        "\n",
        "### 2. Реализация вручную\n",
        "\n",
        "Если не хочется использовать сторонние библиотеки, можно реализовать **One-Hot Encoding** вручную. Например, используя словарь, в котором каждому слову будет сопоставлен его индекс.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sG4PEG60HB7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Пример словаря\n",
        "words = ['cat', 'dog', 'mouse']\n",
        "\n",
        "# Создаем словарь, который будет содержать индексы для каждого слова\n",
        "word_to_index = {word: idx for idx, word in enumerate(words)}\n",
        "\n",
        "# Функция для преобразования слова в его One-Hot представление\n",
        "def one_hot_encoding(word, word_to_index):\n",
        "    # Создаем вектор размером равным количеству слов в словаре\n",
        "    one_hot_vector = [0] * len(word_to_index)\n",
        "\n",
        "    # Получаем индекс слова и ставим 1 на соответствующую позицию\n",
        "    index = word_to_index.get(word)\n",
        "    if index is not None:\n",
        "        one_hot_vector[index] = 1\n",
        "\n",
        "    return one_hot_vector\n",
        "\n",
        "# Применяем функцию для кодирования\n",
        "print(\"One-Hot Encoding:\")\n",
        "for word in words:\n",
        "    print(f\"'{word}': {one_hot_encoding(word, word_to_index)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHtnPI9jHLXC",
        "outputId": "f7ead94f-d636-4e40-cfa5-0a292fae7052"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One-Hot Encoding:\n",
            "'cat': [1, 0, 0]\n",
            "'dog': [0, 1, 0]\n",
            "'mouse': [0, 0, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "### Пояснение:\n",
        "- Мы создаем словарь `word_to_index`, который сопоставляет каждому слову его индекс в списке.\n",
        "- В функции **`one_hot_encoding`** мы создаем вектор с нулями и устанавливаем \"1\" на позиции, которая соответствует индексу текущего слова.\n",
        "- Так как размерность вектора зависит от количества уникальных слов, размерность вектора будет автоматически соответствовать числу слов в словаре.\n",
        "\n",
        "### 3. Реализация с использованием библиотеки `pandas`\n",
        "\n",
        "Также можно использовать библиотеку **`pandas`** для работы с данными, что сделает процесс кодирования еще удобнее.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CsLLPSt6HLou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Пример списка слов\n",
        "words = ['cat', 'dog', 'mouse']\n",
        "\n",
        "# Преобразуем список в DataFrame\n",
        "df = pd.DataFrame(words, columns=['word'])\n",
        "\n",
        "# Применяем One-Hot Encoding с использованием pandas.get_dummies()\n",
        "one_hot_encoded_df = pd.get_dummies(df['word'])\n",
        "\n",
        "# Выводим результат\n",
        "print(\"One-Hot Encoding with pandas:\")\n",
        "print(one_hot_encoded_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ub6sdk0gHTSw",
        "outputId": "79f6cc3e-8a23-4329-d9c3-83dccea10e82"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One-Hot Encoding with pandas:\n",
            "     cat    dog  mouse\n",
            "0   True  False  False\n",
            "1  False   True  False\n",
            "2  False  False   True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "### Пояснение:\n",
        "- Сначала мы создаем DataFrame из списка слов.\n",
        "- Метод **`pd.get_dummies`** автоматически применяет One-Hot кодирование и преобразует категориальные значения в столбцы, где значения будут 1 или 0 в зависимости от наличия или отсутствия категории в строке.\n",
        "\n",
        "Таким образом, реализация **One-Hot Encoding** может быть выполнена разными способами:\n",
        "- Использование **`sklearn`** удобно и быстро, особенно для работы с категориальными данными в более сложных моделях.\n",
        "- Реализация вручную дает полное понимание процесса и позволяет кастомизировать решение под конкретные задачи.\n",
        "- **`pandas`** предоставляет очень удобный и простой метод для работы с табличными данными и кодирования категориальных признаков.\n",
        "\n",
        "Каждый из этих методов имеет свои преимущества, и выбор зависит от задачи, которую вы решаете.\n",
        "\n"
      ],
      "metadata": {
        "id": "mtKZITlkHTfi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# 2. **Bag of Words (BoW)** — Мешок слов\n",
        "\n",
        "**Bag of Words (BoW)** — это один из наиболее распространенных методов представления текстовых данных в виде числовых векторов. Суть метода заключается в том, чтобы преобразовать текст в вектор признаков, где каждый признак представляет собой количество вхождений определенного слова (или термина) в документ. Это очень простой и интуитивно понятный метод, который позволяет эффективно анализировать текстовую информацию.\n",
        "\n",
        "### 2.1 Основные принципы\n",
        "\n",
        "- В **Bag of Words** каждый документ в корпусе преобразуется в вектор фиксированной длины.\n",
        "- Длина вектора равна размеру словаря, то есть количеству уникальных слов в корпусе.\n",
        "- Каждый элемент вектора указывает на количество раз, которое слово встречается в документе.\n",
        "\n",
        "#### Пример\n",
        "Рассмотрим следующий небольшой корпус текстов (документов):\n",
        "\n",
        "- Документ 1: \"cat dog cat\"\n",
        "- Документ 2: \"dog mouse\"\n",
        "- Документ 3: \"cat mouse dog\"\n",
        "\n",
        "Предположим, что мы создаем словарь всех уникальных слов из этого корпуса:\n",
        "\n",
        "**Словарь**: {`cat`, `dog`, `mouse`}\n",
        "\n",
        "Теперь мы преобразуем каждый документ в вектор, который будет отражать количество вхождений каждого слова из словаря:\n",
        "\n",
        "- **Документ 1**: \"cat dog cat\" → Вектор: [2, 1, 0] (2 раза встречается \"cat\", 1 раз — \"dog\", 0 раз — \"mouse\")\n",
        "- **Документ 2**: \"dog mouse\" → Вектор: [0, 1, 1] (0 раз встречается \"cat\", 1 раз — \"dog\", 1 раз — \"mouse\")\n",
        "- **Документ 3**: \"cat mouse dog\" → Вектор: [1, 1, 1] (1 раз встречается \"cat\", 1 раз — \"dog\", 1 раз — \"mouse\")\n",
        "\n",
        "Таким образом, у нас получаются следующие векторы для каждого документа:\n",
        "\n",
        "- **Документ 1**: [2, 1, 0]\n",
        "- **Документ 2**: [0, 1, 1]\n",
        "- **Документ 3**: [1, 1, 1]\n",
        "\n",
        "### 2.2 Математическое описание\n",
        "\n",
        "Пусть у нас есть набор документов $D = \\{d_1, d_2, \\dots, d_m\\}$, и словарь, содержащий $V$ уникальных слов: $\\{w_1, w_2, \\dots, w_V\\}$. Каждый документ $d_i$ можно представить как вектор длины $V$, где элемент $v_{ij}$ — это количество вхождений слова $w_j$ в документ $d_i$.\n",
        "\n",
        "Формально, представление документа $d_i$ в модели **Bag of Words**:\n",
        "\n",
        "$$\n",
        "\\vec{d_i} = [ \\text{count}(w_1, d_i), \\text{count}(w_2, d_i), \\dots, \\text{count}(w_V, d_i)]\n",
        "$$\n",
        "где $\\text{count}(w_j, d_i)$ — это количество вхождений слова $w_j$ в документ $d_i$.\n",
        "\n",
        "Пример:\n",
        "- Документ 1: \"cat dog cat\" → Вектор: [2, 1, 0]\n",
        "- Документ 2: \"dog mouse\" → Вектор: [0, 1, 1]\n",
        "- Документ 3: \"cat mouse dog\" → Вектор: [1, 1, 1]\n",
        "\n",
        "Здесь:\n",
        "- $\\text{count}(cat, d_1) = 2$\n",
        "- $\\text{count}(dog, d_1) = 1$\n",
        "- $\\text{count}(mouse, d_1) = 0$\n",
        "\n",
        "### 2.3 Преобразование в матрицу\n",
        "\n",
        "Множество документов $D$ можно преобразовать в **матрицу признаков**, где строки будут представлять документы, а столбцы — слова из словаря. Каждая ячейка в этой матрице будет содержать количество вхождений слова в соответствующий документ.\n",
        "\n",
        "Для нашего примера:\n",
        "\n",
        "| Документ / Слово | cat | dog | mouse |\n",
        "|------------------|-----|-----|-------|\n",
        "| Документ 1       | 2   | 1   | 0     |\n",
        "| Документ 2       | 0   | 1   | 1     |\n",
        "| Документ 3       | 1   | 1   | 1     |\n",
        "\n",
        "Матрица признаков для трех документов будет выглядеть так:\n",
        "\n",
        "$$\n",
        "\\begin{pmatrix}\n",
        "2 & 1 & 0 \\\\\n",
        "0 & 1 & 1 \\\\\n",
        "1 & 1 & 1 \\\\\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "Каждая строка этой матрицы — это вектор, представляющий соответствующий документ в модели **Bag of Words**.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Для реализации модели **Bag of Words (BoW)** на Python, можно воспользоваться стандартными инструментами, такими как библиотеки `CountVectorizer` из `scikit-learn` или вручную реализовать функцию для подсчета слов. Давайте рассмотрим оба варианта.\n",
        "\n",
        "### Вариант 1: Использование библиотеки `scikit-learn`\n",
        "\n",
        "`scikit-learn` предоставляет готовое решение для преобразования текста в векторную форму с помощью метода **CountVectorizer**. Это наиболее удобный способ, который автоматически создает словарь и преобразует документы в матрицу признаков.\n",
        "\n",
        "Пример реализации:\n",
        "\n"
      ],
      "metadata": {
        "id": "Ea49_llJHwel"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Список документов\n",
        "documents = [\n",
        "    \"cat dog cat\",\n",
        "    \"dog mouse\",\n",
        "    \"cat mouse dog\"\n",
        "]\n",
        "\n",
        "# Инициализация CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Преобразование документов в матрицу признаков\n",
        "X = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Печать вектора признаков для каждого документа\n",
        "print(\"Матрица признаков:\")\n",
        "print(X.toarray())\n",
        "\n",
        "# Печать словаря (связанного списка слов с индексами)\n",
        "print(\"\\nСловарь (индексы слов):\")\n",
        "print(vectorizer.vocabulary_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2IkObJYH1iv",
        "outputId": "e1eef48f-98b9-4b32-9312-6b307a4a2ae9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Матрица признаков:\n",
            "[[2 1 0]\n",
            " [0 1 1]\n",
            " [1 1 1]]\n",
            "\n",
            "Словарь (индексы слов):\n",
            "{'cat': 0, 'dog': 1, 'mouse': 2}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Пояснение:\n",
        "1. **CountVectorizer** автоматически создает словарь из уникальных слов в корпусе и создает матрицу признаков, где каждый документ представлен вектором, указывающим на количество вхождений слов.\n",
        "2. Метод `fit_transform` обучает модель на корпусе и преобразует документы в числовое представление.\n",
        "3. `X.toarray()` возвращает матрицу признаков в виде массива.\n",
        "\n",
        "\n",
        "\n",
        "### Вариант 2: Реализация вручную\n",
        "\n",
        "Если вы хотите реализовать модель вручную, можно создать словарь и подсчитывать вхождения слов в каждый документ. Пример:\n",
        "\n"
      ],
      "metadata": {
        "id": "5z4-lb5JH5fM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Список документов\n",
        "documents = [\n",
        "    \"cat dog cat\",\n",
        "    \"dog mouse\",\n",
        "    \"cat mouse dog\"\n",
        "]\n",
        "\n",
        "# Создание словаря всех уникальных слов\n",
        "vocabulary = sorted(set(\" \".join(documents).split()))\n",
        "\n",
        "# Функция для преобразования документа в вектор признаков\n",
        "def text_to_vector(text, vocabulary):\n",
        "    words = text.split()\n",
        "    word_count = Counter(words)\n",
        "    return [word_count.get(word, 0) for word in vocabulary]\n",
        "\n",
        "# Преобразование каждого документа в вектор\n",
        "vectors = [text_to_vector(doc, vocabulary) for doc in documents]\n",
        "\n",
        "# Вывод результата\n",
        "print(\"Матрица признаков:\")\n",
        "for vec in vectors:\n",
        "    print(vec)\n",
        "\n",
        "print(\"\\nСловарь (индексы слов):\")\n",
        "for idx, word in enumerate(vocabulary):\n",
        "    print(f\"{word}: {idx}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_GTHjl4H-OX",
        "outputId": "5d1aa902-e666-461a-a80d-c725a546ff67"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Матрица признаков:\n",
            "[2, 1, 0]\n",
            "[0, 1, 1]\n",
            "[1, 1, 1]\n",
            "\n",
            "Словарь (индексы слов):\n",
            "cat: 0\n",
            "dog: 1\n",
            "mouse: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "### Пояснение:\n",
        "1. Мы создаем словарь, который состоит из всех уникальных слов в корпусе.\n",
        "2. Для каждого документа вычисляется вектор признаков, где каждый элемент — это количество вхождений соответствующего слова из словаря в данный документ.\n",
        "3. Используется класс `Counter` для подсчета вхождений слов.\n",
        "\n",
        "\n",
        "### Объяснение работы:\n",
        "- Для каждого документа создается вектор, где количество вхождений каждого слова из словаря отражается в соответствующем элементе вектора.\n",
        "- Словарь представляет собой набор уникальных слов из всех документов, и его размер определяет длину вектора признаков для каждого документа.\n",
        "\n",
        "Таким, образом оба подхода — с использованием библиотеки `scikit-learn` и вручную — являются рабочими и эффективными. В большинстве случаев использование `CountVectorizer` предпочтительнее, так как это позволяет избежать лишних операций и упрощает код, но для учебных целей и понимания работы модели BoW, вручную реализованный вариант может быть полезен.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### 2.4 Вариации модели Bag of Words\n",
        "\n",
        "## TF-IDF\n",
        "\n",
        "### 2.4.1. Учитывание частоты слов (Term Frequency — TF)\n",
        "\n",
        "Модель **Bag of Words (BoW)** в своей классической форме учитывает только количество вхождений слов в документы. Однако эта модель не принимает во внимание длину документа, что может привести к тому, что длинные документы будут иметь большее количество слов, что в свою очередь приводит к преобладанию часто встречающихся слов, не обращая внимания на их значимость. Для решения этой проблемы используется **Term Frequency (TF)**, который нормализует количество вхождений каждого слова в документе, что позволяет избежать искажения, связанного с длиной документа.\n",
        "\n",
        "**Формула для Term Frequency (TF):**\n",
        "\n",
        "$$\n",
        "\\text{TF}(w, d) = \\frac{\\text{count}(w, d)}{\\sum_{w' \\in d} \\text{count}(w', d)}\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $\\text{count}(w, d)$ — это количество вхождений слова $w$ в документ $d$,\n",
        "- $\\sum_{w' \\in d} \\text{count}(w', d)$ — сумма всех частот слов в документе $d$.\n",
        "\n",
        "Таким образом, **TF** нормализует количество вхождений слов, превращая его в отношение, что позволяет избежать преобладания длинных документов. Значение **TF** показывает, насколько часто слово $w$ встречается в документе $d$ по сравнению с общим числом слов в этом документе.\n",
        "\n",
        "### 2.4.2. Учитывание важности слов (TF-IDF)\n",
        "\n",
        "**TF-IDF (Term Frequency-Inverse Document Frequency)** — это мера, которая используется для оценки важности термина $w$ в контексте всего корпуса документов. Этот метод основывается на предположении, что слова, которые часто встречаются в одном документе, но редко встречаются в других, имеют более высокую значимость для этого документа. Например, если слово встречается в большинстве документов, то оно может быть общим и не иметь значимой информации для выделенного документа. Напротив, если слово встречается только в нескольких документах, это может свидетельствовать о том, что оно важно для понимания содержания конкретного документа.\n",
        "\n",
        "**Формула для TF-IDF:**\n",
        "\n",
        "$$\n",
        "\\text{TF-IDF}(w, d) = \\text{TF}(w, d) \\times \\text{IDF}(w)\n",
        "$$\n",
        "\n",
        "где:\n",
        "- **TF** — это **Term Frequency**, который мы уже вычислили на предыдущем шаге,\n",
        "- **IDF** — это **Inverse Document Frequency**, мера, которая показывает, насколько слово важно в контексте всего корпуса документов.\n",
        "\n",
        "**Формула для IDF:**\n",
        "\n",
        "$$\n",
        "\\text{IDF}(w) = \\log \\left( \\frac{N}{1 + |\\{d \\in D : w \\in d\\}|} \\right)\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $N$ — общее количество документов в корпусе $D$,\n",
        "- $|\\{d \\in D : w \\in d\\}|$ — количество документов, в которых встречается слово $w$.\n",
        "\n",
        "### 2.4.3. Принцип работы TF-IDF\n",
        "\n",
        "- **TF** отражает частоту появления слова в документе. Чем чаще слово встречается в документе, тем более важным оно является для этого документа.\n",
        "- **IDF** дает более высокие значения для слов, которые встречаются в меньшем числе документов (то есть они более специфичны для конкретных документов), и низкие значения для слов, которые часто встречаются в разных документах (что делает их менее значимыми для каждого отдельного документа).\n",
        "\n",
        "Когда мы умножаем **TF** и **IDF**, мы получаем значение, которое позволяет учесть как частоту слова в документе, так и его распространенность по всему корпусу. Это позволяет выделить важные термины, которые являются специфичными для конкретного документа.\n",
        "\n",
        "\n",
        "\n",
        "## 2.5. Применение TF-IDF в реальных задачах\n",
        "\n",
        "### 2.5.1. Пример 1: Поисковая система\n",
        "\n",
        "Представим, что пользователь вводит запрос в поисковую систему, например, \"купить автомобиль\". Система должна найти документы, которые максимально релевантны этому запросу. В процессе поиска она будет вычислять TF-IDF для каждого слова в документах и в запросе, чтобы вернуть наиболее релевантные результаты.\n",
        "\n",
        "1. Для каждого документа вычисляется TF для слов в запросе.\n",
        "2. Вычисляется IDF для каждого слова в запросе по всему корпусу.\n",
        "3. Далее вычисляется TF-IDF для каждого слова, и на основе этих значений система определяет наиболее важные документы.\n",
        "\n",
        "### 2.5.2. Пример 2: Классификация текстов\n",
        "\n",
        "Предположим, у нас есть набор текстов, относящихся к нескольким категориям, например, новости, спорт, технологии. Для классификации текстов можно вычислить TF-IDF для каждого термина в документе и использовать полученные значения в качестве признаков для машинного обучения. Это позволит классификатору различать тексты по их тематике.\n",
        "\n",
        "### 2.5.3. Пример 3: Кластеризация документов\n",
        "\n",
        "При кластеризации текстов можно использовать метрику TF-IDF для измерения схожести между документами. Схожие документы, как правило, будут иметь схожие значения TF-IDF для ключевых слов. Алгоритм кластеризации, такой как K-means, будет использовать эти значения для группировки документов в кластеры по схожести тем.\n",
        "\n",
        "\n",
        "\n",
        "Таким образом, **TF-IDF** — это основополагающая метрика в области обработки текстов, которая помогает решать важные задачи, такие как информационный поиск, классификация, извлечение информации и кластеризация. Несмотря на свои ограничения, TF-IDF остается одним из самых эффективных методов для анализа и обработки текстов.\n",
        "\n",
        "Важным аспектом является правильное использование TF-IDF в контексте задачи и корпуса данных, а также возможность сочетания его с другими методами, такими как модели на основе нейронных сетей, для получения более точных и глубоких результатов.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Давайте рассмотрим конкретные примеры использования **TF** и **TF-IDF** с вычислениями для лучшего понимания.\n",
        "\n",
        "\n",
        "\n",
        "## Пример 1: Расчет TF и TF-IDF\n",
        "\n",
        "Предположим, у нас есть следующий корпус из трех документов:\n",
        "\n",
        "1. Документ 1: \"машина едет по дороге\"\n",
        "2. Документ 2: \"машина стоит на дороге\"\n",
        "3. Документ 3: \"дорога в городе\"\n",
        "\n",
        "Наши задачи:\n",
        "1. Рассчитать **TF** (Term Frequency) для слова \"машина\" в каждом документе.\n",
        "2. Рассчитать **IDF** (Inverse Document Frequency) для слова \"машина\" по всем трем документам.\n",
        "3. Рассчитать **TF-IDF** для слова \"машина\" в каждом документе.\n",
        "\n",
        "### 1. Расчет TF (Term Frequency)\n",
        "\n",
        "**TF** показывает, как часто слово встречается в конкретном документе, относительно общего числа слов в этом документе. Формула:\n",
        "\n",
        "$$\n",
        "\\text{TF}(w, d) = \\frac{\\text{count}(w, d)}{\\sum_{w' \\in d} \\text{count}(w', d)}\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $\\text{count}(w, d)$ — это количество вхождений слова $w$ в документ $d$,\n",
        "- $\\sum_{w' \\in d} \\text{count}(w', d)$ — общее количество слов в документе $d$.\n",
        "\n",
        "Рассчитаем **TF** для слова \"машина\" в каждом документе.\n",
        "\n",
        "#### Документ 1: \"машина едет по дороге\"\n",
        "- Количество слов: 4\n",
        "- Количество вхождений \"машина\": 1\n",
        "$$\n",
        "\\text{TF}(\\text{\"машина\"}, \\text{Документ 1}) = \\frac{1}{4} = 0.25\n",
        "$$\n",
        "\n",
        "#### Документ 2: \"машина стоит на дороге\"\n",
        "- Количество слов: 4\n",
        "- Количество вхождений \"машина\": 1\n",
        "$$\n",
        "\\text{TF}(\\text{\"машина\"}, \\text{Документ 2}) = \\frac{1}{4} = 0.25\n",
        "$$\n",
        "\n",
        "#### Документ 3: \"дорога в городе\"\n",
        "- Количество слов: 3\n",
        "- Количество вхождений \"машина\": 0\n",
        "$$\n",
        "\\text{TF}(\\text{\"машина\"}, \\text{Документ 3}) = \\frac{0}{3} = 0\n",
        "$$\n",
        "\n",
        "Итак, мы получили следующие значения TF для слова \"машина\":\n",
        "- **Документ 1**: 0.25\n",
        "- **Документ 2**: 0.25\n",
        "- **Документ 3**: 0\n",
        "\n",
        "### 2. Расчет IDF (Inverse Document Frequency)\n",
        "\n",
        "**IDF** показывает, как важно слово в контексте всего корпуса. Чем реже слово встречается в документах, тем выше его значимость для документа. Формула для **IDF**:\n",
        "\n",
        "$$\n",
        "\\text{IDF}(w) = \\log \\left( \\frac{N}{1 + |\\{d \\in D : w \\in d\\}|} \\right)\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $N$ — общее количество документов в корпусе,\n",
        "- $|\\{d \\in D : w \\in d\\}|$ — количество документов, в которых встречается слово $w$.\n",
        "\n",
        "В нашем примере $N = 3$, и слово \"машина\" встречается в двух документах (Документ 1 и Документ 2).\n",
        "\n",
        "$$\n",
        "\\text{IDF}(\\text{\"машина\"}) = \\log \\left( \\frac{3}{1 + 2} \\right) = \\log \\left( \\frac{3}{3} \\right) = \\log(1) = 0\n",
        "$$\n",
        "\n",
        "Так как слово \"машина\" встречается в двух документах, IDF для него равен 0. Это означает, что слово распространено в корпусе и не имеет высокой специфичности для отдельных документов.\n",
        "\n",
        "### 3. Расчет TF-IDF\n",
        "\n",
        "Теперь мы можем рассчитать **TF-IDF** для слова \"машина\" в каждом документе. Формула:\n",
        "\n",
        "$$\n",
        "\\text{TF-IDF}(w, d) = \\text{TF}(w, d) \\times \\text{IDF}(w)\n",
        "$$\n",
        "\n",
        "Рассчитаем **TF-IDF** для слова \"машина\" в каждом документе:\n",
        "\n",
        "- **Документ 1**: $\\text{TF-IDF}(\\text{\"машина\"}, \\text{Документ 1}) = 0.25 \\times 0 = 0$\n",
        "- **Документ 2**: $\\text{TF-IDF}(\\text{\"машина\"}, \\text{Документ 2}) = 0.25 \\times 0 = 0$\n",
        "- **Документ 3**: $\\text{TF-IDF}(\\text{\"машина\"}, \\text{Документ 3}) = 0 \\times 0 = 0$\n",
        "\n",
        "Таким образом, для всех документов значение **TF-IDF** для слова \"машина\" равно 0, потому что IDF равно 0. Это показывает, что слово \"машина\" является слишком распространенным в данном корпусе, чтобы быть значимым для конкретных документов.\n",
        "\n",
        "\n",
        "\n",
        "## Пример 2: Влияние редких слов на TF-IDF\n",
        "\n",
        "Теперь давайте рассмотрим слово \"город\", которое встречается только в одном документе, и повторим расчеты.\n",
        "\n",
        "### 1. Расчет TF для слова \"город\"\n",
        "\n",
        "- **Документ 1**: слово \"город\" не встречается, TF = 0.\n",
        "- **Документ 2**: слово \"город\" не встречается, TF = 0.\n",
        "- **Документ 3**: слово \"город\" встречается один раз, и общее количество слов в документе равно 3:\n",
        "  $$\n",
        "  \\text{TF}(\\text{\"город\"}, \\text{Документ 3}) = \\frac{1}{3} = 0.33\n",
        "  $$\n",
        "\n",
        "### 2. Расчет IDF для слова \"город\"\n",
        "\n",
        "Слово \"город\" встречается только в одном документе, поэтому количество документов, содержащих слово \"город\", равно 1.\n",
        "\n",
        "$$\n",
        "\\text{IDF}(\\text{\"город\"}) = \\log \\left( \\frac{3}{1 + 1} \\right) = \\log \\left( \\frac{3}{2} \\right) \\approx 0.176\n",
        "$$\n",
        "\n",
        "### 3. Расчет TF-IDF для слова \"город\"\n",
        "\n",
        "Теперь рассчитываем **TF-IDF** для слова \"город\" в документе 3:\n",
        "\n",
        "$$\n",
        "\\text{TF-IDF}(\\text{\"город\"}, \\text{Документ 3}) = 0.33 \\times 0.176 \\approx 0.058\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "### Итоговые результаты\n",
        "\n",
        "| Слово     | Документ 1 (TF) | Документ 2 (TF) | Документ 3 (TF) | IDF   | Документ 1 (TF-IDF) | Документ 2 (TF-IDF) | Документ 3 (TF-IDF) |\n",
        "|--|--|--|--|-||||\n",
        "| машина    | 0.25            | 0.25            | 0               | 0     | 0                   | 0                   | 0                   |\n",
        "| дорога    | 0.25            | 0.25            | 0.33            | 0.176 | 0                   | 0                   | 0.058               |\n",
        "| город     | 0               | 0               | 0.33            | 0.176 | 0                   | 0                   | 0.058               |\n",
        "\n",
        "Таким образом, из приведенных примеров видно, что **TF-IDF** помогает выделять важные термины для каждого документа в корпусе. Слова, которые часто встречаются в документах, имеют низкий **IDF**, а слова, которые встречаются реже, имеют более высокое значение **IDF**. Умножение **TF** и **IDF** дает **TF-IDF**, который отражает важность слова для документа в контексте всего корпуса.\n",
        "\n",
        "Использование TF-IDF с библиотекой scikit-learn\n"
      ],
      "metadata": {
        "id": "Z35I_rcfIA-B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Корпус документов\n",
        "corpus = [\n",
        "    \"машина едет по дороге\",\n",
        "    \"машина стоит на дороге\",\n",
        "    \"дорога в городе\"\n",
        "]\n",
        "\n",
        "# Создание объекта TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Преобразование корпуса в матрицу TF-IDF\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Получение векторных представлений (матрица разреженных данных)\n",
        "print(X.toarray())\n",
        "\n",
        "# Получение соответствующих слов (терминов)\n",
        "print(vectorizer.get_feature_names_out())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1OBtUKgsIKh6",
        "outputId": "c2399741-4919-432c-df32-116f5d468d27"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.         0.42804604 0.5628291  0.42804604 0.\n",
            "  0.5628291  0.        ]\n",
            " [0.         0.         0.42804604 0.         0.42804604 0.5628291\n",
            "  0.         0.5628291 ]\n",
            " [0.70710678 0.70710678 0.         0.         0.         0.\n",
            "  0.         0.        ]]\n",
            "['городе' 'дорога' 'дороге' 'едет' 'машина' 'на' 'по' 'стоит']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Пример с лематизации:\n",
        "\n"
      ],
      "metadata": {
        "id": "5eyxg3oWIOGN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download ru_core_news_sm\n",
        "\n",
        "import spacy\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Загрузка модели spaCy для русского языка\n",
        "nlp = spacy.load('ru_core_news_sm')\n",
        "\n",
        "# Пример корпуса\n",
        "corpus = [\n",
        "    \"машина едет по дороге\",\n",
        "    \"машина стоит на дороге\",\n",
        "    \"дорога в городе\"\n",
        "]\n",
        "\n",
        "# Функция для лемматизации текста с использованием spaCy\n",
        "def lemmatize_text(text):\n",
        "    # Применяем модель spaCy к тексту\n",
        "    doc = nlp(text)\n",
        "    # Извлекаем леммы всех слов\n",
        "    lemmatized_text = ' '.join([token.lemma_ for token in doc])\n",
        "    return lemmatized_text\n",
        "\n",
        "# Лемматизация всего корпуса\n",
        "lemmatized_corpus = [lemmatize_text(text) for text in corpus]\n",
        "\n",
        "# Создание объекта TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Преобразование лемматизированного корпуса в матрицу TF-IDF\n",
        "X = vectorizer.fit_transform(lemmatized_corpus)\n",
        "\n",
        "# Получение векторных представлений\n",
        "tfidf_matrix = X.toarray()\n",
        "\n",
        "# Получение слов, для которых рассчитан TF-IDF\n",
        "words = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Вывод результатов\n",
        "for word, tfidf_values in zip(words, tfidf_matrix.T):\n",
        "    print(f\"Слово: {word} | Значения TF-IDF: {tfidf_values}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uq3BXbsaIRlq",
        "outputId": "f8b30576-5951-4e54-c55f-5bb4b2bf7656"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ru-core-news-sm==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/ru_core_news_sm-3.7.0/ru_core_news_sm-3.7.0-py3-none-any.whl (15.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from ru-core-news-sm==3.7.0) (3.7.5)\n",
            "Collecting pymorphy3>=1.0.0 (from ru-core-news-sm==3.7.0)\n",
            "  Downloading pymorphy3-2.0.2-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting dawg-python>=0.7.1 (from pymorphy3>=1.0.0->ru-core-news-sm==3.7.0)\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting pymorphy3-dicts-ru (from pymorphy3>=1.0.0->ru-core-news-sm==3.7.0)\n",
            "  Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (0.13.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->ru-core-news-sm==3.7.0) (0.1.2)\n",
            "Downloading pymorphy3-2.0.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymorphy3-dicts-ru, dawg-python, pymorphy3, ru-core-news-sm\n",
            "Successfully installed dawg-python-0.7.2 pymorphy3-2.0.2 pymorphy3-dicts-ru-2.4.417150.4580142 ru-core-news-sm-3.7.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('ru_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Слово: город | Значения TF-IDF: [0.       0.       0.861037]\n",
            "Слово: дорога | Значения TF-IDF: [0.34520502 0.34520502 0.50854232]\n",
            "Слово: ехать | Значения TF-IDF: [0.5844829 0.        0.       ]\n",
            "Слово: машина | Значения TF-IDF: [0.44451431 0.44451431 0.        ]\n",
            "Слово: на | Значения TF-IDF: [0.        0.5844829 0.       ]\n",
            "Слово: по | Значения TF-IDF: [0.5844829 0.        0.       ]\n",
            "Слово: стоить | Значения TF-IDF: [0.        0.5844829 0.       ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "#Word2Vec (CBOW и Skip-Gram)\n",
        "\n",
        "**Цель лекции**: Понимание алгоритмов Word2Vec, включая методы Continuous Bag of Words (CBOW) и Skip-Gram. Мы разберем их принципы, математическую основу, а также их отличия и применение.\n",
        "\n",
        "\n",
        "\n",
        "### Введение в Word2Vec\n",
        "\n",
        "**Word2Vec** — это модель представления слов в виде плотных векторных представлений. Эти представления (или эмбеддинги) создаются с помощью нейронных сетей и используют контекст слов для определения их значений. Модель Word2Vec включает два основных алгоритма:\n",
        "\n",
        "1. **Continuous Bag of Words (CBOW)**\n",
        "2. **Skip-Gram**\n",
        "\n",
        "Обе модели могут быть обучены с использованием простого подхода на основе нейронных сетей, где задача — научиться предсказывать слова на основе контекста или наоборот. В результате этого обучения слова, которые часто встречаются в схожем контексте, будут иметь близкие векторные представления.\n",
        "\n",
        "### 1. Continuous Bag of Words (CBOW)\n",
        "\n",
        "#### Описание\n",
        "\n",
        "Метод **Continuous Bag of Words (CBOW)** — это одна из архитектур модели **Word2Vec**, которая обучает нейронную сеть для предсказания центрального слова на основе контекста. Идея заключается в том, чтобы для заданного контекста (нескольких слов вокруг центрального слова) предсказать это центральное слово.\n",
        "\n",
        "Метод CBOW противоположен модели **Skip-Gram**, где модель предсказывает контекстные слова на основе центрального. В CBOW наоборот — предсказывается центральное слово, исходя из контекстных слов. Таким образом, CBOW — это более \"обратный\" подход, чем Skip-Gram.\n",
        "\n",
        "##### Принцип работы CBOW:\n",
        "\n",
        "1. **Контекст** — это набор слов, окружающих целевое слово. Контекст может быть задан как несколько слов до и после целевого.\n",
        "   \n",
        "   Например, для предложения:\n",
        "   ```\n",
        "   The cat sat on the mat.\n",
        "   ```\n",
        "   Если цель — предсказать центральное слово \"sat\", то контекстными словами будут \"The\", \"cat\", \"on\", \"the\", \"mat\". Таким образом, задача модели CBOW будет предсказать слово \"sat\" на основе этих контекстных слов.\n",
        "\n",
        "2. Модель получает векторные представления (эмбеддинги) всех контекстных слов и пытается через нейронную сеть предсказать центральное слово. Как правило, для обучения используется метрика ошибки, минимизируя которую, модель улучшает свои предсказания.\n",
        "\n",
        "#### Архитектура нейронной сети CBOW\n",
        "\n",
        "Архитектура модели CBOW включает три основных слоя:\n",
        "\n",
        "1. **Входной слой**:\n",
        "   - На вход модели подаются векторные представления каждого контекстного слова. Это означает, что на вход модель получает несколько векторов, которые представляют контекстные слова.\n",
        "   - Размер контекста можно настроить: например, в модели с контекстом в 2 слова с каждой стороны (t-2, t-1, t+1, t+2) для центрального слова будет 4 контекстных слова.\n",
        "\n",
        "2. **Скрытый слой**:\n",
        "   - В скрытом слое контекстные слова обрабатываются вместе. Вместо того чтобы обрабатывать каждый вектор отдельно, обычно их усредняют или суммируют, получая один вектор. Этот вектор затем передается на выходной слой.\n",
        "   - Усреднение или суммирование позволяет получить представление контекста в виде одного вектора фиксированной размерности.\n",
        "\n",
        "3. **Выходной слой**:\n",
        "   - На выходном слое модель должна предсказать целевое слово. Для этого используется слой с softmax-функцией, который возвращает вероятности для каждого слова в словаре. На основе этих вероятностей модель выбирает наиболее вероятное центральное слово.\n",
        "\n",
        "#### Математическое описание CBOW\n",
        "\n",
        "1. Пусть:\n",
        "   - $W \\in \\mathbb{R}^{V \\times D}$ — это матрица эмбеддингов для слов, где $V$ — размер словаря, а $D$ — размерность эмбеддинга (представления слова).\n",
        "   - $C = \\{w_{t-2}, w_{t-1}, w_{t+1}, w_{t+2}\\}$ — это контекстные слова для центрального слова $w_t$.\n",
        "\n",
        "2. Цель модели — минимизировать ошибку предсказания целевого слова $w_t$, используя контекст $C$.\n",
        "\n",
        "Предположим, что контекстные слова $C$ имеют векторы $\\vec{c_1}, \\vec{c_2}, \\ldots, \\vec{c_k}$, где $k$ — это количество контекстных слов. Далее, для каждого контекста предсказывается целевое слово $\\hat{w_t}$, которое стремится к реальному целевому слову $w_t$.\n",
        "\n",
        "Предсказание центрального слова $\\hat{w_t}$ для каждого контекста $C$ производится с использованием **softmax** функции. Для этого суммируются или усредняются векторы контекстных слов и затем используется вектор $\\vec{v_c}$, который является средним (или суммой) всех контекстных слов:\n",
        "\n",
        "$$\n",
        "\\vec{v_c} = \\frac{1}{k} \\sum_{i=1}^k \\vec{c_i}\n",
        "$$\n",
        "\n",
        "Затем предсказание для центрального слова $\\hat{w_t}$ рассчитывается через softmax:\n",
        "\n",
        "$$\n",
        "P(w_t | C) = \\frac{\\exp(\\vec{W_t}^T \\vec{v_c})}{\\sum_{w=1}^{V} \\exp(\\vec{W_w}^T \\vec{v_c})}\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $\\vec{W_t}$ — это вектор для целевого слова $w_t$,\n",
        "- $\\vec{v_c}$ — это усредненный вектор контекстных слов,\n",
        "- Сумма в знаменателе — это нормировка через softmax, которая обеспечивает, что выходной вектор $P(w_t | C)$ будет представлять собой вероятности для всех слов в словаре.\n",
        "\n",
        "#### Обучение модели CBOW\n",
        "\n",
        "Во время обучения модель CBOW настраивает свои параметры (эмбеддинги слов) таким образом, чтобы минимизировать ошибку предсказания целевого слова для каждого контекста. Это осуществляется через обратное распространение ошибки и оптимизацию с помощью градиентного спуска.\n",
        "\n",
        "### Важные особенности CBOW:\n",
        "\n",
        "- **Скорость обучения**: Модель CBOW, как правило, обучается быстрее, чем модель Skip-Gram, потому что она использует контекст из нескольких слов для предсказания одного целевого слова. Это позволяет более эффективно работать с большими объемами данных.\n",
        "- **Смещение**: Контекстные слова в модели CBOW не имеют значимой последовательности, что делает эту модель менее чувствительной к порядку слов. В отличие от Skip-Gram, CBOW рассматривает контекст как \"мешок слов\" (bag of words).\n",
        "  \n",
        "#### Применение:\n",
        "\n",
        "- **Обучение эмбеддингов слов**: CBOW используется для создания векторных представлений слов, которые сохраняют семантические и синтаксические связи между словами. Это может быть полезно для задач обработки естественного языка, таких как классификация текста, поиск информации, машинный перевод и т. д.\n",
        "  \n",
        "Модель CBOW, несмотря на свою простоту, является мощным инструментом для создания высококачественных эмбеддингов слов и широко используется в практике машинного обучения для обработки текста.\n",
        "\n",
        "Давайте рассмотрим пример работы модели **Continuous Bag of Words (CBOW)** с конкретными числами и формулами, чтобы лучше понять, как она работает.\n",
        "\n",
        "### Пример:\n",
        "Предположим, у нас есть предложение:\n",
        "\n",
        "```\n",
        "The cat sat on the mat.\n",
        "```\n",
        "\n",
        "И наша цель — предсказать центральное слово (\"sat\") на основе контекстных слов. Для этого используем CBOW.\n",
        "\n",
        "#### 1. Контекст\n",
        "Если размер окна контекста равен 2, то для центрального слова \"sat\" контекстными словами будут: **\"The\", \"cat\", \"on\", \"the\", \"mat\"**. Таким образом, контекст $C = \\{\\text{\"The\"}, \\text{\"cat\"}, \\text{\"on\"}, \\text{\"the\"}, \\text{\"mat\"}\\}$.\n",
        "\n",
        "#### 2. Векторные представления слов\n",
        "Предположим, что у нас есть эмбеддинги слов размерности 3 (для упрощения примера). Эти векторы могут быть следующими:\n",
        "\n",
        "| Слово | Вектор (эмбеддинг)       |\n",
        "|-|--|\n",
        "| The   | $[1, 0, 0]$            |\n",
        "| cat   | $[0, 1, 0]$            |\n",
        "| sat   | $[0, 0, 1]$            |\n",
        "| on    | $[1, 1, 0]$            |\n",
        "| the   | $[0, 0, 1]$            |\n",
        "| mat   | $[1, 0, 1]$            |\n",
        "\n",
        "Теперь у нас есть векторы для всех слов в контексте.\n",
        "\n",
        "#### 3. Усреднение векторов контекста\n",
        "Чтобы получить представление контекста, мы усредняем векторы всех контекстных слов:\n",
        "\n",
        "Контекст $C$ состоит из 5 слов: \"The\", \"cat\", \"on\", \"the\", \"mat\".\n",
        "\n",
        "Векторы этих слов:\n",
        "- $\\vec{The} = [1, 0, 0]$\n",
        "- $\\vec{cat} = [0, 1, 0]$\n",
        "- $\\vec{on} = [1, 1, 0]$\n",
        "- $\\vec{the} = [0, 0, 1]$\n",
        "- $\\vec{mat} = [1, 0, 1]$\n",
        "\n",
        "Теперь усредним эти векторы:\n",
        "\n",
        "$$\n",
        "\\vec{v_c} = \\frac{1}{5} \\left([1, 0, 0] + [0, 1, 0] + [1, 1, 0] + [0, 0, 1] + [1, 0, 1]\\right)\n",
        "$$\n",
        "\n",
        "Суммируем векторы:\n",
        "\n",
        "$$\n",
        "[1 + 0 + 1 + 0 + 1, 0 + 1 + 1 + 0 + 0, 0 + 0 + 0 + 1 + 1] = [3, 2, 2]\n",
        "$$\n",
        "\n",
        "Теперь делим на 5 (количество контекстных слов):\n",
        "\n",
        "$$\n",
        "\\vec{v_c} = \\frac{1}{5} [3, 2, 2] = [0.6, 0.4, 0.4]\n",
        "$$\n",
        "\n",
        "#### 4. Вычисление вероятности для целевого слова\n",
        "Теперь, когда у нас есть вектор контекста $\\vec{v_c} = [0.6, 0.4, 0.4]$, мы используем его для предсказания центрального слова. Нам нужно рассчитать вероятности для всех слов в словаре. В словаре у нас есть 6 слов: \"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\".\n",
        "\n",
        "Для вычисления вероятности для каждого слова мы используем формулу softmax:\n",
        "\n",
        "$$\n",
        "P(w_t | C) = \\frac{\\exp(\\vec{W_t}^T \\vec{v_c})}{\\sum_{w=1}^{V} \\exp(\\vec{W_w}^T \\vec{v_c})}\n",
        "$$\n",
        "\n",
        "Где:\n",
        "- $\\vec{W_t}$ — вектор для целевого слова,\n",
        "- $\\vec{v_c}$ — усреднённый вектор контекста.\n",
        "\n",
        "Предположим, что у нас есть матрица эмбеддингов для всех слов:\n",
        "\n",
        "| Слово | Вектор эмбеддинга  |\n",
        "|-|--|\n",
        "| The   | $[1, 0, 0]$      |\n",
        "| cat   | $[0, 1, 0]$      |\n",
        "| sat   | $[0, 0, 1]$      |\n",
        "| on    | $[1, 1, 0]$      |\n",
        "| the   | $[0, 0, 1]$      |\n",
        "| mat   | $[1, 0, 1]$      |\n",
        "\n",
        "Рассчитаем $\\vec{W_t}^T \\vec{v_c}$ для каждого слова. Это скалярное произведение вектора контекста $\\vec{v_c} = [0.6, 0.4, 0.4]$ и вектора каждого слова.\n",
        "\n",
        "1. Для слова \"The\":\n",
        "$$\n",
        "   \\vec{W_{\\text{The}}}^T \\vec{v_c} = [1, 0, 0] \\cdot [0.6, 0.4, 0.4] = 1 \\times 0.6 + 0 \\times 0.4 + 0 \\times 0.4 = 0.6\n",
        "$$\n",
        "\n",
        "2. Для слова \"cat\":\n",
        "$$\n",
        "   \\vec{W_{\\text{cat}}}^T \\vec{v_c} = [0, 1, 0] \\cdot [0.6, 0.4, 0.4] = 0 \\times 0.6 + 1 \\times 0.4 + 0 \\times 0.4 = 0.4\n",
        "$$\n",
        "\n",
        "3. Для слова \"sat\":\n",
        "$$\n",
        "   \\vec{W_{\\text{sat}}}^T \\vec{v_c} = [0, 0, 1] \\cdot [0.6, 0.4, 0.4] = 0 \\times 0.6 + 0 \\times 0.4 + 1 \\times 0.4 = 0.4\n",
        "$$\n",
        "\n",
        "4. Для слова \"on\":\n",
        "$$\n",
        "   \\vec{W_{\\text{on}}}^T \\vec{v_c} = [1, 1, 0] \\cdot [0.6, 0.4, 0.4] = 1 \\times 0.6 + 1 \\times 0.4 + 0 \\times 0.4 = 1.0\n",
        "$$\n",
        "\n",
        "5. Для слова \"the\":\n",
        "$$\n",
        "   \\vec{W_{\\text{the}}}^T \\vec{v_c} = [0, 0, 1] \\cdot [0.6, 0.4, 0.4] = 0 \\times 0.6 + 0 \\times 0.4 + 1 \\times 0.4 = 0.4\n",
        "$$\n",
        "\n",
        "6. Для слова \"mat\":\n",
        "$$\n",
        "   \\vec{W_{\\text{mat}}}^T \\vec{v_c} = [1, 0, 1] \\cdot [0.6, 0.4, 0.4] = 1 \\times 0.6 + 0 \\times 0.4 + 1 \\times 0.4 = 1.0\n",
        "$$\n",
        "\n",
        "Теперь применим softmax для нормализации:\n",
        "\n",
        "$$\n",
        "P(w_t | C) = \\frac{\\exp( \\vec{W_t}^T \\vec{v_c} )}{\\sum_{w=1}^{V} \\exp( \\vec{W_w}^T \\vec{v_c} )}\n",
        "$$\n",
        "\n",
        "Нормализуем эти значения, чтобы получить вероятности:\n",
        "\n",
        "$$\n",
        "\\sum \\exp(\\vec{W_w}^T \\vec{v_c}) = \\exp(0.6) + \\exp(0.4) + \\exp(0.4) + \\exp(1.0) + \\exp(0.4) + \\exp(1.0)\n",
        "$$\n",
        "\n",
        "Посчитаем экспоненты и их сумму:\n",
        "\n",
        "$$\n",
        "\\exp(0.6) \\approx 1.822, \\quad \\exp(0.4) \\approx 1.221, \\quad \\exp(1.0) \\approx 2.718\n",
        "$$\n",
        "$$\n",
        "\\sum \\exp = 1.822 + 1.221 + 1.221 + 2.718 + 1.221 + 2.718 \\approx 11.921\n",
        "$$\n",
        "\n",
        "Теперь вычислим вероятность для каждого слова:\n",
        "\n",
        "- Для \"The\":\n",
        "$$\n",
        "  P(\\text{The}) = \\frac{\\exp(0.6)}{11.921} = \\frac{1.822}{11.921} \\approx 0.153\n",
        "$$\n",
        "\n",
        "- Для \"cat\":\n",
        "$$\n",
        "  P(\\text{cat}) = \\frac{\\exp(0.4)}\n",
        "\n",
        "{11.921} = \\frac{1.221}{11.921} \\approx 0.102\n",
        "$$\n",
        "\n",
        "- Для \"sat\":\n",
        "$$\n",
        "  P(\\text{sat}) = \\frac{\\exp(0.4)}{11.921} = \\frac{1.221}{11.921} \\approx 0.102\n",
        "$$\n",
        "\n",
        "- Для \"on\":\n",
        "$$\n",
        "  P(\\text{on}) = \\frac{\\exp(1.0)}{11.921} = \\frac{2.718}{11.921} \\approx 0.228\n",
        "$$\n",
        "\n",
        "- Для \"the\":\n",
        "$$\n",
        "  P(\\text{the}) = \\frac{\\exp(0.4)}{11.921} = \\frac{1.221}{11.921} \\approx 0.102\n",
        "$$\n",
        "\n",
        "- Для \"mat\":\n",
        "$$\n",
        "  P(\\text{mat}) = \\frac{\\exp(1.0)}{11.921} = \\frac{2.718}{11.921} \\approx 0.228\n",
        "$$\n",
        "\n",
        "#### 5. Выбор слова\n",
        "После применения softmax, мы получаем вероятности для каждого слова в словаре. Модель выберет слово с наибольшей вероятностью как центральное слово. В этом случае вероятность для \"on\" и \"mat\" максимальна (по 0.228), так что модель может выбрать одно из этих слов как центральное.\n",
        "\n",
        "Таким образом, в реальной задаче CBOW будет предсказывать слово \"sat\" на основе контекста.\n",
        "\n",
        "Таким образом, этот пример иллюстрирует, как модель CBOW использует контекстные слова для предсказания центрального слова. Мы рассмотрели, как вычисляются векторы для слов, как они усредняются, и как применяется функция softmax для нормализации и предсказания целевого слова.\n",
        "\n",
        "Давайте реализуем модель Continuous Bag of Words (CBOW) на Python с использованием простого примера. Мы будем работать с эмбеддингами слов и вычислять вероятности для центрального слова, используя модель CBOW с функцией softmax.\n"
      ],
      "metadata": {
        "id": "aGIZU316IVDy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Шаг 1: Создание словаря и эмбеддингов слов\n",
        "vocab = [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
        "word_to_index = {word: idx for idx, word in enumerate(vocab)}  # Словарь для индексов слов\n",
        "index_to_word = {idx: word for word, idx in word_to_index.items()}  # Обратный словарь\n",
        "\n",
        "# Эмбеддинги слов (предположим, что они уже обучены и имеют размерность 3)\n",
        "embeddings = np.array([\n",
        "    [1, 0, 0],  # The\n",
        "    [0, 1, 0],  # cat\n",
        "    [0, 0, 1],  # sat\n",
        "    [1, 1, 0],  # on\n",
        "    [0, 0, 1],  # the\n",
        "    [1, 0, 1]   # mat\n",
        "])\n",
        "\n",
        "# Шаг 2: Контекстные слова для предсказания центрального\n",
        "context_words = [\"The\", \"cat\", \"on\", \"the\", \"mat\"]  # Контекстные слова для центрального слова \"sat\"\n",
        "context_indices = [word_to_index[word] for word in context_words]\n",
        "\n",
        "# Шаг 3: Усреднение векторов контекстных слов\n",
        "context_vectors = embeddings[context_indices]\n",
        "context_vector = np.mean(context_vectors, axis=0)\n",
        "print(f\"Вектор контекста: {context_vector}\")\n",
        "\n",
        "# Шаг 4: Вычисление вероятности для каждого слова с помощью softmax\n",
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x))  # Для стабильности вычислений\n",
        "    return exp_x / np.sum(exp_x)\n",
        "\n",
        "# Вычисление скалярного произведения для каждого слова в словаре\n",
        "scores = np.dot(embeddings, context_vector)\n",
        "\n",
        "# Применение softmax для нормализации\n",
        "probabilities = softmax(scores)\n",
        "\n",
        "# Шаг 5: Вывод вероятностей для каждого слова\n",
        "print(\"\\nВероятности для каждого слова:\")\n",
        "for i, word in enumerate(vocab):\n",
        "    print(f\"{word}: {probabilities[i]:.4f}\")\n",
        "\n",
        "# Шаг 6: Выбор слова с наибольшей вероятностью\n",
        "predicted_word_index = np.argmax(probabilities)\n",
        "predicted_word = index_to_word[predicted_word_index]\n",
        "print(f\"\\nПредсказанное центральное слово: {predicted_word}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Buh6otvIlaY",
        "outputId": "3ef349ce-faa3-4af9-d340-0e822e25159c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Вектор контекста: [0.6 0.4 0.4]\n",
            "\n",
            "Вероятности для каждого слова:\n",
            "The: 0.1553\n",
            "cat: 0.1271\n",
            "sat: 0.1271\n",
            "on: 0.2317\n",
            "the: 0.1271\n",
            "mat: 0.2317\n",
            "\n",
            "Предсказанное центральное слово: on\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Если вы хотите использовать векторное представление слов в модели CBOW для обработки реальных текстов, то на практике для обучения и использования эмбеддингов слов часто применяются готовые библиотеки, такие как Gensim, которая позволяет легко работать с моделями Word2Vec и обучать их на текстах. Векторные представления слов, полученные с помощью таких моделей, можно использовать для задач, подобных CBOW, как в вашем примере.\n",
        "\n",
        "Вот как можно использовать модель Word2Vec из библиотеки gensim, чтобы получить векторные представления слов и применить их в модели CBOW:\n"
      ],
      "metadata": {
        "id": "nAik_7QRIrPO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "\n",
        "# Шаг 1: Подготовим текстовые данные для обучения\n",
        "# Пример текста, который будет использован для обучения модели\n",
        "sentences = [\n",
        "    [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"],\n",
        "    [\"the\", \"dog\", \"sat\", \"on\", \"the\", \"mat\"],\n",
        "    [\"the\", \"cat\", \"is\", \"on\", \"the\", \"mat\"],\n",
        "    [\"the\", \"dog\", \"is\", \"on\", \"the\", \"mat\"]\n",
        "]\n",
        "\n",
        "# Шаг 2: Обучение модели Word2Vec с использованием CBOW\n",
        "# Параметр sg=0 указывает, что используется модель CBOW (если sg=1, то используется Skip-Gram)\n",
        "model = Word2Vec(sentences, vector_size=3, window=2, sg=0, min_count=1)\n",
        "\n",
        "# Шаг 3: Получение векторов слов\n",
        "# Получим вектор для слова \"cat\"\n",
        "cat_vector = model.wv[\"cat\"]\n",
        "print(f\"Вектор для слова 'cat': {cat_vector}\")\n",
        "\n",
        "# Шаг 4: Подсчёт среднего вектора контекста\n",
        "context_words = [\"the\", \"dog\", \"sat\", \"on\", \"the\", \"mat\"]\n",
        "context_vectors = [model.wv[word] for word in context_words]\n",
        "context_vector = np.mean(context_vectors, axis=0)\n",
        "print(f\"Вектор контекста: {context_vector}\")\n",
        "\n",
        "# Шаг 5: Применение softmax для предсказания центрального слова\n",
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x))  # Для стабильности вычислений\n",
        "    return exp_x / np.sum(exp_x)\n",
        "\n",
        "# Вычисление скалярного произведения для каждого слова в словаре\n",
        "scores = np.dot(model.wv.vectors, context_vector)\n",
        "\n",
        "# Применение softmax для нормализации\n",
        "probabilities = softmax(scores)\n",
        "\n",
        "# Шаг 6: Вывод вероятностей для каждого слова\n",
        "print(\"\\nВероятности для каждого слова:\")\n",
        "for i, word in enumerate(model.wv.index_to_key):\n",
        "    print(f\"{word}: {probabilities[i]:.4f}\")\n",
        "\n",
        "# Шаг 7: Выбор слова с наибольшей вероятностью\n",
        "predicted_word_index = np.argmax(probabilities)\n",
        "predicted_word = model.wv.index_to_key[predicted_word_index]\n",
        "print(f\"\\nПредсказанное центральное слово: {predicted_word}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-S83jKChIu1q",
        "outputId": "8efd447b-ebe3-48a5-ac02-891d55456c29"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Вектор для слова 'cat': [-0.27617383 -0.3149606   0.24372554]\n",
            "Вектор контекста: [ 0.04468367  0.05318629 -0.03218791]\n",
            "\n",
            "Вероятности для каждого слова:\n",
            "the: 0.1417\n",
            "mat: 0.1432\n",
            "on: 0.1470\n",
            "is: 0.1439\n",
            "dog: 0.1440\n",
            "sat: 0.1428\n",
            "cat: 0.1374\n",
            "\n",
            "Предсказанное центральное слово: on\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 2. Skip-Gram\n",
        "\n",
        "#### Описание\n",
        "\n",
        "Метод **Skip-Gram** является одним из подходов для обучения представлений слов (word embeddings). В модели **Skip-Gram** цель заключается в том, чтобы для каждого центрального слова предсказать его контекстные слова. Это противоположно методу **CBOW (Continuous Bag of Words)**, в котором на основе контекста предсказывается центральное слово.\n",
        "\n",
        "Например, в предложении:\n",
        "\n",
        "```\n",
        "The cat sat on the mat.\n",
        "```\n",
        "\n",
        "Если мы выбрали слово **\"sat\"** как центральное, то контекстными словами будут слова, окружающие его — **\"The\"**, **\"cat\"**, **\"on\"**, **\"the\"**, **\"mat\"**.\n",
        "\n",
        "#### Архитектура нейронной сети Skip-Gram\n",
        "\n",
        "Архитектура модели **Skip-Gram** включает несколько слоев:\n",
        "\n",
        "1. **Входной слой**:\n",
        "   - Векторное представление центрального слова $w_t$, которое преобразуется в вектор $\\vec{v_t}$.\n",
        "\n",
        "2. **Скрытый слой**:\n",
        "   - Скрытый слой, как правило, представляет собой линейную модель, которая преобразует векторное представление целевого слова в пространство скрытых признаков.\n",
        "\n",
        "3. **Выходной слой**:\n",
        "   - С помощью выходного слоя модель предсказывает вероятности для каждого контекстного слова, основываясь на центральном слове.\n",
        "\n",
        "#### Математическое описание\n",
        "\n",
        "Предположим, что:\n",
        "- $w_t$ — это целевое слово (central word).\n",
        "- $C = \\{w_{t-2}, w_{t-1}, w_{t+1}, w_{t+2}\\}$ — это контекстные слова, где $t$ — индекс центрального слова, а $C$ — множество слов, находящихся в пределах некоторого контекстного окна вокруг $w_t$.\n",
        "\n",
        "Цель модели **Skip-Gram** — минимизировать ошибку предсказания контекстных слов $C$ на основе целевого слова $w_t$. Для этого вычисляется вероятность появления каждого контекстного слова $w_{t+k}$ (где $k$ — сдвиг относительно центрального слова) через **softmax** функцию:\n",
        "\n",
        "$$\n",
        "P(w_{t+k} | w_t) = \\frac{\\exp(\\vec{W_{t+k}}^T \\vec{v_t})}{\\sum_{w=1}^{V} \\exp(\\vec{W_w}^T \\vec{v_t})}\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $\\vec{v_t}$ — векторное представление целевого слова $w_t$,\n",
        "- $\\vec{W_{t+k}}$ — вектор контекстного слова $w_{t+k}$,\n",
        "- $V$ — размер словаря (количество уникальных слов в тексте),\n",
        "- Сумма в знаменателе — это нормировка через softmax, которая гарантирует, что сумма вероятностей по всем словам будет равна 1.\n",
        "\n",
        "Таким образом, вероятность контекстного слова $w_{t+k}$ зависит от скалярного произведения вектора центрального слова $\\vec{v_t}$ с вектором контекстного слова $\\vec{W_{t+k}}$.\n",
        "\n",
        "#### Функция потерь\n",
        "\n",
        "Для всего набора контекстных слов $C$, функция потерь $L$ вычисляется как сумма логарифмов вероятностей предсказанных контекстных слов:\n",
        "\n",
        "$$\n",
        "L = - \\sum_{w_{t+k} \\in C} \\log P(w_{t+k} | w_t)\n",
        "$$\n",
        "\n",
        "Задача состоит в минимизации этой функции потерь с помощью методов оптимизации, таких как стохастический градиентный спуск (SGD), чтобы научить модель правильно предсказывать контекстные слова.\n",
        "\n",
        "### Шаги минимизации функции потерь\n",
        "\n",
        "#### Шаг 1: Вычисление градиентов\n",
        "\n",
        "Для минимизации функции потерь необходимо вычислить частные производные (градиенты) функции потерь по отношению к параметрам модели. Эти параметры включают:\n",
        "- Вектор $\\vec{v_t}$ для центрального слова $w_t$,\n",
        "- Вектор $\\vec{W_{t+k}}$ для каждого контекстного слова $w_{t+k}$.\n",
        "\n",
        "##### Градиент по отношению к вектору центрального слова $\\vec{v_t}$:\n",
        "\n",
        "Для каждого контекстного слова $w_{t+k}$ вычисляется градиент функции потерь по отношению к вектору $\\vec{v_t}$:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial \\vec{v_t}} = - \\sum_{w_{t+k} \\in C} \\left( 1 - P(w_{t+k} | w_t) \\right) \\vec{W_{t+k}}\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $1 - P(w_{t+k} | w_t)$ — это ошибка предсказания контекстного слова,\n",
        "- $\\vec{W_{t+k}}$ — вектор контекстного слова $w_{t+k}$.\n",
        "\n",
        "##### Градиент по отношению к вектору контекстного слова $\\vec{W_{t+k}}$:\n",
        "\n",
        "Градиент функции потерь по отношению к вектору контекстного слова $\\vec{W_{t+k}}$ для каждого контекстного слова $w_{t+k}$ вычисляется как:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial \\vec{W_{t+k}}} = - \\left( 1 - P(w_{t+k} | w_t) \\right) \\vec{v_t}\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $1 - P(w_{t+k} | w_t)$ — ошибка предсказания контекстного слова,\n",
        "- $\\vec{v_t}$ — вектор целевого слова.\n",
        "\n",
        "#### Шаг 2: Обновление параметров модели\n",
        "\n",
        "После вычисления градиентов для параметров $\\vec{v_t}$ и $\\vec{W_{t+k}}$ модель обновляет эти параметры с использованием стохастического градиентного спуска (SGD) или его модификаций.\n",
        "\n",
        "Для обновления вектора $\\vec{v_t}$ используется правило:\n",
        "\n",
        "$$\n",
        "\\vec{v_t} = \\vec{v_t} - \\eta \\frac{\\partial L}{\\partial \\vec{v_t}}\n",
        "$$\n",
        "\n",
        "Для обновления вектора $\\vec{W_{t+k}}$ используется правило:\n",
        "\n",
        "$$\n",
        "\\vec{W_{t+k}} = \\vec{W_{t+k}} - \\eta \\frac{\\partial L}{\\partial \\vec{W_{t+k}}}\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $\\eta$ — коэффициент обучения (learning rate), который контролирует шаг обновления.\n",
        "\n",
        "#### Шаг 3: Повторение процесса\n",
        "\n",
        "Этот процесс повторяется для всех обучающих примеров в наборе данных:\n",
        "1. Для каждого центрального слова $w_t$ в контексте вычисляются вероятности контекстных слов с помощью модели.\n",
        "2. На основе этих вероятностей вычисляются ошибки предсказания.\n",
        "3. Затем обновляются параметры модели (вектора слов).\n",
        "\n",
        "#### Шаг 4: Использование отрицательных выборок (Negative Sampling)\n",
        "\n",
        "Для ускорения обучения на больших словарях и объемных данных применяется метод **отрицательной выборки (Negative Sampling)**. Вместо того чтобы обновлять параметры для всех слов в словаре, обновляются только параметры для целевого и нескольких случайно выбранных контекстных слов (положительных примеров) и их случайных отрицательных примеров. Это значительно снижает вычислительные затраты.\n",
        "\n",
        "### Различия между CBOW и Skip-Gram\n",
        "\n",
        "| **Особенность**        | **CBOW (Continuous Bag of Words)**         | **Skip-Gram**                    |\n",
        "||-|-|\n",
        "| **Цель**               | Предсказать центральное слово на основе контекста | Предсказать контекстные слова на основе центрального слова |\n",
        "| **Вход**               | Контекстные слова (около центрального слова) | Центральное слово               |\n",
        "| **Выход**              | Центральное слово                        | Контекстные слова                |\n",
        "| **Подходит для**       | Часто встречающихся слов                 | Редких слов                      |\n",
        "| **Процесс обучения**   | Быстрее для больших объемов данных, поскольку предсказание одного слова связано с множеством контекстов | Может требовать больше времени для редких слов, поскольку для каждого целевого слова нужно предсказать несколько контекстных слов |\n",
        "| **Применение**         | Подходит для задач, где важен контекст для предсказания центрального слова (например, в задачах классификации) | Подходит для задач, где необходимо лучше учитывать редкие слова или для предсказания векторов для каждого слова в контексте |\n",
        "\n",
        "Таким образом, метод **Skip-Gram** эффективен для работы с редкими словами, так как модель учится предсказывать контекстные слова, используя центральное слово. Это позволяет улучшить качество векторных представлений для таких слов. В отличие от **CBOW**, метод **Skip-Gram** более затратен по времени, но дает лучшие результаты для редких слов.\n",
        "\n",
        "\n",
        "\n",
        "### Пример с расчетами для Skip-Gram\n",
        "\n",
        "#### 1. Заданные данные:\n",
        "\n",
        "Предположим, у нас есть текст:\n",
        "\n",
        "```\n",
        "The cat sat on the mat.\n",
        "```\n",
        "\n",
        "Мы будем использовать **Skip-Gram** для предсказания контекстных слов для центрального слова **\"sat\"**.\n",
        "\n",
        "- Центральное слово: **\"sat\"**.\n",
        "- Контекстные слова: **\"The\"**, **\"cat\"**, **\"on\"**, **\"the\"**, **\"mat\"**. (Контекстное окно размера 2, т.е. два слова слева и два справа от центрального).\n",
        "\n",
        "### 2. Векторные представления слов (word embeddings)\n",
        "\n",
        "Предположим, что векторные представления слов ($\\vec{v_t}$ и $\\vec{W}$) — это 3-мерные векторы. Пусть наши вектора слов следующие:\n",
        "\n",
        "- $\\vec{v_{\\text{sat}}} = [0.5, 0.1, 0.4]$\n",
        "- $\\vec{W_{\\text{The}}} = [0.3, 0.2, 0.1]$\n",
        "- $\\vec{W_{\\text{cat}}} = [0.4, 0.5, 0.2]$\n",
        "- $\\vec{W_{\\text{on}}} = [0.6, 0.1, 0.3]$\n",
        "- $\\vec{W_{\\text{mat}}} = [0.2, 0.3, 0.7]$\n",
        "\n",
        "Здесь $\\vec{v_{\\text{sat}}}$ — это вектор для центрального слова **\"sat\"**, а $\\vec{W_{\\text{*}}}$ — это векторы для контекстных слов. Мы будем использовать эти данные для расчета функции потерь и градиентов.\n",
        "\n",
        "### 3. Расчет вероятностей контекстных слов с помощью softmax\n",
        "\n",
        "Для вычисления вероятности появления контекстных слов, основываясь на центральном слове, используем **softmax**. Для этого сначала вычислим скалярные произведения векторов центрального и контекстных слов:\n",
        "\n",
        "#### Для контекстного слова \"The\":\n",
        "\n",
        "$$\n",
        "\\vec{v_{\\text{sat}}}^T \\vec{W_{\\text{The}}} = (0.5 \\times 0.3) + (0.1 \\times 0.2) + (0.4 \\times 0.1) = 0.15 + 0.02 + 0.04 = 0.21\n",
        "$$\n",
        "\n",
        "#### Для контекстного слова \"cat\":\n",
        "\n",
        "$$\n",
        "\\vec{v_{\\text{sat}}}^T \\vec{W_{\\text{cat}}} = (0.5 \\times 0.4) + (0.1 \\times 0.5) + (0.4 \\times 0.2) = 0.20 + 0.05 + 0.08 = 0.33\n",
        "$$\n",
        "\n",
        "#### Для контекстного слова \"on\":\n",
        "\n",
        "$$\n",
        "\\vec{v_{\\text{sat}}}^T \\vec{W_{\\text{on}}} = (0.5 \\times 0.6) + (0.1 \\times 0.1) + (0.4 \\times 0.3) = 0.30 + 0.01 + 0.12 = 0.43\n",
        "$$\n",
        "\n",
        "#### Для контекстного слова \"mat\":\n",
        "\n",
        "$$\n",
        "\\vec{v_{\\text{sat}}}^T \\vec{W_{\\text{mat}}} = (0.5 \\times 0.2) + (0.1 \\times 0.3) + (0.4 \\times 0.7) = 0.10 + 0.03 + 0.28 = 0.41\n",
        "$$\n",
        "\n",
        "Теперь применим **softmax** для этих значений, чтобы получить вероятности для каждого контекстного слова. Формула **softmax** для каждого контекстного слова $w_{t+k}$ выглядит так:\n",
        "\n",
        "$$\n",
        "P(w_{t+k} | w_t) = \\frac{\\exp(\\vec{v_t}^T \\vec{W_{t+k}})}{\\sum_{w=1}^{V} \\exp(\\vec{v_t}^T \\vec{W_w})}\n",
        "$$\n",
        "\n",
        "Для начала, найдем сумму экспонент:\n",
        "\n",
        "$$\n",
        "\\sum_{w=1}^{V} \\exp(\\vec{v_t}^T \\vec{W_w}) = \\exp(0.21) + \\exp(0.33) + \\exp(0.43) + \\exp(0.41)\n",
        "$$\n",
        "$$\n",
        "= 1.233 + 1.393 + 1.537 + 1.509 = 5.672\n",
        "$$\n",
        "\n",
        "Теперь вычислим вероятности для каждого контекстного слова:\n",
        "\n",
        "#### Для \"The\":\n",
        "\n",
        "$$\n",
        "P(\\text{The} | \\text{sat}) = \\frac{\\exp(0.21)}{5.672} = \\frac{1.233}{5.672} = 0.217\n",
        "$$\n",
        "\n",
        "#### Для \"cat\":\n",
        "\n",
        "$$\n",
        "P(\\text{cat} | \\text{sat}) = \\frac{\\exp(0.33)}{5.672} = \\frac{1.393}{5.672} = 0.246\n",
        "$$\n",
        "\n",
        "#### Для \"on\":\n",
        "\n",
        "$$\n",
        "P(\\text{on} | \\text{sat}) = \\frac{\\exp(0.43)}{5.672} = \\frac{1.537}{5.672} = 0.271\n",
        "$$\n",
        "\n",
        "#### Для \"mat\":\n",
        "\n",
        "$$\n",
        "P(\\text{mat} | \\text{sat}) = \\frac{\\exp(0.41)}{5.672} = \\frac{1.509}{5.672} = 0.266\n",
        "$$\n",
        "\n",
        "### 4. Функция потерь\n",
        "\n",
        "Теперь, используя вычисленные вероятности, можем вычислить функцию потерь для всей выборки контекстных слов. Функция потерь $L$ для модели **Skip-Gram** выражается как сумма логарифмов вероятностей предсказанных контекстных слов:\n",
        "\n",
        "$$\n",
        "L = - \\sum_{w_{t+k} \\in C} \\log P(w_{t+k} | w_t)\n",
        "$$\n",
        "\n",
        "Поскольку в нашем контексте $C = \\{\\text{The}, \\text{cat}, \\text{on}, \\text{mat}\\}$, получаем:\n",
        "\n",
        "$$\n",
        "L = - [\\log(0.217) + \\log(0.246) + \\log(0.271) + \\log(0.266)]\n",
        "$$\n",
        "$$\n",
        "L = - [-1.516 - 1.406 - 1.308 - 1.322]\n",
        "$$\n",
        "$$\n",
        "L = 5.552\n",
        "$$\n",
        "\n",
        "Таким образом, функция потерь для данной выборки равна **5.552**.\n",
        "\n",
        "### 5. Обновление параметров модели\n",
        "\n",
        "Теперь, чтобы минимизировать функцию потерь, можно использовать метод стохастического градиентного спуска (SGD) для обновления векторов слов. Сначала вычислим градиенты функции потерь по параметрам модели. Градиенты будут вычисляться для каждого контекстного слова, используя производные функции потерь по отношению к вектору целевого слова и вектору контекстного слова.\n",
        "\n",
        "#### Градиенты для векторов:\n",
        "\n",
        "Для каждого контекстного слова $w_{t+k}$, обновление параметров можно выразить следующим образом:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial \\vec{v_t}} = - \\sum_{w_{t+k} \\in C} \\left( \\frac{1}{P(w_{t+k} | w_t)} \\cdot \\vec{W_{t+k}} \\right)\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial \\vec{W_{t+k}}} = - \\frac{1}{P(w_{t+k} | w_t)} \\cdot \\vec{v_t}\n",
        "$$\n",
        "\n",
        "Эти градиенты можно использовать для обновления параметров с помощью шага обучения $\\eta$:\n",
        "\n",
        "$$\n",
        "\\vec{v_t} \\leftarrow \\vec{v_t} - \\eta \\frac{\\partial L}{\\partial \\vec{v_t}}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\vec{W_{t+k}} \\leftarrow \\vec{W_{t+k}} - \\eta \\frac{\\partial L}{\\partial \\vec{W_{t+k}}}\n",
        "$$\n",
        "\n",
        "Таким образом, мы рассмотрели полный процесс работы метода **Skip-Gram**:\n",
        "\n",
        "1. Расчет вероятностей контекстных слов с помощью **softmax**,\n",
        "2. Вычисление функции потерь,\n",
        "3. Применение стохастического градиентного спуска для обновления параметров модели.\n",
        "\n",
        "Этот процесс позволяет обучать модели для создания качественных векторных представлений слов, которые могут быть использованы в различных задачах обработки естественного языка.\n",
        "\n",
        "\n",
        "\n",
        "Ниже приведен пример реализации модели Skip-Gram на Python, включая вычисления для softmax, функции потерь и обновления параметров с использованием стохастического градиентного спуска (SGD).\n",
        "\n",
        "Реализация модели Skip-Gram на Python\n"
      ],
      "metadata": {
        "id": "i-9LElw5I7Q2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# 1. Заданные данные\n",
        "\n",
        "# Центральное слово: \"sat\"\n",
        "# Контекстные слова: \"The\", \"cat\", \"on\", \"the\", \"mat\"\n",
        "context_words = [\"The\", \"cat\", \"on\", \"The\", \"mat\"]  # Ensure consistency with capitalization\n",
        "\n",
        "# Векторные представления слов\n",
        "v_sat = np.array([0.5, 0.1, 0.4])  # Вектор центрального слова \"sat\"\n",
        "W_the = np.array([0.3, 0.2, 0.1])\n",
        "W_cat = np.array([0.4, 0.5, 0.2])\n",
        "W_on = np.array([0.6, 0.1, 0.3])\n",
        "W_mat = np.array([0.2, 0.3, 0.7])\n",
        "\n",
        "# Контекстные слова в виде векторов\n",
        "word_vectors = {\n",
        "    \"The\": W_the,\n",
        "    \"cat\": W_cat,\n",
        "    \"on\": W_on,\n",
        "    \"mat\": W_mat\n",
        "}\n",
        "\n",
        "# 2. Функция softmax\n",
        "def softmax(scores):\n",
        "    exp_scores = np.exp(scores - np.max(scores))  # Numerical stability fix\n",
        "    return exp_scores / np.sum(exp_scores)\n",
        "\n",
        "# 3. Функция потерь\n",
        "def compute_loss(probabilities, context_words):\n",
        "    # Логарифмическая функция потерь\n",
        "    loss = -sum(np.log(probabilities[word]) for word in context_words)\n",
        "    return loss\n",
        "\n",
        "# 4. Градиенты для обновления параметров\n",
        "def compute_gradients(v_t, word_vectors, probabilities, context_words):\n",
        "    grad_v_t = np.zeros_like(v_t)  # Градиент для вектора центрального слова\n",
        "    grad_word_vectors = {word: np.zeros_like(word_vectors[word]) for word in word_vectors}  # Градиенты для векторов контекстных слов\n",
        "\n",
        "    for word in context_words:\n",
        "        grad_factor = 1 / probabilities[word]  # Множитель градиента\n",
        "        grad_v_t -= grad_factor * word_vectors[word]\n",
        "        grad_word_vectors[word] -= grad_factor * v_t\n",
        "\n",
        "    return grad_v_t, grad_word_vectors\n",
        "\n",
        "# 5. Обновление параметров с использованием SGD\n",
        "def update_parameters(v_t, word_vectors, grad_v_t, grad_word_vectors, learning_rate=0.1):\n",
        "    v_t -= learning_rate * grad_v_t\n",
        "    for word in word_vectors:\n",
        "        word_vectors[word] -= learning_rate * grad_word_vectors[word]\n",
        "\n",
        "    return v_t, word_vectors\n",
        "\n",
        "# 6. Пример шага обучения\n",
        "\n",
        "# Скалярные произведения центрального слова с контекстными словами\n",
        "scores = {word: np.dot(v_sat, word_vectors[word]) for word in context_words}\n",
        "\n",
        "# Применяем softmax и сохраняем результаты в словарь\n",
        "softmax_probs = softmax(list(scores.values()))\n",
        "probabilities = {word: prob for word, prob in zip(context_words, softmax_probs)}\n",
        "\n",
        "# Вычисляем функцию потерь\n",
        "loss = compute_loss(probabilities, context_words)\n",
        "print(\"Initial loss:\", loss)\n",
        "\n",
        "# Вычисляем градиенты\n",
        "grad_v_t, grad_word_vectors = compute_gradients(v_sat, word_vectors, probabilities, context_words)\n",
        "\n",
        "# Обновляем параметры с помощью SGD\n",
        "v_sat, word_vectors = update_parameters(v_sat, word_vectors, grad_v_t, grad_word_vectors)\n",
        "\n",
        "# Выводим новые значения\n",
        "print(\"\\nUpdated central word vector (v_sat):\", v_sat)\n",
        "for word, vec in word_vectors.items():\n",
        "    print(f\"Updated vector for '{word}':\", vec)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "OUu9aJRyI8FW",
        "outputId": "04e05be5-5345-47f3-b6da-0f54481b799b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'mat'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-4ee374bb2dc2>\u001b[0m in \u001b[0;36m<cell line: 65>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;31m# Вычисляем функцию потерь\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobabilities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Initial loss:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-4ee374bb2dc2>\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(probabilities, context_words)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobabilities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m# Логарифмическая функция потерь\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobabilities\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcontext_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-4ee374bb2dc2>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobabilities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m# Логарифмическая функция потерь\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobabilities\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcontext_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'mat'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Для реализации модели Skip-Gram с использованием готовых библиотек в Python, можно воспользоваться такими популярными инструментами, как Gensim или PyTorch. В данном случае мы сосредоточимся на библиотеке Gensim, которая является одним из самых популярных инструментов для обучения векторных представлений слов (word embeddings).\n",
        "\n",
        "Реализация модели Skip-Gram с использованием Gensim"
      ],
      "metadata": {
        "id": "cqjqb0djJEUQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "import logging\n",
        "\n",
        "# Включаем логирование для отслеживания процесса обучения модели\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "\n",
        "# Подготовим данные (массив слов из текста)\n",
        "sentences = [\n",
        "    [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
        "]\n",
        "\n",
        "# Обучаем модель Skip-Gram\n",
        "# С параметром sg=1 мы указываем, что используем модель Skip-Gram (если sg=0, используется CBOW)\n",
        "model = Word2Vec(sentences, vector_size=3, window=2, sg=1, min_count=1)\n",
        "\n",
        "# Проверим, как выглядит вектор для слова \"sat\"\n",
        "vector_sat = model.wv[\"sat\"]\n",
        "print(\"Vector for 'sat':\", vector_sat)\n",
        "\n",
        "# Теперь мы можем найти наиболее похожие слова для центрального слова \"sat\"\n",
        "similar_words = model.wv.most_similar(\"sat\", topn=3)\n",
        "print(\"Most similar words to 'sat':\", similar_words)\n",
        "\n",
        "# Сохраним модель\n",
        "model.save(\"skipgram_model.model\")\n",
        "\n",
        "# Загрузим модель (если нужно)\n",
        "# model = Word2Vec.load(\"skipgram_model.model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZ_NgvEwJJht",
        "outputId": "d597831c-4557-4f32-d277-8152f9555aae"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector for 'sat': [-0.12544572  0.24601682 -0.05111571]\n",
            "Most similar words to 'sat': [('cat', 0.9267787337303162), ('on', 0.4850040376186371), ('the', -0.09387321770191193)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Оптимизация и обучение моделей Word2Vec (CBOW и Skip-Gram)\n",
        "\n",
        "Для эффективного обучения моделей **CBOW (Continuous Bag of Words)** и **Skip-Gram** часто применяются методы, позволяющие значительно ускорить вычисления и снизить вычислительные затраты. Одними из таких методов являются **Negative Sampling** и **Hierarchical Softmax**. Эти методы решают проблему вычислительной сложности, связанной с необходимостью вычислять вероятности для всех слов из словаря, что может быть чрезвычайно ресурсоемким, особенно для больших словарей.\n",
        "\n",
        "#### Negative Sampling\n",
        "\n",
        "**Negative Sampling** — это метод, который используется для ускорения обучения моделей Word2Vec, особенно в контексте Skip-Gram модели. Вместо того чтобы вычислять вероятность всех слов в словаре через прямой softmax (что требует обработки огромного числа слов), этот метод фокусируется на выборке только нескольких слов для обновления весов модели. Суть метода заключается в следующем:\n",
        "\n",
        "1. **Основная идея**: Вместо того, чтобы рассчитывать вероятность для каждого слова в словаре, изначально выбираются несколько случайных слов, которые не связаны с текущим контекстом (так называемые \"отрицательные\" примеры).\n",
        "   \n",
        "2. **Процесс**:\n",
        "    - Для каждого положительного примера (слово из контекста), выбираются несколько отрицательных слов, которые не имеют отношения к текущему контексту.\n",
        "    - Модель учится различать положительные примеры (слова, которые реально встречаются в контексте) и отрицательные (слова, случайным образом выбранные из словаря).\n",
        "    - Обучение сводится к задаче бинарной классификации для каждой пары (контекст, слово). Это значительно снижает вычислительные затраты, так как требуется обновить параметры только для небольшой части слов из словаря, а не для всего словаря.\n",
        "\n",
        "3. **Преимущества**:\n",
        "   - **Скорость**: Значительно уменьшается количество операций, поскольку теперь не нужно вычислять полные вероятности для всех слов в словаре. Вместо этого только несколько слов подвергаются изменениям за один шаг.\n",
        "   - **Эффективность памяти**: Обновляются веса только для небольшого подмножества слов, что позволяет эффективно использовать память, даже при работе с большими словарями.\n",
        "\n",
        "4. **Алгоритм**:\n",
        "    - Для каждого целевого слова и контекстных слов в Skip-Gram модели (или наоборот в CBOW) модель предсказывает вероятность появления контекстных слов.\n",
        "    - Вместо того чтобы вычислять полное распределение вероятностей с помощью softmax, выбирается несколько негативных примеров.\n",
        "    - Каждое слово представляется в виде вектора, и задача модели — научиться правильно различать контекстные и случайные слова, минимизируя ошибку в предсказаниях.\n",
        "\n",
        "5. **Проблемы**: Выбор \"отрицательных\" слов не всегда прост, так как необходимо подобрать такие слова, которые не связаны с контекстом, но в то же время достаточно часто встречаются в языке. Часто используются **удельные вероятности** для выборки отрицательных примеров, что позволяет сбалансировать процесс.\n",
        "\n",
        "6. **Математика**: Для каждой пары контекстного слова $w_c$ и отрицательного слова $w_n$, задача модели сводится к минимизации функции потерь (например, бинарной кросс-энтропии):\n",
        "$$\n",
        "   L = - \\log \\sigma(v_{w_c} \\cdot v_{w_o}) - \\sum_{n=1}^{k} \\log \\sigma(-v_{w_n} \\cdot v_{w_o})\n",
        "$$\n",
        "   где $\\sigma$ — это сигмоидная функция, $v_{w_c}$ и $v_{w_n}$ — это векторные представления слов, $k$ — количество отрицательных примеров.\n",
        "\n",
        "#### Hierarchical Softmax\n",
        "\n",
        "**Hierarchical Softmax** — это еще один способ ускорения вычислений, использующий иерархическую структуру для вычисления вероятности. В отличие от прямого softmax, который требует вычисления вероятностей для всех слов в словаре, иерархический softmax использует дерево для представления распределений вероятности. В этом случае задача сводится к вычислению вероятностей вдоль пути от корня дерева до целевого слова.\n",
        "\n",
        "1. **Структура дерева**: Вместо того чтобы учитывать каждое слово из словаря, все слова представляются в виде дерева. Каждое слово имеет свой путь к корню дерева.\n",
        "   \n",
        "2. **Алгоритм**:\n",
        "   - Для каждого слова вычисляется вероятность путем прохождения от корня дерева к этому слову, вычисляя вероятность на каждом шаге, используя сигмоидную функцию для каждой ветви.\n",
        "   - Это требует вычисления логарифма вероятностей только для узлов, которые лежат на пути от корня к целевому слову.\n",
        "\n",
        "3. **Преимущества**:\n",
        "   - Вместо вычисления вероятности для всех слов из словаря, модель вычисляет вероятность, используя гораздо меньшее количество операций — только $O(\\log |V|)$ вычислений для каждого слова (где $|V|$ — размер словаря).\n",
        "   - Это сильно ускоряет процесс обучения, особенно при работе с большими словарями.\n",
        "\n",
        "4. **Проблемы**: Несмотря на более низкие затраты времени, этот метод также не всегда подходит для всех типов данных, поскольку производительность зависит от структуры дерева и способа его построения.\n",
        "\n",
        "#### Сравнение Negative Sampling и Hierarchical Softmax\n",
        "\n",
        "- **Negative Sampling** лучше подходит, когда модель должна быстро учиться на основе меньшего числа примеров. Он снижает вычислительные затраты, сводя задачу к бинарной классификации для каждого контекста.\n",
        "- **Hierarchical Softmax** — это более элегантный способ оптимизации, который использует иерархию, но может быть менее интуитивно понятен и труднее настраиваем.\n",
        "\n",
        "Оба метода имеют свои применения и могут использоваться в зависимости от контекста и особенностей задачи, при этом каждый из них значительно ускоряет обучение моделей Word2Vec, позволяя работать с большими объемами данных без необходимости вычислять полные вероятности для каждого слова.\n",
        "\n",
        "\n",
        "### Пример оптимизации и обучения моделей Word2Vec с использованием Negative Sampling и Hierarchical Softmax\n",
        "\n",
        "Предположим, у нас есть текстовый корпус, состоящий из нескольких предложений, и мы хотим обучить модель Word2Vec для получения векторных представлений слов. Рассмотрим два способа оптимизации: **Negative Sampling** и **Hierarchical Softmax**.\n",
        "\n",
        "#### 1. Исходные данные:\n",
        "Предположим, что у нас есть небольшой корпус:\n",
        "```\n",
        "\"кошки и собаки любят гулять\"\n",
        "```\n",
        "\n",
        "Словарь будет включать только уникальные слова:\n",
        "```\n",
        "[\"кошки\", \"и\", \"собаки\", \"любят\", \"гулять\"]\n",
        "```\n",
        "Размер словаря $|V| = 5$.\n",
        "\n",
        "#### 2. Модель Word2Vec\n",
        "Для этого примера предположим, что мы обучаем модель **Skip-Gram**. В этой модели цель — предсказать контекстные слова для заданного целевого слова. Например, для целевого слова \"собаки\" контекстными словами будут \"кошки\", \"и\", \"любят\", \"гулять\".\n",
        "\n",
        "##### 2.1. **Negative Sampling**\n",
        "Вместо того чтобы вычислять вероятность всех слов в словаре с помощью обычного softmax, мы используем **Negative Sampling**. Допустим, мы выбираем 2 отрицательных примера для каждого положительного контекста.\n",
        "\n",
        "**Шаги обучения с Negative Sampling**:\n",
        "1. **Положительные примеры**: для целевого слова \"собаки\" контекстными словами будут \"кошки\", \"и\", \"любят\", \"гулять\".\n",
        "2. **Отрицательные примеры**: выбираем случайные слова, не связанные с \"собаки\" (например, \"кошки\", \"и\" — это два случайных слова, которые не относятся к контексту).\n",
        "3. Мы обучаем модель различать контекстные и случайные слова, минимизируя функцию потерь.\n",
        "\n",
        "Математически это выглядит так:\n",
        "\n",
        "Для пары контекстного слова $w_c$ и $k$ отрицательных слов $w_n$:\n",
        "$$\n",
        "L = - \\log \\sigma(v_{w_c} \\cdot v_{w_o}) - \\sum_{n=1}^{k} \\log \\sigma(-v_{w_n} \\cdot v_{w_o})\n",
        "$$\n",
        "где:\n",
        "- $\\sigma$ — это сигмоидная функция.\n",
        "- $v_{w_c}$, $v_{w_o}$, и $v_{w_n}$ — векторные представления контекстного слова, целевого слова и отрицательных слов соответственно.\n",
        "- $k$ — количество отрицательных примеров (в нашем случае $k = 2$).\n",
        "\n",
        "Таким образом, для каждого контекстного слова \"собаки\" мы обновляем веса только для нескольких выбранных слов, а не для всех 5 слов в словаре, что ускоряет обучение.\n",
        "\n",
        "##### 2.2. **Hierarchical Softmax**\n",
        "**Hierarchical Softmax** использует иерархическую структуру для вычисления вероятности. Все слова из словаря представляются как листья в бинарном дереве.\n",
        "\n",
        "**Шаги обучения с Hierarchical Softmax**:\n",
        "1. Строим бинарное дерево для словаря. Например, если у нас 5 слов, то дерево может выглядеть следующим образом:\n",
        "   - Корень дерева имеет два поддерева (например, \"кошки\" и \"собаки\").\n",
        "   - Листья дерева — это сами слова.\n",
        "\n",
        "2. Для целевого слова \"собаки\" мы проходим по пути от корня до этого слова, обновляя веса на каждом шаге.\n",
        "\n",
        "Математически для вычисления вероятности слова $w_o$ при использовании иерархического softmax:\n",
        "$$\n",
        "P(w_o) = \\prod_{i=1}^{L} \\sigma(h_i \\cdot v_{w_o})\n",
        "$$\n",
        "где:\n",
        "- $h_i$ — скрытые векторы на пути от корня до слова $w_o$,\n",
        "- $\\sigma$ — сигмоидная функция.\n",
        "\n",
        "Для каждого слова достаточно пройти по дереву (что требует $O(\\log |V|)$ вычислений), что значительно сокращает вычислительные затраты по сравнению с обычным softmax.\n",
        "\n",
        "#### 3. Сравнение двух методов:\n",
        "- **Negative Sampling** — использует случайные отрицательные примеры, что позволяет уменьшить количество обновляемых параметров и значительно ускорить обучение. Однако выбор отрицательных примеров может быть сложным.\n",
        "- **Hierarchical Softmax** — использует иерархию, что снижает количество вычислений для каждого слова. Однако процесс построения и оптимизации дерева может быть сложным.\n",
        "\n",
        "#### 4. Заключение:\n",
        "- **Negative Sampling** лучше подходит, если важно быстро обучать модель с небольшим количеством положительных примеров.\n",
        "- **Hierarchical Softmax** более эффективен при больших словарях, так как количество операций для вычисления вероятности для каждого слова зависит от глубины дерева.\n",
        "\n",
        "Оба метода значимо ускоряют процесс обучения, позволяя работать с большими объемами данных без вычисления полной вероятности для каждого слова.\n",
        "\n",
        "\n",
        "Для реализации методов **Negative Sampling** и **Hierarchical Softmax** на Python, мы начнем с реализации простого варианта модели **Skip-Gram** с использованием данных текстов и создадим соответствующие алгоритмы для обоих методов оптимизации.\n",
        "\n",
        "### Шаг 1: Подготовка данных\n",
        "Создадим простой корпус текста и подготовим данные для обучения.\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import random\n",
        "\n",
        "# Исходный текстовый корпус\n",
        "corpus = [\"кошки и собаки любят гулять\"]\n",
        "\n",
        "# Токенизация текста\n",
        "def tokenize(text):\n",
        "    return text.lower().split()\n",
        "\n",
        "tokens = tokenize(corpus[0])\n",
        "\n",
        "# Создание словаря и индексации\n",
        "word_counts = Counter(tokens)\n",
        "vocab = list(word_counts.keys())\n",
        "word_to_index = {word: i for i, word in enumerate(vocab)}\n",
        "index_to_word = {i: word for i, word in enumerate(vocab)}\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "print(\"Словарь:\", vocab)\n",
        "print(\"Индексы слов:\", word_to_index)\n",
        "```\n",
        "\n",
        "### Шаг 2: Реализация модели Skip-Gram с **Negative Sampling**\n",
        "\n",
        "Мы реализуем функцию для обучения модели **Skip-Gram** с методом **Negative Sampling**. Для этого будем использовать векторные представления слов, которые будем обновлять на основе контекстных слов и отрицательных примеров.\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Параметры модели\n",
        "embedding_dim = 5  # Размерность векторных представлений\n",
        "negative_samples = 2  # Количество отрицательных примеров\n",
        "\n",
        "# Инициализация векторов слов\n",
        "class SkipGramNegativeSampling(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(SkipGramNegativeSampling, self).__init__()\n",
        "        self.in_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.out_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.in_embeddings.weight.data.uniform_(-0.5, 0.5)\n",
        "        self.out_embeddings.weight.data.uniform_(-0.5, 0.5)\n",
        "\n",
        "    def forward(self, target, context, negative_samples):\n",
        "        target_emb = self.in_embeddings(target)\n",
        "        context_emb = self.out_embeddings(context)\n",
        "        negative_emb = self.out_embeddings(negative_samples)\n",
        "        \n",
        "        positive_score = torch.sum(target_emb * context_emb, dim=1)\n",
        "        negative_score = torch.sum(target_emb * negative_emb, dim=2)\n",
        "        \n",
        "        positive_loss = torch.sum(torch.log(torch.sigmoid(positive_score)))\n",
        "        negative_loss = torch.sum(torch.log(torch.sigmoid(-negative_score)), dim=1)\n",
        "        \n",
        "        return -(positive_loss + negative_loss)\n",
        "\n",
        "# Инициализация модели и оптимизатора\n",
        "model = SkipGramNegativeSampling(vocab_size, embedding_dim)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# Функция для выборки отрицательных примеров\n",
        "def get_negative_samples(target_idx, num_samples, vocab_size):\n",
        "    negatives = []\n",
        "    while len(negatives) < num_samples:\n",
        "        negative = random.randint(0, vocab_size - 1)\n",
        "        if negative != target_idx:\n",
        "            negatives.append(negative)\n",
        "    return torch.tensor(negatives)\n",
        "\n",
        "# Обучение модели\n",
        "epochs = 1000\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    for i in range(1, len(tokens) - 1):\n",
        "        target_word = tokens[i]\n",
        "        context_words = [tokens[i-1], tokens[i+1]]  # контекстные слова (по одному слева и справа)\n",
        "        \n",
        "        target_idx = word_to_index[target_word]\n",
        "        context_idx = [word_to_index[word] for word in context_words]\n",
        "        \n",
        "        # Выборка отрицательных примеров\n",
        "        negative_samples = get_negative_samples(target_idx, negative_samples, vocab_size)\n",
        "        \n",
        "        # Преобразование индексов в тензоры\n",
        "        target_tensor = torch.tensor([target_idx])\n",
        "        context_tensor = torch.tensor(context_idx)\n",
        "        \n",
        "        # Обратное распространение ошибки\n",
        "        optimizer.zero_grad()\n",
        "        loss = model(target_tensor, context_tensor, negative_samples)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Эпоха {epoch}, Потери: {total_loss}\")\n",
        "\n",
        "# Получение обученных векторов слов\n",
        "word_vectors = model.in_embeddings.weight.data.numpy()\n",
        "print(\"Векторное представление слова 'собаки':\", word_vectors[word_to_index[\"собаки\"]])\n",
        "```\n",
        "\n",
        "### Шаг 3: Реализация **Hierarchical Softmax**\n",
        "\n",
        "Теперь мы реализуем метод **Hierarchical Softmax**, который будет использовать бинарное дерево для вычисления вероятности.\n",
        "\n",
        "```python\n",
        "class SkipGramHierarchicalSoftmax(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(SkipGramHierarchicalSoftmax, self).__init__()\n",
        "        self.in_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.out_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.in_embeddings.weight.data.uniform_(-0.5, 0.5)\n",
        "        self.out_embeddings.weight.data.uniform_(-0.5, 0.5)\n",
        "\n",
        "    def forward(self, target, context, huffman_tree):\n",
        "        target_emb = self.in_embeddings(target)\n",
        "        context_emb = self.out_embeddings(context)\n",
        "        \n",
        "        # Прохождение по дереву Хаффмана\n",
        "        path, labels = huffman_tree[target]\n",
        "        \n",
        "        loss = 0\n",
        "        for p, label in zip(path, labels):\n",
        "            loss += torch.log(torch.sigmoid(torch.sum(target_emb * p * context_emb)))\n",
        "            loss += torch.log(torch.sigmoid(-torch.sum(target_emb * p * context_emb)))\n",
        "        return -loss\n",
        "\n",
        "# Пример инициализации и обучения с использованием Hierarchical Softmax\n",
        "model_hs = SkipGramHierarchicalSoftmax(vocab_size, embedding_dim)\n",
        "optimizer_hs = optim.SGD(model_hs.parameters(), lr=0.01)\n",
        "\n",
        "# Псевдокод для дерева Хаффмана (реальная реализация потребует построения дерева Хаффмана)\n",
        "# Для простоты будем считать, что дерево уже готово\n",
        "# (В реальной задаче потребуется построить дерево Хаффмана)\n",
        "huffman_tree = {word: (path, labels) for word, path, labels in zip(vocab, tree_paths, tree_labels)}\n",
        "\n",
        "# Обучение модели с иерархическим softmax\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    for i in range(1, len(tokens) - 1):\n",
        "        target_word = tokens[i]\n",
        "        context_words = [tokens[i-1], tokens[i+1]]\n",
        "        \n",
        "        target_idx = word_to_index[target_word]\n",
        "        context_idx = [word_to_index[word] for word in context_words]\n",
        "        \n",
        "        target_tensor = torch.tensor([target_idx])\n",
        "        context_tensor = torch.tensor(context_idx)\n",
        "        \n",
        "        optimizer_hs.zero_grad()\n",
        "        loss = model_hs(target_tensor, context_tensor, huffman_tree)\n",
        "        loss.backward()\n",
        "        optimizer_hs.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Эпоха {epoch}, Потери: {total_loss}\")\n",
        "```\n",
        "\n",
        "Таким образом, в этом примере мы показали, как реализовать **Skip-Gram** с методами **Negative Sampling** и **Hierarchical Softmax** на Python, используя PyTorch. В модели с **Negative Sampling** обучение ускоряется за счет того, что обновляются веса только для небольшого набора слов, а в случае **Hierarchical Softmax** используется бинарное дерево для вычисления вероятностей, что также помогает уменьшить вычислительные затраты.\n",
        "\n",
        "\n",
        "Для реализации методов Negative Sampling и Hierarchical Softmax с использованием готовых библиотек, мы можем воспользоваться библиотекой Gensim, которая предоставляет мощные инструменты для работы с моделями Word2Vec, включая оптимизации, такие как Negative Sampling и Hierarchical Softmax.\n",
        "\n"
      ],
      "metadata": {
        "id": "afTxehcmJMJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Преимущества Word2Vec\n",
        "\n",
        "1. **Эмбеддинги слов**: Модели обучают слова в виде плотных векторов, которые могут быть использованы для многих задач NLP.\n",
        "2. **Семантические и синтаксические связи**: Слова, которые часто встречаются в схожем контексте, имеют близкие векторные представления.\n",
        "3. **Эффективность**: Word2Vec может обучаться на больших текстовых корпусах за приемлемое время.\n",
        "\n",
        "Таким образом, модели Word2Vec (CBOW и Skip-Gram) являются мощным инструментом для создания векторных представлений слов, которые сохраняют контекстную и семантическую информацию. Хотя оба алгоритма используют нейронные сети для обучения, их различия в подходах к обработке контекста и предсказаниям центральных слов делают их полезными для различных типов задач.\n"
      ],
      "metadata": {
        "id": "vpOEYXxOJfD5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. \"Модель GloVe (Global Vectors for Word Representation)\"\n",
        "\n",
        "### Введение\n",
        "\n",
        "Модель **GloVe** (Global Vectors for Word Representation) — это алгоритм, предназначенный для обучения векторных представлений слов, который был предложен Стэнфордским университетом в 2014 году (Томми Шмидт, Кристофер Маннинг, и др.). Модель объединяет преимущества двух основных подходов к представлению слов: **контекстуальных моделей** (таких как Word2Vec) и **методов, использующих глобальную статистику** (таких как LSA — Latent Semantic Analysis). GloVe пытается воспользоваться глобальной статистикой корпуса текстов, чтобы понять семантическую близость между словами и представить эти слова в виде плотных векторов.\n",
        "\n",
        "## Проблема, решаемая GloVe\n",
        "\n",
        "Модели векторных представлений слов, такие как Word2Vec, пытаются учитывать локальные контексты слов, т.е. то, какие слова чаще всего окружают данное слово в пределах небольших контекстных окон. Однако такие методы не всегда могут захватить более глобальные зависимости между словами, такие как их частотное соотношение на глобальном уровне текста.\n",
        "\n",
        "Напротив, традиционные методы для выделения смысловых тем (например, LSA) используют матрицы со статистикой по всем парам слов в корпусе, но могут сталкиваться с проблемой \"разреженности\", когда такая матрица становится слишком большой и неинформативной.\n",
        "\n",
        "Модель GloVe сочетает в себе оба подхода и пытается найти компромисс: она использует глобальную статистику, но при этом обучает векторы слов с учётом их контекстов, чтобы получить компактные и информативные представления.\n",
        "\n",
        "## Основная идея модели GloVe\n",
        "\n",
        "Основной идеей GloVe является использование **коэффициентов совместной вероятности** для формирования векторных представлений слов. В отличие от Word2Vec, который обучает представления слов, минимизируя ошибки предсказания на основе локального контекста, GloVe минимизирует ошибку на основе глобальной статистики — именно коэффициентов совместной вероятности слов в корпусе.\n",
        "\n",
        "Предположим, у нас есть большой корпус текстов, из которого мы извлекаем совместные вероятности для пар слов. Эти вероятности описывают, насколько часто два слова встречаются вместе в контексте.\n",
        "\n",
        "Математически:\n",
        "\n",
        "- Пусть $X_{ij}$ — это количество совместных появлений слов $w_i$ и $w_j$ в корпусе.\n",
        "- Пусть $P_{ij}$ — это вероятность того, что $w_i$ и $w_j$ появляются в контексте, которая вычисляется как:\n",
        "  \n",
        "$$\n",
        "  P_{ij} = \\frac{X_{ij}}{\\sum_{j} X_{ij}}\n",
        "$$\n",
        "\n",
        "Модель GloVe строит представления слов $\\mathbf{v}_i$ и $\\mathbf{v}_j$ таким образом, чтобы для каждой пары слов $w_i$ и $w_j$ выполнялось следующее приближение для их совместной вероятности $P_{ij}$:\n",
        "\n",
        "$$\n",
        "f(P_{ij}) = \\mathbf{v}_i^T \\mathbf{v}_j + b_i + b_j\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $f(P_{ij})$ — это функция, которая трансформирует вероятность совместного появления слов в некую величину, которая соответствует логарифму вероятности или её приближению.\n",
        "- $\\mathbf{v}_i$ и $\\mathbf{v}_j$ — векторные представления слов $w_i$ и $w_j$.\n",
        "- $b_i$ и $b_j$ — это смещения (bias), связанные с каждым словом.\n",
        "\n",
        "Модель минимизирует ошибку на всех возможных парах слов в корпусе, используя следующую целевую функцию:\n",
        "\n",
        "$$\n",
        "J = \\sum_{i,j=1}^V f(P_{ij}) \\left( \\mathbf{v}_i^T \\mathbf{v}_j + b_i + b_j - \\log(P_{ij}) \\right)^2\n",
        "$$\n",
        "\n",
        "Здесь:\n",
        "- $V$ — размер словаря.\n",
        "- $P_{ij}$ — вероятность совместного появления слов $w_i$ и $w_j$, которую можно вычислить из статистики корпуса.\n",
        "\n",
        "Функция $f(P_{ij})$ обычно используется для сглаживания, так как для частых слов часто имеет смысл нормировать вероятность, а для редких — игнорировать слишком маленькие значения.\n",
        "\n",
        "### Построение векторных представлений слов\n",
        "\n",
        "Модель GloVe создает векторные представления слов, используя оптимизацию для минимизации целевой функции. Эти представления могут быть использованы в различных задачах NLP, таких как:\n",
        "- Классификация текста\n",
        "- Поиск с учетом семантической близости\n",
        "- Перевод с помощью машинного обучения\n",
        "\n",
        "Важно заметить, что векторные представления, полученные с помощью GloVe, захватывают такие семантические и синтаксические отношения между словами, как:\n",
        "- Синонимия (например, \"король\" и \"королева\")\n",
        "- Антонимия (например, \"большой\" и \"маленький\")\n",
        "- Операции с числовыми понятиями (например, \"король\" - \"мужчина\" + \"женщина\" = \"королева\")\n",
        "\n",
        "## Функция потерь и её модификации\n",
        "\n",
        "### Взвешивание ошибки\n",
        "\n",
        "Для того чтобы корректно обработать большое количество разреженных совместных вероятностей, в модели GloVe используется функция весов $f(P_{ij})$, которая помогает уменьшить влияние редких и малозначимых случаев. Один из вариантов такой функции — это **модифицированная** функция весов, которая учитывает только те пары слов, для которых $P_{ij}$ достаточно велико, например:\n",
        "\n",
        "$$\n",
        "f(P_{ij}) = \\frac{P_{ij}}{(P_{ij} + \\alpha)}\n",
        "$$\n",
        "\n",
        "где $\\alpha$ — это параметр, который контролирует, на сколько сильно редкие слова будут влиять на результат.\n",
        "\n",
        "### Формула ошибки\n",
        "\n",
        "\n",
        "Для вычисления градиентов целевой функции модели GloVe, которая минимизируется с использованием градиентного спуска, мы начнем с вывода частных производных целевой функции по векторами слов $\\mathbf{v}_i$, $\\mathbf{v}_j$, и смещениям $b_i$, $b_j$.\n",
        "\n",
        "Целевая функция GloVe выглядит следующим образом:\n",
        "\n",
        "$$\n",
        "J = \\sum_{i,j=1}^V f(P_{ij}) \\left( \\mathbf{v}_i^T \\mathbf{v}_j + b_i + b_j - \\log(P_{ij}) \\right)^2\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $P_{ij}$ — это частота совместного появления слов $w_i$ и $w_j$ в корпусе,\n",
        "- $f(P_{ij})$ — это весовая функция, которая зависит от значения $P_{ij}$,\n",
        "- $\\mathbf{v}_i$ и $\\mathbf{v}_j$ — это векторные представления слов $w_i$ и $w_j$,\n",
        "- $b_i$ и $b_j$ — смещения для слов $w_i$ и $w_j$.\n",
        "\n",
        "### Обозначения:\n",
        "- $X_{ij} = \\mathbf{v}_i^T \\mathbf{v}_j + b_i + b_j$ — предсказанная моделью величина совместного появления слов $w_i$ и $w_j$,\n",
        "- $\\hat{P}_{ij} = \\log(P_{ij})$ — это логарифм истинной частоты совместного появления.\n",
        "\n",
        "Для оптимизации, необходимо вычислить частные производные целевой функции по каждому параметру: $\\mathbf{v}_i$, $\\mathbf{v}_j$, $b_i$, и $b_j$.\n",
        "\n",
        "### Шаг 1: Производная по $\\mathbf{v}_i$\n",
        "\n",
        "Целевая функция $J$ относительно $\\mathbf{v}_i$ будет иметь следующий вид:\n",
        "\n",
        "$$\n",
        "J_i = \\sum_{j=1}^V f(P_{ij}) \\left( X_{ij} - \\hat{P}_{ij} \\right)^2\n",
        "$$\n",
        "\n",
        "Для нахождения градиента по $\\mathbf{v}_i$, применим цепное правило:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial J}{\\partial \\mathbf{v}_i} = \\sum_{j=1}^V \\frac{\\partial}{\\partial \\mathbf{v}_i} \\left( f(P_{ij}) \\left( X_{ij} - \\hat{P}_{ij} \\right)^2 \\right)\n",
        "$$\n",
        "\n",
        "Используем цепное правило для вычисления:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial}{\\partial \\mathbf{v}_i} \\left( f(P_{ij}) \\left( X_{ij} - \\hat{P}_{ij} \\right)^2 \\right) = 2 f(P_{ij}) \\left( X_{ij} - \\hat{P}_{ij} \\right) \\frac{\\partial X_{ij}}{\\partial \\mathbf{v}_i}\n",
        "$$\n",
        "\n",
        "Здесь, $\\frac{\\partial X_{ij}}{\\partial \\mathbf{v}_i}$ вычисляется как:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial X_{ij}}{\\partial \\mathbf{v}_i} = \\mathbf{v}_j\n",
        "$$\n",
        "\n",
        "Таким образом, градиент по $\\mathbf{v}_i$ будет:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial J}{\\partial \\mathbf{v}_i} = 2 \\sum_{j=1}^V f(P_{ij}) \\left( X_{ij} - \\hat{P}_{ij} \\right) \\mathbf{v}_j\n",
        "$$\n",
        "\n",
        "### Шаг 2: Производная по $\\mathbf{v}_j$\n",
        "\n",
        "Для вычисления градиента по $\\mathbf{v}_j$, аналогичным образом получаем:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial J}{\\partial \\mathbf{v}_j} = 2 \\sum_{i=1}^V f(P_{ij}) \\left( X_{ij} - \\hat{P}_{ij} \\right) \\mathbf{v}_i\n",
        "$$\n",
        "\n",
        "### Шаг 3: Производная по $b_i$\n",
        "\n",
        "Теперь вычислим градиент по смещению $b_i$. Для этого учитываем, что $X_{ij} = \\mathbf{v}_i^T \\mathbf{v}_j + b_i + b_j$, и $b_i$ появляется только в части $b_i + b_j$:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial J}{\\partial b_i} = 2 \\sum_{j=1}^V f(P_{ij}) \\left( X_{ij} - \\hat{P}_{ij} \\right)\n",
        "$$\n",
        "\n",
        "### Шаг 4: Производная по $b_j$\n",
        "\n",
        "Аналогично, градиент по $b_j$:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial J}{\\partial b_j} = 2 \\sum_{i=1}^V f(P_{ij}) \\left( X_{ij} - \\hat{P}_{ij} \\right)\n",
        "$$\n",
        "\n",
        "### Итоговые градиенты:\n",
        "\n",
        "1. **Градиент по $\\mathbf{v}_i$:**\n",
        "\n",
        "$$\n",
        "\\frac{\\partial J}{\\partial \\mathbf{v}_i} = 2 \\sum_{j=1}^V f(P_{ij}) \\left( \\mathbf{v}_i^T \\mathbf{v}_j + b_i + b_j - \\log(P_{ij}) \\right) \\mathbf{v}_j\n",
        "$$\n",
        "\n",
        "2. **Градиент по $\\mathbf{v}_j$:**\n",
        "\n",
        "$$\n",
        "\\frac{\\partial J}{\\partial \\mathbf{v}_j} = 2 \\sum_{i=1}^V f(P_{ij}) \\left( \\mathbf{v}_i^T \\mathbf{v}_j + b_i + b_j - \\log(P_{ij}) \\right) \\mathbf{v}_i\n",
        "$$\n",
        "\n",
        "3. **Градиент по $b_i$:**\n",
        "\n",
        "$$\n",
        "\\frac{\\partial J}{\\partial b_i} = 2 \\sum_{j=1}^V f(P_{ij}) \\left( \\mathbf{v}_i^T \\mathbf{v}_j + b_i + b_j - \\log(P_{ij}) \\right)\n",
        "$$\n",
        "\n",
        "4. **Градиент по $b_j$:**\n",
        "\n",
        "$$\n",
        "\\frac{\\partial J}{\\partial b_j} = 2 \\sum_{i=1}^V f(P_{ij}) \\left( \\mathbf{v}_i^T \\mathbf{v}_j + b_i + b_j - \\log(P_{ij}) \\right)\n",
        "$$\n",
        "\n",
        "### Примечания:\n",
        "\n",
        "- Функция $f(P_{ij})$ — это весовая функция, которая зависит от частоты совместного появления $P_{ij}$. Обычно она принимает форму:\n",
        "\n",
        "$$\n",
        "f(P_{ij}) = \\frac{P_{ij}}{(P_{ij} + \\alpha)}\n",
        "$$\n",
        "\n",
        "где $\\alpha$ — это параметр, регулирующий влияние редких слов. Важно, что для $P_{ij}$ меньше $\\alpha$, вес уменьшается.\n",
        "\n",
        "- Эти градиенты можно использовать для обновления параметров с помощью метода градиентного спуска.\n",
        "\n",
        "\n",
        "## Алгоритм обучения модели GloVe\n",
        "\n",
        "Процесс обучения модели GloVe состоит из следующих этапов:\n",
        "\n",
        "1. **Построение матрицы совместных вероятностей:** Для каждого корпуса текста вычисляется матрица совместных вероятностей $P_{ij}$ для всех пар слов.\n",
        "2. **Обучение векторных представлений:** Используя целевую функцию, модель минимизирует ошибку и обучает векторы слов с учетом глобальной статистики корпуса.\n",
        "3. **Оптимизация:** Модель применяет стохастический градиентный спуск (SGD) или его варианты для оптимизации функции потерь.\n",
        "\n",
        "## Особенности модели\n",
        "\n",
        "- **Гибкость:** Модель GloVe позволяет легко адаптировать размер векторов слов, а также выбирать различные функции весов для разных типов данных.\n",
        "- **Контекст:** В отличие от других моделей, GloVe использует глобальную информацию о словах, что позволяет создавать более точные и информативные представления.\n",
        "- **Производительность:** Несмотря на свою сложность, модель GloVe может быть эффективно обучена на больших корпусах текстов.\n",
        "\n",
        "## Применение GloVe\n",
        "\n",
        "- **Семантический поиск:** Благодаря векторным представлениям слов модель GloVe позволяет проводить поиск информации, основываясь не на точных совпадениях, а на смысловой близости запросов.\n",
        "- **Анализ текста:** Векторные представления используются для извлечения смысловых отношений между словами, что полезно для задач обработки естественного языка, таких как анализ тональности, классификация текста и другие.\n",
        "\n",
        "Таким образом, модель GloVe является мощным инструментом для обработки и анализа текстов, позволяя эффективно использовать глобальную статистику для обучения векторных представлений слов. Она сочетает в себе лучшие свойства методов, ориентированных на локальные контексты, и методов, работающих с глобальной статистикой, что делает её удобным инструментом для широкого спектра задач в области обработки естественного языка.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Давайте рассмотрим, как работает эта модель на основе простого корпуса текстов и покажем, как вычисляются векторные представления слов с использованием формул.\n",
        "\n",
        "### Пример корпуса\n",
        "\n",
        "Предположим, у нас есть следующий маленький корпус текстов:\n",
        "\n",
        "1. \"I love machine learning\"\n",
        "2. \"Machine learning is fun\"\n",
        "3. \"I enjoy learning\"\n",
        "\n",
        "Из этого корпуса мы будем строить матрицу совместных вероятностей для слов.\n",
        "\n",
        "### 1. Построение матрицы частоты совместных слов\n",
        "\n",
        "Сначала нам нужно вычислить частоты совместных появлений слов в нашем корпусе. Для этого создадим **матрицу частот совместных слов** $X$, где $X_{ij}$ — это количество раз, когда слово $w_i$ и слово $w_j$ появляются в одном контексте.\n",
        "\n",
        "Допустим, что контекстное окно равно 1 слову, то есть, слова считаются \"совместными\", если они стоят рядом.\n",
        "\n",
        "|         | **I** | **love** | **machine** | **learning** | **is** | **fun** | **enjoy** |\n",
        "||-|-|-|--|--||--|\n",
        "| **I**       | 0     | 1        | 1           | 1            | 0      | 0       | 1         |\n",
        "| **love**    | 1     | 0        | 1           | 1            | 0      | 0       | 0         |\n",
        "| **machine** | 1     | 1        | 0           | 2            | 1      | 0       | 0         |\n",
        "| **learning**| 1     | 1        | 2           | 0            | 1      | 1       | 1         |\n",
        "| **is**      | 0     | 0        | 1           | 1            | 0      | 1       | 0         |\n",
        "| **fun**     | 0     | 0        | 0           | 1            | 1      | 0       | 0         |\n",
        "| **enjoy**   | 1     | 0        | 0           | 1            | 0      | 0       | 0         |\n",
        "\n",
        "Эта матрица $X$ показывает, сколько раз два слова появляются рядом в контексте.\n",
        "\n",
        "### 2. Преобразование частот в вероятности\n",
        "\n",
        "Далее, мы рассчитываем вероятности совместного появления слов. Для этого нормализуем значения в матрице $X$, разделив каждую строку на сумму её элементов. Это даст нам **матрицу вероятностей совместных слов** $P_{ij}$.\n",
        "\n",
        "Для первой строки:\n",
        "\n",
        "$$\n",
        "P_{\\text{I,love}} = \\frac{X_{\\text{I,love}}}{\\sum_j X_{\\text{I,j}}} = \\frac{1}{1+1+1+1+0+0+1} = \\frac{1}{5}\n",
        "$$\n",
        "\n",
        "Для других строк аналогично вычисляются вероятности. В результате получаем матрицу вероятностей совместного появления:\n",
        "\n",
        "|         | **I** | **love** | **machine** | **learning** | **is** | **fun** | **enjoy** |\n",
        "||-|-|-|--|--||--|\n",
        "| **I**       | 0     | 0.2      | 0.2         | 0.2          | 0      | 0       | 0.2       |\n",
        "| **love**    | 0.2   | 0        | 0.2         | 0.2          | 0      | 0       | 0         |\n",
        "| **machine** | 0.2   | 0.2      | 0           | 0.4          | 0.2    | 0       | 0         |\n",
        "| **learning**| 0.2   | 0.2      | 0.4         | 0            | 0.2    | 0.2     | 0.2       |\n",
        "| **is**      | 0     | 0        | 0.2         | 0.2          | 0      | 0.2     | 0         |\n",
        "| **fun**     | 0     | 0        | 0           | 0.2          | 0.2    | 0       | 0         |\n",
        "| **enjoy**   | 0.2   | 0        | 0           | 0.2          | 0      | 0       | 0         |\n",
        "\n",
        "### 3. Построение целевой функции GloVe\n",
        "\n",
        "Теперь, зная матрицу вероятностей совместных слов, модель GloVe будет обучать векторные представления слов. Векторные представления $\\mathbf{v}_i$ и $\\mathbf{v}_j$ для слов $w_i$ и $w_j$ должны быть такими, чтобы для каждой пары слов выполнялось следующее приближение для совместной вероятности $P_{ij}$:\n",
        "\n",
        "$$\n",
        "f(P_{ij}) = \\mathbf{v}_i^T \\mathbf{v}_j + b_i + b_j - \\log(P_{ij})\n",
        "$$\n",
        "\n",
        "Модель GloVe минимизирует ошибку:\n",
        "\n",
        "$$\n",
        "J = \\sum_{i,j=1}^V f(P_{ij}) \\left( \\mathbf{v}_i^T \\mathbf{v}_j + b_i + b_j - \\log(P_{ij}) \\right)^2\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $f(P_{ij})$ — это весовая функция, которая контролирует влияние вероятности совместного появления. Обычно используется функция вида $f(P_{ij}) = \\frac{P_{ij}}{(P_{ij} + \\alpha)}$, где $\\alpha$ — параметр, регулирующий влияние редких слов.\n",
        "- $\\mathbf{v}_i$ и $\\mathbf{v}_j$ — векторные представления слов $w_i$ и $w_j$.\n",
        "- $b_i$ и $b_j$ — смещения для слов.\n",
        "\n",
        "### 4. Обучение модели\n",
        "\n",
        "Процесс обучения заключается в оптимизации целевой функции $J$, чтобы минимизировать ошибку предсказания для всех возможных пар слов. Мы используем **метод градиентного спуска** для обновления параметров $\\mathbf{v}_i$, $\\mathbf{v}_j$, $b_i$ и $b_j$.\n",
        "\n",
        "Алгоритм градиентного спуска для обновления параметров выглядит так:\n",
        "\n",
        "$$\n",
        "\\mathbf{v}_i \\leftarrow \\mathbf{v}_i - \\eta \\frac{\\partial J}{\\partial \\mathbf{v}_i}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\mathbf{v}_j \\leftarrow \\mathbf{v}_j - \\eta \\frac{\\partial J}{\\partial \\mathbf{v}_j}\n",
        "$$\n",
        "\n",
        "где $\\eta$ — это шаг обучения (learning rate).\n",
        "\n",
        "### 5. Итоги\n",
        "\n",
        "После выполнения этого процесса обучения, векторные представления $\\mathbf{v}_i$ и $\\mathbf{v}_j$ будут содержать числовые векторы, которые отражают семантическую схожесть слов. Например, слова, которые часто появляются в схожих контекстах, будут иметь похожие векторные представления.\n",
        "\n",
        "Таким образом, модель GloVe обучает векторные представления слов, используя **глобальную информацию о совместных вероятностях слов** в корпусе. В отличие от моделей, ориентированных только на локальный контекст, GloVe сочетает в себе локальные и глобальные аспекты для обучения более точных и информативных представлений слов.\n",
        "\n",
        "\n",
        "\n",
        "Реализация модели **GloVe** с нуля в Python требует нескольких шагов, включая обработку текста, создание матрицы частот совместных слов и обучение векторных представлений с использованием градиентного спуска. Для упрощения примера будем использовать небольшой корпус текста, как в предыдущем примере.\n",
        "\n",
        "Ниже представлена базовая реализация модели GloVe. Для упрощения мы будем использовать небольшие параметры и корпус текста, а также просто обучать модель с использованием градиентного спуска.\n",
        "\n",
        "### Шаги:\n",
        "\n",
        "1. **Создание матрицы совместных частот**.\n",
        "2. **Преобразование частот в вероятности**.\n",
        "3. **Инициализация векторных представлений**.\n",
        "4. **Минимизация целевой функции с использованием градиентного спуска**.\n",
        "\n",
        "### Реализация:\n"
      ],
      "metadata": {
        "id": "a7e0_TI0Jip0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "import math\n",
        "import random\n",
        "\n",
        "# 1. Создадим пример корпуса текстов\n",
        "corpus = [\n",
        "    \"I love machine learning\",\n",
        "    \"Machine learning is fun\",\n",
        "    \"I enjoy learning\"\n",
        "]\n",
        "\n",
        "# 2. Обработка корпуса (создание словаря и контекстных пар)\n",
        "def build_vocab(corpus):\n",
        "    word_freq = Counter()\n",
        "    for sentence in corpus:\n",
        "        for word in sentence.split():\n",
        "            word_freq[word] += 1\n",
        "    return word_freq\n",
        "\n",
        "# Создаем словарь и индексируем слова\n",
        "vocab = build_vocab(corpus)\n",
        "word_to_id = {word: idx for idx, word in enumerate(vocab)}\n",
        "id_to_word = {idx: word for word, idx in word_to_id.items()}\n",
        "V = len(vocab)  # Размер словаря\n",
        "\n",
        "# 3. Создание матрицы совместных частот\n",
        "def create_cooccurrence_matrix(corpus, window_size=1):\n",
        "    cooccurrence_matrix = np.zeros((V, V))\n",
        "    for sentence in corpus:\n",
        "        words = sentence.split()\n",
        "        for i, word in enumerate(words):\n",
        "            word_id = word_to_id[word]\n",
        "            # Рассматриваем слова в пределах контекстного окна\n",
        "            start = max(0, i - window_size)\n",
        "            end = min(len(words), i + window_size + 1)\n",
        "            for j in range(start, end):\n",
        "                if i != j:\n",
        "                    context_word_id = word_to_id[words[j]]\n",
        "                    cooccurrence_matrix[word_id, context_word_id] += 1\n",
        "    return cooccurrence_matrix\n",
        "\n",
        "# Создаем матрицу частот совместных слов\n",
        "cooccurrence_matrix = create_cooccurrence_matrix(corpus, window_size=1)\n",
        "\n",
        "# 4. Инициализация векторных представлений и смещений\n",
        "def initialize_vectors(V, embedding_dim=2):\n",
        "    W = np.random.uniform(-0.5, 0.5, (V, embedding_dim))\n",
        "    b = np.zeros(V)  # Смещения для слов\n",
        "    return W, b\n",
        "\n",
        "embedding_dim = 2\n",
        "W, b = initialize_vectors(V, embedding_dim)\n",
        "\n",
        "# 5. Функция потерь GloVe\n",
        "def glove_loss(W, b, cooccurrence_matrix, alpha=0.75, x_max=100, lambda_reg=0.1):\n",
        "    loss = 0\n",
        "    for i in range(V):\n",
        "        for j in range(V):\n",
        "            if cooccurrence_matrix[i, j] > 0:\n",
        "                # Глобальная функция веса\n",
        "                weight = (cooccurrence_matrix[i, j] / x_max) ** alpha if cooccurrence_matrix[i, j] < x_max else 1\n",
        "                # Расчет ошибки\n",
        "                prediction = np.dot(W[i], W[j]) + b[i] + b[j]\n",
        "                loss += weight * (prediction - np.log(cooccurrence_matrix[i, j])) ** 2\n",
        "                # Регуляризация\n",
        "                loss += lambda_reg * (np.linalg.norm(W[i]) ** 2 + np.linalg.norm(W[j]) ** 2)\n",
        "    return loss\n",
        "\n",
        "# 6. Градиентный спуск для оптимизации\n",
        "def train_glove(W, b, cooccurrence_matrix, learning_rate=0.01, epochs=1000):\n",
        "    for epoch in range(epochs):\n",
        "        loss = glove_loss(W, b, cooccurrence_matrix)\n",
        "        if epoch % 100 == 0:\n",
        "            print(f\"Epoch {epoch}, Loss: {loss}\")\n",
        "\n",
        "        # Обновляем вектора и смещения с использованием градиентного спуска\n",
        "        for i in range(V):\n",
        "            for j in range(V):\n",
        "                if cooccurrence_matrix[i, j] > 0:\n",
        "                    weight = (cooccurrence_matrix[i, j] / 100) ** 0.75 if cooccurrence_matrix[i, j] < 100 else 1\n",
        "                    diff = np.dot(W[i], W[j]) + b[i] + b[j] - np.log(cooccurrence_matrix[i, j])\n",
        "                    grad_W_i = weight * diff * W[j] + 2 * 0.1 * W[i]\n",
        "                    grad_W_j = weight * diff * W[i] + 2 * 0.1 * W[j]\n",
        "                    grad_b_i = weight * diff\n",
        "                    grad_b_j = weight * diff\n",
        "\n",
        "                    # Обновляем параметры\n",
        "                    W[i] -= learning_rate * grad_W_i\n",
        "                    W[j] -= learning_rate * grad_W_j\n",
        "                    b[i] -= learning_rate * grad_b_i\n",
        "                    b[j] -= learning_rate * grad_b_j\n",
        "    return W, b\n",
        "\n",
        "# 7. Обучаем модель\n",
        "W, b = train_glove(W, b, cooccurrence_matrix)\n",
        "\n",
        "# 8. Результат: векторные представления\n",
        "def get_word_vector(word):\n",
        "    word_id = word_to_id[word]\n",
        "    return W[word_id]\n",
        "\n",
        "# Пример: Получаем векторное представление для слова \"machine\"\n",
        "print(get_word_vector(\"machine\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDS5Cme8Q3Kp",
        "outputId": "1d199cf3-2449-41fa-b714-7800fc012c8c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.5392021035794872\n",
            "Epoch 100, Loss: 0.08646091508686206\n",
            "Epoch 200, Loss: 0.020842844123965847\n",
            "Epoch 300, Loss: 0.00627227485039026\n",
            "Epoch 400, Loss: 0.002216820422656117\n",
            "Epoch 500, Loss: 0.0008762878395693146\n",
            "Epoch 600, Loss: 0.0003701267517558548\n",
            "Epoch 700, Loss: 0.00016191817295603664\n",
            "Epoch 800, Loss: 7.213019771076706e-05\n",
            "Epoch 900, Loss: 3.246633861730879e-05\n",
            "[-7.56723489e-05 -1.96834607e-05]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Объяснение кода:\n",
        "\n",
        "1. **Предобработка данных**:\n",
        "   - Мы создаем корпус, состоящий из нескольких предложений.\n",
        "   - В функции `build_vocab()` считаем частоту появления каждого слова.\n",
        "   - Создаем отображение слов в индексы, которое позволит быстро обращаться к словам.\n",
        "\n",
        "2. **Создание матрицы совместных частот**:\n",
        "   - Функция `create_cooccurrence_matrix()` создает матрицу, где на пересечении строк и столбцов стоят частоты, с которой два слова появляются рядом (в пределах контекстного окна).\n",
        "   \n",
        "3. **Инициализация векторов слов и смещений**:\n",
        "   - Вектора слов и смещения инициализируются случайными значениями, используя нормальное распределение.\n",
        "   \n",
        "4. **Целевая функция и обучение**:\n",
        "   - Функция `glove_loss()` реализует целевую функцию GloVe, которая минимизирует разницу между предсказанным и реальным логарифмом частоты совместных слов.\n",
        "   - Мы также используем регуляризацию, чтобы предотвратить переобучение.\n",
        "   - В функции `train_glove()` осуществляется оптимизация параметров с использованием градиентного спуска.\n",
        "\n",
        "5. **Результат**:\n",
        "   - После обучения можно извлекать векторные представления слов с помощью функции `get_word_vector()`.\n",
        "\n",
        "### Ожидаемый результат:\n",
        "\n",
        "Программа обучает модель GloVe на небольшом корпусе текста и выводит векторное представление для слова \"machine\". Вектора будут зависеть от контекста в тексте и будут обучаться так, чтобы схожие слова имели похожие вектора.\n",
        "\n",
        "### Замечания:\n",
        "\n",
        "1. Этот пример является **упрощенной** реализацией, и в реальных приложениях используется более сложная версия GloVe с оптимизированными вычислениями.\n",
        "2. Модель использует **градиентный спуск**, что требует времени на обучение, особенно на больших данных.\n",
        "\n",
        "\n",
        "Испоьзование gensim:\n"
      ],
      "metadata": {
        "id": "oFS5r8AoQ9k0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Скачиваем необходимый ресурс для токенизации\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Пример корпуса для обучения\n",
        "corpus = [\n",
        "    \"I love machine learning\",\n",
        "    \"Machine learning is fun\",\n",
        "    \"I enjoy learning\"\n",
        "]\n",
        "\n",
        "# Токенизация текста\n",
        "tokenized_corpus = [word_tokenize(sentence.lower()) for sentence in corpus]\n",
        "\n",
        "# Обучаем модель GloVe\n",
        "model = gensim.models.Word2Vec(tokenized_corpus, vector_size=100, window=5, min_count=1, sg=0)\n",
        "\n",
        "# Получаем вектор для слова\n",
        "vector = model.wv['machine']\n",
        "print(\"Вектор для слова 'machine':\")\n",
        "print(vector)\n",
        "\n",
        "# Пример поиска наиболее похожих слов\n",
        "similar_words = model.wv.most_similar(\"machine\", topn=3)\n",
        "print(\"Три слова, похожие на 'machine':\")\n",
        "for word, similarity in similar_words:\n",
        "    print(f\"{word}: {similarity}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fw4AESl7RC1L",
        "outputId": "72bd0776-f79b-4b37-e032-c3d123b8d8c0"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Вектор для слова 'machine':\n",
            "[-8.6196875e-03  3.6657380e-03  5.1898835e-03  5.7419385e-03\n",
            "  7.4669183e-03 -6.1676754e-03  1.1056137e-03  6.0472824e-03\n",
            " -2.8400505e-03 -6.1735227e-03 -4.1022300e-04 -8.3689485e-03\n",
            " -5.6000124e-03  7.1045388e-03  3.3525396e-03  7.2256695e-03\n",
            "  6.8002474e-03  7.5307419e-03 -3.7891543e-03 -5.6180597e-04\n",
            "  2.3483764e-03 -4.5190323e-03  8.3887316e-03 -9.8581640e-03\n",
            "  6.7646410e-03  2.9144168e-03 -4.9328315e-03  4.3981876e-03\n",
            " -1.7395747e-03  6.7113843e-03  9.9648498e-03 -4.3624435e-03\n",
            " -5.9933780e-04 -5.6956373e-03  3.8508223e-03  2.7866268e-03\n",
            "  6.8910765e-03  6.1010956e-03  9.5384968e-03  9.2734173e-03\n",
            "  7.8980681e-03 -6.9895042e-03 -9.1558648e-03 -3.5575271e-04\n",
            " -3.0998408e-03  7.8943167e-03  5.9385742e-03 -1.5456629e-03\n",
            "  1.5109634e-03  1.7900408e-03  7.8175711e-03 -9.5101865e-03\n",
            " -2.0553112e-04  3.4691966e-03 -9.3897223e-04  8.3817719e-03\n",
            "  9.0107834e-03  6.5365066e-03 -7.1162102e-04  7.7104042e-03\n",
            " -8.5343346e-03  3.2071066e-03 -4.6379971e-03 -5.0889552e-03\n",
            "  3.5896183e-03  5.3703394e-03  7.7695143e-03 -5.7665063e-03\n",
            "  7.4333609e-03  6.6254963e-03 -3.7098003e-03 -8.7456414e-03\n",
            "  5.4374672e-03  6.5097557e-03 -7.8755023e-04 -6.7098560e-03\n",
            " -7.0859254e-03 -2.4970602e-03  5.1432536e-03 -3.6652375e-03\n",
            " -9.3700597e-03  3.8267397e-03  4.8844791e-03 -6.4285635e-03\n",
            "  1.2085581e-03 -2.0748770e-03  2.4403334e-05 -9.8835090e-03\n",
            "  2.6920044e-03 -4.7501065e-03  1.0876465e-03 -1.5762246e-03\n",
            "  2.1966731e-03 -7.8815762e-03 -2.7171839e-03  2.6631986e-03\n",
            "  5.3466819e-03 -2.3915148e-03 -9.5100943e-03  4.5058788e-03]\n",
            "Три слова, похожие на 'machine':\n",
            "enjoy: 0.06797593832015991\n",
            "is: 0.0093911774456501\n",
            "fun: 0.0045030200853943825\n"
          ]
        }
      ]
    }
  ]
}